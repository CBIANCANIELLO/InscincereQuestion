{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastai.vision import *\n",
    "from fastai import * \n",
    "from fastai.text import *\n",
    "from pathlib import Path \n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"my bar!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "from torchvision import transforms, utils\n",
    "from sklearn.metrics import confusion_matrix, f1_score, auc, roc_curve, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = True\n",
    "clean = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('../input/embeddings'),\n",
       " WindowsPath('../input/glove.840B.300d'),\n",
       " WindowsPath('../input/sample_submission.csv'),\n",
       " WindowsPath('../input/test.csv'),\n",
       " WindowsPath('../input/tmp_lm'),\n",
       " WindowsPath('../input/train.csv')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if lm: path = Path('../input'); \n",
    "list(path.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_df = pd.read_csv(path/'train.csv')\n",
    "test_df = pd.read_csv(path/'test.csv')\n",
    "allText_df = train_df.copy().append(test_df.copy(), sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clean:\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "    allText_df[\"question_text\"] = allText_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "\n",
    "    train_df['question_text'] = train_df['question_text'].apply(lambda x: clean_text(x))\n",
    "    test_df['question_text'] = test_df['question_text'].apply(lambda x: clean_text(x))\n",
    "    allText_df[\"question_text\"] = allText_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "\n",
    "    train_df['question_text'] = train_df['question_text'].apply(lambda x: clean_numbers(x))\n",
    "    test_df['question_text'] = test_df['question_text'].apply(lambda x:clean_numbers(x))\n",
    "    allText_df[\"question_text\"] = allText_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "\n",
    "    train_df['question_text'] = train_df['question_text'].apply(lambda x: replace_typical_mispell(x))\n",
    "    test_df['question_text'] = test_df['question_text'].apply(lambda x: replace_typical_mispell(x))\n",
    "    allText_df[\"question_text\"] = allText_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "\n",
    "#train_x = train_df['question_text'].fillna('_##_').values\n",
    "#test_x = test_df['question_text'].fillna('_##_').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import zipfile\n",
    "zf = zipfile.ZipFile(path/'test.csv.zip')\n",
    "df_test = pd.read_csv(zf.open('test.csv'))\n",
    "zf = zipfile.ZipFile(path/'train.csv.zip')\n",
    "df_train = pd.read_csv(zf.open('train.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_train.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences():\n",
    "    tokenizer = lambda text: text.split() # the function above is the function we will be using to tokenize the text\n",
    "    TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer, lower=True)\n",
    "    LABEL = torchtext.data.Field(sequential=False, use_vocab=False) # sequential and use_vocab=False since no text (binary)\n",
    "    train_datafields = [(\"qid\", None), (\"question_text\", TEXT), (\"target\", LABEL)]\n",
    "    train = torchtext.data.TabularDataset( # If we had a validation set as well, we would add an additional .splits(...)\n",
    "                        # path=\"data/train_cleaned_v2.csv\", # the root directory where the data lies\n",
    "                        path = path/'train.csv',\n",
    "                        format='csv',\n",
    "                        # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "                        skip_header=True, \n",
    "                        fields=train_datafields)\n",
    "    test_datafields = [(\"qid\", None),\n",
    "                     (\"question_text\", TEXT)] \n",
    "    test = torchtext.data.TabularDataset( \n",
    "                path=path/'test.csv',\n",
    "                format=\"csv\",\n",
    "                skip_header=True,\n",
    "                fields=test_datafields)\n",
    "    return TEXT, LABEL, train, test\n",
    "# Chat Conversation End\n",
    "# Type a message...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT,LABEL,train,test = prepare_sequences()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LABEL.batch_first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, test, vectors = \"glove.6B.50d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_storage = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,_ = train.split(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "train_iter = BucketIterator(\n",
    "    train, # we pass in the datasets we want the iterator to draw data from\n",
    "    batch_size= 64, \n",
    "    sort_key= lambda x: len(x.question_text), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "    sort_within_batch=False, # sorting would add bias\n",
    "    repeat=False)\n",
    "\n",
    "test_iter = Iterator(\n",
    "    test,\n",
    "    batch_size = 64,\n",
    "    sort = False,\n",
    "    sort_within_batch = False,\n",
    "    repeat = False)\n",
    "\n",
    "ngpu = 0\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#iter(train_iter).__next__().question_text\n",
    "len(train_iter)\n",
    "label_Size = next(iter(train_iter)).target\n",
    "len(label_Size[label_Size==1])\n",
    "#train_iter.batches\n",
    "#train_iter = iter(train_iter)\n",
    "#train_iter.__next__().question_text\n",
    "#train_iter.__next__().target\n",
    "#iter(train_iter).__next__().question_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaller,_ = train.split(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaller.examples[0].question_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5872"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(train)\n",
    "len(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.examples[0].question_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([527626, 50])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT.vocab.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 50\n",
    "hidden_size = 150\n",
    "ver1 = False\n",
    "sigBCE = True\n",
    "bidir = True\n",
    "layer_LSTM = 2\n",
    "layer_Lin = 3\n",
    "dropout_LSTM = 0.1\n",
    "dropout_Lin = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        \n",
    "        hidden_size = 64\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab),embed_size)\n",
    "        self.embedding.weight = nn.Parameter(TEXT.vocab.vectors)\n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size*4,2)\n",
    "        self.relu = nn.ReLU()\n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(16,1)\n",
    "        \n",
    "    def forward(self,train):\n",
    "        #print(train.shape)\n",
    "        h_embedding = self.embedding(train)\n",
    "        #print(h_embedding.shape)\n",
    "        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding,0)))\n",
    "        #print(h_embedding.shape)\n",
    "        \n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        #print(h_lstm.shape)\n",
    "        h_gru, _ = self.gru(h_lstm)\n",
    "        #print(h_gru.shape)\n",
    "        \n",
    "        avg_pool = torch.mean(h_gru,1)\n",
    "        #print(avg_pool.shape)\n",
    "        #avg_pool = avg_pool.view(-1,64)\n",
    "        max_pool, _ = torch.max(h_gru,1)\n",
    "        #print(max_pool.shape)\n",
    "        \n",
    "        \n",
    "        conc = torch.cat((avg_pool, max_pool),1)\n",
    "        print(conc.shape)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        #print(conc.shape)\n",
    "        #conc = self.dropout(conc)\n",
    "        print(conc.shape)\n",
    "        #out = nn.Sigmoid()(conc)\n",
    "        \n",
    "        out = nn.LogSoftmax(dim=1)(conc)\n",
    "        \n",
    "        #out = self.out(conc)\n",
    "        #print(out[:,-1].shape)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "batch = next(iter(train_iter)).question_text\n",
    "net = NeuralNet()\n",
    "#net(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch = next(iter(train_iter)).question_text\n",
    "#batch[30]\n",
    "#layer_Lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1])\n",
      "tensor([[0.5038],\n",
      "        [0.5027],\n",
      "        [0.5037],\n",
      "        [0.5036],\n",
      "        [0.5037],\n",
      "        [0.5022],\n",
      "        [0.5034],\n",
      "        [0.5029],\n",
      "        [0.5016],\n",
      "        [0.5011],\n",
      "        [0.5035],\n",
      "        [0.5013],\n",
      "        [0.5041],\n",
      "        [0.5030],\n",
      "        [0.5035],\n",
      "        [0.5039],\n",
      "        [0.5040],\n",
      "        [0.5035],\n",
      "        [0.5035],\n",
      "        [0.5021],\n",
      "        [0.5024],\n",
      "        [0.5024],\n",
      "        [0.5026],\n",
      "        [0.5030],\n",
      "        [0.5039],\n",
      "        [0.5030],\n",
      "        [0.5037],\n",
      "        [0.5033],\n",
      "        [0.5035],\n",
      "        [0.5038],\n",
      "        [0.5016],\n",
      "        [0.5026],\n",
      "        [0.5037],\n",
      "        [0.5026],\n",
      "        [0.5029],\n",
      "        [0.5017],\n",
      "        [0.5030],\n",
      "        [0.5020],\n",
      "        [0.5009],\n",
      "        [0.5031],\n",
      "        [0.5036],\n",
      "        [0.5035],\n",
      "        [0.5030],\n",
      "        [0.5035],\n",
      "        [0.5033],\n",
      "        [0.5027],\n",
      "        [0.5011],\n",
      "        [0.5034],\n",
      "        [0.5038],\n",
      "        [0.5022],\n",
      "        [0.5031],\n",
      "        [0.5032],\n",
      "        [0.5035],\n",
      "        [0.5013],\n",
      "        [0.5009],\n",
      "        [0.5038],\n",
      "        [0.5038],\n",
      "        [0.5029],\n",
      "        [0.5034],\n",
      "        [0.5023],\n",
      "        [0.5041],\n",
      "        [0.5031],\n",
      "        [0.5024],\n",
      "        [0.5035]])\n"
     ]
    }
   ],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "                      \n",
    "        self.hidden_dim = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab),embed_size)\n",
    "        self.embedding.weight = nn.Parameter(TEXT.vocab.vectors)\n",
    "        \n",
    "        \n",
    "        #self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        \n",
    "        self.lstm_Layer = nn.LSTM(embed_size, hidden_size,num_layers = layer_LSTM, dropout = dropout_LSTM, bidirectional = bidir)\n",
    "        \n",
    "        self.lin_Layer = []\n",
    "        \n",
    "        self.lin_Layer_Dropout = dropout_Lin\n",
    "        \n",
    "        if bidir:\n",
    "            scale = 2\n",
    "        else:\n",
    "            scale = 1\n",
    "        \n",
    "        for _ in range(layer_Lin - 1):\n",
    "            self.lin_Layer.append(nn.Linear(hidden_size*scale,hidden_size*scale))\n",
    "            self.lin_Layer = nn.ModuleList(self.lin_Layer)\n",
    "        if sigBCE:\n",
    "            self.hidden2tag = nn.Linear(hidden_size*scale, 1)\n",
    "        else:\n",
    "            self.hidden2tag = nn.Linear(hidden_size*scale, 2)\n",
    "    \n",
    "    def forward(self, train):\n",
    "        #print(train.shape)\n",
    "        embeds = self.embedding(train)\n",
    "        #print(embeds.shape)\n",
    "        #print(embeds.view(len(train),64,-1).shape)\n",
    "        #print(embeds.view(len(train),train.shape[1],-1).shape)\n",
    "        lstm_out, _ = self.lstm_Layer(embeds)#embeds.view(len(train),train.shape[1],-1))\n",
    "        #print(lstm_out.shape)\n",
    "        \n",
    "        if ver1:\n",
    "            #print(lstm_out.shape)\n",
    "            feature = lstm_out[-1,:,:] # -1 grabs last layer in list\n",
    "            #print(feature.shape)\n",
    "        \n",
    "            for layer in self.lin_Layer:\n",
    "                feature = layer(feature)\n",
    "                feature = F.relu(feature)\n",
    "                predict = self.hidden2tag(feature)\n",
    "        \n",
    "            feature = F.relu(feature)\n",
    "            predict = self.hidden2tag(feature)\n",
    "            if sigBCE:\n",
    "                return nn.Sigmoid()(predict)\n",
    "            else:\n",
    "                return nn.LogSoftmax(dim=1)(predict)\n",
    "        else:\n",
    "           \n",
    "            avg_pool = torch.mean(lstm_out,0)\n",
    "            #print(avg_pool.shape)\n",
    "            target_space = self.hidden2tag(avg_pool)#.view(len(train),-1))\n",
    "            #print(target_space.shape)\n",
    "            if sigBCE:\n",
    "                return nn.Sigmoid()(target_space)\n",
    "            else:\n",
    "                return F.log_softmax(target_space, dim=1)\n",
    "            #print(target_score.shape)\n",
    "\n",
    "inputs_check = next(iter(train_iter)).question_text\n",
    "\n",
    "#print(inputs_check.shape)\n",
    "\n",
    "model = LSTMTagger()\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = inputs_check\n",
    "    #print(len(inputs))\n",
    "    score = model(inputs)\n",
    "    print(score.shape)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    if  isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger()\n",
    "model.apply(weight_init)\n",
    "if sigBCE:\n",
    "    loss_function = nn.BCELoss(reduction='none')\n",
    "else:\n",
    "    loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 out of:  2041  With Loss of:  0.7078242897987366  Acc:  1.5625  Best Score of:  0.53529346\n",
      "2 out of:  2041  With Loss of:  0.6793862581253052  Acc:  95.3125  Best Score of:  0.49622568\n",
      "3 out of:  2041  With Loss of:  0.7397185564041138  Acc:  90.625  Best Score of:  0.45170102\n",
      "4 out of:  2041  With Loss of:  0.6710286140441895  Acc:  95.3125  Best Score of:  0.45772377\n",
      "5 out of:  2041  With Loss of:  0.7044904232025146  Acc:  90.625  Best Score of:  0.46176317\n",
      "6 out of:  2041  With Loss of:  0.7219658493995667  Acc:  90.625  Best Score of:  0.5096081\n",
      "7 out of:  2041  With Loss of:  0.6641263365745544  Acc:  73.4375  Best Score of:  0.5970915\n",
      "8 out of:  2041  With Loss of:  0.6130262613296509  Acc:  84.375  Best Score of:  0.65311146\n",
      "9 out of:  2041  With Loss of:  0.6637488007545471  Acc:  90.625  Best Score of:  0.6803183\n",
      "10 out of:  2041  With Loss of:  0.6823036670684814  Acc:  84.375  Best Score of:  0.71590024\n",
      "11 out of:  2041  With Loss of:  0.7127294540405273  Acc:  70.3125  Best Score of:  0.7232753\n",
      "12 out of:  2041  With Loss of:  0.6367743015289307  Acc:  90.625  Best Score of:  0.7610818\n",
      "13 out of:  2041  With Loss of:  0.7462981343269348  Acc:  68.75  Best Score of:  0.7440852\n",
      "14 out of:  2041  With Loss of:  0.7206459045410156  Acc:  64.0625  Best Score of:  0.80398923\n",
      "15 out of:  2041  With Loss of:  1.0109745264053345  Acc:  18.75  Best Score of:  0.8392553\n",
      "16 out of:  2041  With Loss of:  0.5711464285850525  Acc:  67.1875  Best Score of:  0.80769974\n",
      "17 out of:  2041  With Loss of:  0.736291766166687  Acc:  85.9375  Best Score of:  0.76384264\n",
      "18 out of:  2041  With Loss of:  0.6671839356422424  Acc:  90.625  Best Score of:  0.6555712\n",
      "19 out of:  2041  With Loss of:  0.6669522523880005  Acc:  87.5  Best Score of:  0.6375299\n",
      "20 out of:  2041  With Loss of:  0.8290989995002747  Acc:  90.625  Best Score of:  0.62835866\n",
      "21 out of:  2041  With Loss of:  0.6560075283050537  Acc:  93.75  Best Score of:  0.59932923\n",
      "22 out of:  2041  With Loss of:  0.5945904850959778  Acc:  92.1875  Best Score of:  0.56348336\n",
      "23 out of:  2041  With Loss of:  0.5574844479560852  Acc:  90.625  Best Score of:  0.5822738\n",
      "24 out of:  2041  With Loss of:  0.7559227347373962  Acc:  89.0625  Best Score of:  0.58335274\n",
      "25 out of:  2041  With Loss of:  0.6152659058570862  Acc:  89.0625  Best Score of:  0.5692043\n",
      "26 out of:  2041  With Loss of:  0.6663414835929871  Acc:  92.1875  Best Score of:  0.57737535\n",
      "27 out of:  2041  With Loss of:  0.7237600684165955  Acc:  85.9375  Best Score of:  0.57635325\n",
      "28 out of:  2041  With Loss of:  0.6104779839515686  Acc:  93.75  Best Score of:  0.5832272\n",
      "29 out of:  2041  With Loss of:  0.6312709450721741  Acc:  84.375  Best Score of:  0.5913901\n",
      "30 out of:  2041  With Loss of:  0.7106273770332336  Acc:  85.9375  Best Score of:  0.5925195\n",
      "31 out of:  2041  With Loss of:  0.7090083360671997  Acc:  81.25  Best Score of:  0.5919054\n",
      "32 out of:  2041  With Loss of:  0.6985085010528564  Acc:  84.375  Best Score of:  0.65761006\n",
      "33 out of:  2041  With Loss of:  0.576442301273346  Acc:  81.25  Best Score of:  0.71692306\n",
      "34 out of:  2041  With Loss of:  0.6527758240699768  Acc:  76.5625  Best Score of:  0.72562504\n",
      "35 out of:  2041  With Loss of:  0.7192947268486023  Acc:  31.25  Best Score of:  0.784837\n",
      "36 out of:  2041  With Loss of:  0.6794192790985107  Acc:  10.9375  Best Score of:  0.82341534\n",
      "37 out of:  2041  With Loss of:  0.7037240862846375  Acc:  15.625  Best Score of:  0.86837983\n",
      "38 out of:  2041  With Loss of:  0.6492758393287659  Acc:  15.625  Best Score of:  0.8778992\n",
      "39 out of:  2041  With Loss of:  0.5542317628860474  Acc:  25.0  Best Score of:  0.8679512\n",
      "40 out of:  2041  With Loss of:  0.5972146987915039  Acc:  37.5  Best Score of:  0.8854798\n",
      "41 out of:  2041  With Loss of:  0.6623349189758301  Acc:  64.0625  Best Score of:  0.92059803\n",
      "42 out of:  2041  With Loss of:  0.6097853779792786  Acc:  65.625  Best Score of:  0.90810126\n",
      "43 out of:  2041  With Loss of:  0.6167764663696289  Acc:  79.6875  Best Score of:  0.9158599\n",
      "44 out of:  2041  With Loss of:  0.5587049126625061  Acc:  84.375  Best Score of:  0.91532993\n",
      "45 out of:  2041  With Loss of:  0.7638782262802124  Acc:  82.8125  Best Score of:  0.90566117\n",
      "46 out of:  2041  With Loss of:  0.6622270941734314  Acc:  92.1875  Best Score of:  0.8824815\n",
      "47 out of:  2041  With Loss of:  0.4687698781490326  Acc:  76.5625  Best Score of:  0.8754318\n",
      "48 out of:  2041  With Loss of:  0.6377734541893005  Acc:  93.75  Best Score of:  0.82147604\n",
      "49 out of:  2041  With Loss of:  1.1484779119491577  Acc:  89.0625  Best Score of:  0.7780849\n",
      "50 out of:  2041  With Loss of:  1.2186564207077026  Acc:  79.6875  Best Score of:  0.71693754\n",
      "51 out of:  2041  With Loss of:  0.9849857091903687  Acc:  85.9375  Best Score of:  0.71839756\n",
      "52 out of:  2041  With Loss of:  0.618053138256073  Acc:  73.4375  Best Score of:  0.66765225\n",
      "53 out of:  2041  With Loss of:  0.6772852540016174  Acc:  51.5625  Best Score of:  0.67964303\n",
      "54 out of:  2041  With Loss of:  0.6730625629425049  Acc:  7.8125  Best Score of:  0.64537376\n",
      "55 out of:  2041  With Loss of:  0.7548472881317139  Acc:  3.125  Best Score of:  0.6239659\n",
      "56 out of:  2041  With Loss of:  0.7308164834976196  Acc:  4.6875  Best Score of:  0.6036212\n",
      "57 out of:  2041  With Loss of:  0.7515597939491272  Acc:  3.125  Best Score of:  0.58164763\n",
      "58 out of:  2041  With Loss of:  0.7271267175674438  Acc:  3.125  Best Score of:  0.5497596\n",
      "59 out of:  2041  With Loss of:  0.6957732439041138  Acc:  4.6875  Best Score of:  0.51627105\n",
      "60 out of:  2041  With Loss of:  0.6890246868133545  Acc:  93.75  Best Score of:  0.48059\n",
      "61 out of:  2041  With Loss of:  0.6026384234428406  Acc:  98.4375  Best Score of:  0.45594326\n",
      "62 out of:  2041  With Loss of:  0.7114512920379639  Acc:  93.75  Best Score of:  0.425447\n",
      "63 out of:  2041  With Loss of:  0.7980045080184937  Acc:  90.625  Best Score of:  0.40353015\n",
      "64 out of:  2041  With Loss of:  0.7755190134048462  Acc:  90.625  Best Score of:  0.41678342\n",
      "65 out of:  2041  With Loss of:  0.7855841517448425  Acc:  92.1875  Best Score of:  0.48322606\n",
      "66 out of:  2041  With Loss of:  0.7441712021827698  Acc:  90.625  Best Score of:  0.50850105\n",
      "67 out of:  2041  With Loss of:  0.6151884198188782  Acc:  90.625  Best Score of:  0.59561414\n",
      "68 out of:  2041  With Loss of:  0.7009590864181519  Acc:  89.0625  Best Score of:  0.69775444\n",
      "69 out of:  2041  With Loss of:  0.7130666971206665  Acc:  62.5  Best Score of:  0.71733946\n",
      "70 out of:  2041  With Loss of:  0.675216555595398  Acc:  50.0  Best Score of:  0.7582908\n",
      "71 out of:  2041  With Loss of:  0.5840021967887878  Acc:  56.25  Best Score of:  0.76355803\n",
      "72 out of:  2041  With Loss of:  0.6202727556228638  Acc:  45.3125  Best Score of:  0.80042315\n",
      "73 out of:  2041  With Loss of:  0.6208299398422241  Acc:  43.75  Best Score of:  0.80242145\n",
      "74 out of:  2041  With Loss of:  0.7077040672302246  Acc:  65.625  Best Score of:  0.82983863\n",
      "75 out of:  2041  With Loss of:  0.6007050275802612  Acc:  76.5625  Best Score of:  0.8299572\n",
      "76 out of:  2041  With Loss of:  0.5170853734016418  Acc:  85.9375  Best Score of:  0.820402\n",
      "77 out of:  2041  With Loss of:  0.272917777299881  Acc:  98.4375  Best Score of:  0.83547914\n",
      "78 out of:  2041  With Loss of:  0.9289477467536926  Acc:  93.75  Best Score of:  0.65584064\n",
      "79 out of:  2041  With Loss of:  1.0354297161102295  Acc:  92.1875  Best Score of:  0.50283015\n",
      "80 out of:  2041  With Loss of:  0.8465152382850647  Acc:  95.3125  Best Score of:  0.63014865\n",
      "81 out of:  2041  With Loss of:  0.4711233377456665  Acc:  98.4375  Best Score of:  0.65873605\n",
      "82 out of:  2041  With Loss of:  0.7955926656723022  Acc:  90.625  Best Score of:  0.69081336\n",
      "83 out of:  2041  With Loss of:  0.8815903663635254  Acc:  92.1875  Best Score of:  0.72097933\n",
      "84 out of:  2041  With Loss of:  0.9296987652778625  Acc:  87.5  Best Score of:  0.7035515\n",
      "85 out of:  2041  With Loss of:  0.44900527596473694  Acc:  85.9375  Best Score of:  0.7850794\n",
      "86 out of:  2041  With Loss of:  0.5875518321990967  Acc:  82.8125  Best Score of:  0.8389485\n",
      "87 out of:  2041  With Loss of:  0.5173535943031311  Acc:  78.125  Best Score of:  0.8482059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 out of:  2041  With Loss of:  0.5991429686546326  Acc:  67.1875  Best Score of:  0.83838075\n",
      "89 out of:  2041  With Loss of:  0.6968387365341187  Acc:  62.5  Best Score of:  0.86916745\n",
      "90 out of:  2041  With Loss of:  0.6687873601913452  Acc:  50.0  Best Score of:  0.8883598\n",
      "91 out of:  2041  With Loss of:  0.6300121545791626  Acc:  48.4375  Best Score of:  0.87979513\n",
      "92 out of:  2041  With Loss of:  0.5952639579772949  Acc:  48.4375  Best Score of:  0.87692165\n",
      "93 out of:  2041  With Loss of:  0.6341680884361267  Acc:  59.375  Best Score of:  0.86297137\n",
      "94 out of:  2041  With Loss of:  0.6974425315856934  Acc:  73.4375  Best Score of:  0.89352936\n",
      "95 out of:  2041  With Loss of:  0.6679660081863403  Acc:  57.8125  Best Score of:  0.8825971\n",
      "96 out of:  2041  With Loss of:  0.5764116048812866  Acc:  53.125  Best Score of:  0.9046061\n",
      "97 out of:  2041  With Loss of:  0.6145943403244019  Acc:  79.6875  Best Score of:  0.85516363\n",
      "98 out of:  2041  With Loss of:  0.5814437866210938  Acc:  82.8125  Best Score of:  0.86889684\n",
      "99 out of:  2041  With Loss of:  0.6731371879577637  Acc:  84.375  Best Score of:  0.8561858\n",
      "100 out of:  2041  With Loss of:  0.689185380935669  Acc:  85.9375  Best Score of:  0.8537087\n",
      "101 out of:  2041  With Loss of:  0.7554597854614258  Acc:  85.9375  Best Score of:  0.87992865\n",
      "102 out of:  2041  With Loss of:  0.5230228304862976  Acc:  90.625  Best Score of:  0.8561121\n",
      "103 out of:  2041  With Loss of:  0.5818784832954407  Acc:  78.125  Best Score of:  0.91088384\n",
      "104 out of:  2041  With Loss of:  0.6321176290512085  Acc:  84.375  Best Score of:  0.94893426\n",
      "105 out of:  2041  With Loss of:  0.8385536074638367  Acc:  73.4375  Best Score of:  0.96466655\n",
      "106 out of:  2041  With Loss of:  0.7782519459724426  Acc:  71.875  Best Score of:  0.9721567\n",
      "107 out of:  2041  With Loss of:  0.4970109164714813  Acc:  85.9375  Best Score of:  0.9674603\n",
      "108 out of:  2041  With Loss of:  0.5572260022163391  Acc:  54.6875  Best Score of:  0.96429926\n",
      "109 out of:  2041  With Loss of:  0.6814321279525757  Acc:  78.125  Best Score of:  0.96872574\n",
      "110 out of:  2041  With Loss of:  0.8261076807975769  Acc:  75.0  Best Score of:  0.9669197\n",
      "111 out of:  2041  With Loss of:  0.4291643500328064  Acc:  78.125  Best Score of:  0.963154\n",
      "112 out of:  2041  With Loss of:  0.6521565318107605  Acc:  87.5  Best Score of:  0.9163268\n",
      "113 out of:  2041  With Loss of:  0.6754966974258423  Acc:  75.0  Best Score of:  0.9378458\n",
      "114 out of:  2041  With Loss of:  0.5938442945480347  Acc:  65.625  Best Score of:  0.95562816\n",
      "115 out of:  2041  With Loss of:  0.6881722807884216  Acc:  67.1875  Best Score of:  0.9371332\n",
      "116 out of:  2041  With Loss of:  0.7393203973770142  Acc:  68.75  Best Score of:  0.94529676\n",
      "117 out of:  2041  With Loss of:  0.7405092120170593  Acc:  68.75  Best Score of:  0.8912668\n",
      "118 out of:  2041  With Loss of:  0.699616551399231  Acc:  45.3125  Best Score of:  0.84828216\n",
      "119 out of:  2041  With Loss of:  0.6584907174110413  Acc:  56.25  Best Score of:  0.8308027\n",
      "120 out of:  2041  With Loss of:  0.6014055013656616  Acc:  81.25  Best Score of:  0.7331628\n",
      "121 out of:  2041  With Loss of:  0.6203979849815369  Acc:  89.0625  Best Score of:  0.7015957\n",
      "122 out of:  2041  With Loss of:  0.6617832183837891  Acc:  71.875  Best Score of:  0.64758897\n",
      "123 out of:  2041  With Loss of:  0.6718576550483704  Acc:  92.1875  Best Score of:  0.5949386\n",
      "124 out of:  2041  With Loss of:  0.5580171346664429  Acc:  98.4375  Best Score of:  0.49865562\n",
      "125 out of:  2041  With Loss of:  0.7850449085235596  Acc:  89.0625  Best Score of:  0.4226592\n",
      "126 out of:  2041  With Loss of:  0.8418987989425659  Acc:  90.625  Best Score of:  0.44703624\n",
      "127 out of:  2041  With Loss of:  0.712437093257904  Acc:  95.3125  Best Score of:  0.43911055\n",
      "128 out of:  2041  With Loss of:  0.7086178660392761  Acc:  93.75  Best Score of:  0.4572033\n",
      "129 out of:  2041  With Loss of:  0.70835280418396  Acc:  93.75  Best Score of:  0.56754804\n",
      "130 out of:  2041  With Loss of:  0.7237330079078674  Acc:  85.9375  Best Score of:  0.6229527\n",
      "131 out of:  2041  With Loss of:  0.6437459588050842  Acc:  79.6875  Best Score of:  0.7596892\n",
      "132 out of:  2041  With Loss of:  0.5535534620285034  Acc:  78.125  Best Score of:  0.8476903\n",
      "133 out of:  2041  With Loss of:  0.6255016326904297  Acc:  68.75  Best Score of:  0.9122665\n",
      "134 out of:  2041  With Loss of:  0.7535011768341064  Acc:  53.125  Best Score of:  0.9006307\n",
      "135 out of:  2041  With Loss of:  0.6522361040115356  Acc:  64.0625  Best Score of:  0.95866597\n",
      "136 out of:  2041  With Loss of:  0.48078662157058716  Acc:  73.4375  Best Score of:  0.9730661\n",
      "137 out of:  2041  With Loss of:  0.5489208698272705  Acc:  59.375  Best Score of:  0.9786916\n",
      "138 out of:  2041  With Loss of:  0.6067700982093811  Acc:  59.375  Best Score of:  0.9754431\n",
      "139 out of:  2041  With Loss of:  0.8855713605880737  Acc:  56.25  Best Score of:  0.960725\n",
      "140 out of:  2041  With Loss of:  0.6740592122077942  Acc:  73.4375  Best Score of:  0.9484792\n",
      "141 out of:  2041  With Loss of:  0.5308661460876465  Acc:  71.875  Best Score of:  0.9633796\n",
      "142 out of:  2041  With Loss of:  0.7346969842910767  Acc:  75.0  Best Score of:  0.8894616\n",
      "143 out of:  2041  With Loss of:  0.7489776015281677  Acc:  79.6875  Best Score of:  0.8795276\n",
      "144 out of:  2041  With Loss of:  0.5502016544342041  Acc:  85.9375  Best Score of:  0.8539049\n",
      "145 out of:  2041  With Loss of:  0.5666558742523193  Acc:  85.9375  Best Score of:  0.7575128\n",
      "146 out of:  2041  With Loss of:  0.6459115743637085  Acc:  85.9375  Best Score of:  0.7201235\n",
      "147 out of:  2041  With Loss of:  0.5753428936004639  Acc:  90.625  Best Score of:  0.76785374\n",
      "148 out of:  2041  With Loss of:  0.6075885891914368  Acc:  92.1875  Best Score of:  0.75266314\n",
      "149 out of:  2041  With Loss of:  0.5980980396270752  Acc:  89.0625  Best Score of:  0.72144693\n",
      "150 out of:  2041  With Loss of:  0.5996993184089661  Acc:  93.75  Best Score of:  0.7535029\n",
      "151 out of:  2041  With Loss of:  0.676369309425354  Acc:  89.0625  Best Score of:  0.76001996\n",
      "152 out of:  2041  With Loss of:  0.43449506163597107  Acc:  87.5  Best Score of:  0.91586477\n",
      "153 out of:  2041  With Loss of:  0.5575435161590576  Acc:  95.3125  Best Score of:  0.8313969\n",
      "154 out of:  2041  With Loss of:  0.5898613929748535  Acc:  84.375  Best Score of:  0.7735419\n",
      "155 out of:  2041  With Loss of:  0.7299069166183472  Acc:  73.4375  Best Score of:  0.85503066\n",
      "156 out of:  2041  With Loss of:  0.5660817623138428  Acc:  81.25  Best Score of:  0.74854696\n",
      "157 out of:  2041  With Loss of:  0.5660762786865234  Acc:  76.5625  Best Score of:  0.8530458\n",
      "158 out of:  2041  With Loss of:  0.3721572458744049  Acc:  93.75  Best Score of:  0.81203294\n",
      "159 out of:  2041  With Loss of:  0.638626754283905  Acc:  89.0625  Best Score of:  0.8302683\n",
      "160 out of:  2041  With Loss of:  0.7689157128334045  Acc:  85.9375  Best Score of:  0.8580448\n",
      "161 out of:  2041  With Loss of:  0.48201674222946167  Acc:  90.625  Best Score of:  0.86283433\n",
      "162 out of:  2041  With Loss of:  0.6958279013633728  Acc:  89.0625  Best Score of:  0.94622284\n",
      "163 out of:  2041  With Loss of:  0.4284203350543976  Acc:  89.0625  Best Score of:  0.95379025\n",
      "164 out of:  2041  With Loss of:  0.5517901182174683  Acc:  87.5  Best Score of:  0.95706177\n",
      "165 out of:  2041  With Loss of:  0.5050072073936462  Acc:  81.25  Best Score of:  0.9133985\n",
      "166 out of:  2041  With Loss of:  0.5659985542297363  Acc:  90.625  Best Score of:  0.9567032\n",
      "167 out of:  2041  With Loss of:  0.5419267416000366  Acc:  71.875  Best Score of:  0.9909288\n",
      "168 out of:  2041  With Loss of:  0.7205726504325867  Acc:  59.375  Best Score of:  0.9220231\n",
      "169 out of:  2041  With Loss of:  0.6111751794815063  Acc:  87.5  Best Score of:  0.9494281\n",
      "170 out of:  2041  With Loss of:  0.733750581741333  Acc:  92.1875  Best Score of:  0.43732345\n",
      "171 out of:  2041  With Loss of:  0.6167254447937012  Acc:  96.875  Best Score of:  0.3973105\n",
      "172 out of:  2041  With Loss of:  0.5788108110427856  Acc:  93.75  Best Score of:  0.5488097\n",
      "173 out of:  2041  With Loss of:  0.5700181722640991  Acc:  87.5  Best Score of:  0.7990866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174 out of:  2041  With Loss of:  0.739393413066864  Acc:  56.25  Best Score of:  0.86976063\n",
      "175 out of:  2041  With Loss of:  0.4852377772331238  Acc:  79.6875  Best Score of:  0.95925146\n",
      "176 out of:  2041  With Loss of:  0.3704402446746826  Acc:  93.75  Best Score of:  0.90120935\n",
      "177 out of:  2041  With Loss of:  0.44643446803092957  Acc:  95.3125  Best Score of:  0.8994443\n",
      "178 out of:  2041  With Loss of:  0.5330154299736023  Acc:  90.625  Best Score of:  0.8463084\n",
      "179 out of:  2041  With Loss of:  0.5425804853439331  Acc:  90.625  Best Score of:  0.9729417\n",
      "180 out of:  2041  With Loss of:  0.7584200501441956  Acc:  76.5625  Best Score of:  0.9990049\n",
      "181 out of:  2041  With Loss of:  0.6397473216056824  Acc:  70.3125  Best Score of:  0.9765286\n",
      "182 out of:  2041  With Loss of:  0.7980847954750061  Acc:  54.6875  Best Score of:  0.9990798\n",
      "183 out of:  2041  With Loss of:  0.7678173184394836  Acc:  51.5625  Best Score of:  0.9983524\n",
      "184 out of:  2041  With Loss of:  0.758500337600708  Acc:  62.5  Best Score of:  0.9853182\n",
      "185 out of:  2041  With Loss of:  0.604042649269104  Acc:  75.0  Best Score of:  0.90706825\n",
      "186 out of:  2041  With Loss of:  0.5089979767799377  Acc:  56.25  Best Score of:  0.8598989\n",
      "187 out of:  2041  With Loss of:  0.5672513842582703  Acc:  39.0625  Best Score of:  0.79508024\n",
      "188 out of:  2041  With Loss of:  0.5676513314247131  Acc:  40.625  Best Score of:  0.85523856\n",
      "189 out of:  2041  With Loss of:  0.6883330345153809  Acc:  71.875  Best Score of:  0.7424843\n",
      "190 out of:  2041  With Loss of:  0.6215066909790039  Acc:  85.9375  Best Score of:  0.6298411\n",
      "191 out of:  2041  With Loss of:  0.4575055241584778  Acc:  93.75  Best Score of:  0.82500863\n",
      "192 out of:  2041  With Loss of:  0.5585628747940063  Acc:  90.625  Best Score of:  0.6423845\n",
      "193 out of:  2041  With Loss of:  0.41651642322540283  Acc:  95.3125  Best Score of:  0.8855098\n",
      "194 out of:  2041  With Loss of:  0.5718675851821899  Acc:  71.875  Best Score of:  0.9617858\n",
      "195 out of:  2041  With Loss of:  0.6795910000801086  Acc:  71.875  Best Score of:  0.97413945\n",
      "196 out of:  2041  With Loss of:  0.527008056640625  Acc:  60.9375  Best Score of:  0.99973196\n",
      "197 out of:  2041  With Loss of:  0.9183273315429688  Acc:  54.6875  Best Score of:  0.9948272\n",
      "198 out of:  2041  With Loss of:  0.29501980543136597  Acc:  76.5625  Best Score of:  0.9954613\n",
      "199 out of:  2041  With Loss of:  0.5004639029502869  Acc:  90.625  Best Score of:  0.92271554\n",
      "200 out of:  2041  With Loss of:  0.3286747932434082  Acc:  95.3125  Best Score of:  0.9834864\n",
      "201 out of:  2041  With Loss of:  0.576619029045105  Acc:  92.1875  Best Score of:  0.89084804\n",
      "202 out of:  2041  With Loss of:  0.42521122097969055  Acc:  92.1875  Best Score of:  0.9740161\n",
      "203 out of:  2041  With Loss of:  0.8797218799591064  Acc:  87.5  Best Score of:  0.53154665\n",
      "204 out of:  2041  With Loss of:  0.2807862460613251  Acc:  92.1875  Best Score of:  0.93527544\n",
      "205 out of:  2041  With Loss of:  0.48310962319374084  Acc:  87.5  Best Score of:  0.99710745\n",
      "206 out of:  2041  With Loss of:  0.6017851829528809  Acc:  71.875  Best Score of:  0.9975867\n",
      "207 out of:  2041  With Loss of:  0.3810844421386719  Acc:  73.4375  Best Score of:  0.9996871\n",
      "208 out of:  2041  With Loss of:  0.3813236355781555  Acc:  81.25  Best Score of:  0.97880447\n",
      "209 out of:  2041  With Loss of:  0.5510221123695374  Acc:  90.625  Best Score of:  0.8960564\n",
      "210 out of:  2041  With Loss of:  0.5090829730033875  Acc:  79.6875  Best Score of:  0.99935955\n",
      "211 out of:  2041  With Loss of:  0.634742021560669  Acc:  89.0625  Best Score of:  0.854075\n",
      "212 out of:  2041  With Loss of:  0.46512898802757263  Acc:  96.875  Best Score of:  0.91420025\n",
      "213 out of:  2041  With Loss of:  0.33720195293426514  Acc:  95.3125  Best Score of:  0.77665347\n",
      "214 out of:  2041  With Loss of:  0.5192252993583679  Acc:  98.4375  Best Score of:  0.8344864\n",
      "215 out of:  2041  With Loss of:  0.3919002413749695  Acc:  84.375  Best Score of:  0.9857684\n",
      "216 out of:  2041  With Loss of:  0.4482288062572479  Acc:  75.0  Best Score of:  0.99112236\n",
      "217 out of:  2041  With Loss of:  0.35714197158813477  Acc:  90.625  Best Score of:  0.9996301\n",
      "218 out of:  2041  With Loss of:  0.36760640144348145  Acc:  89.0625  Best Score of:  0.99821484\n",
      "219 out of:  2041  With Loss of:  0.3682329058647156  Acc:  84.375  Best Score of:  0.99378306\n",
      "220 out of:  2041  With Loss of:  0.6306180953979492  Acc:  95.3125  Best Score of:  0.72366685\n",
      "221 out of:  2041  With Loss of:  0.2505819797515869  Acc:  93.75  Best Score of:  0.9659792\n",
      "222 out of:  2041  With Loss of:  0.16669973731040955  Acc:  95.3125  Best Score of:  0.957196\n",
      "223 out of:  2041  With Loss of:  0.45324793457984924  Acc:  93.75  Best Score of:  0.9907375\n",
      "224 out of:  2041  With Loss of:  0.6110873818397522  Acc:  95.3125  Best Score of:  0.9785404\n",
      "225 out of:  2041  With Loss of:  0.09272146970033646  Acc:  98.4375  Best Score of:  0.99940825\n",
      "226 out of:  2041  With Loss of:  0.5350944995880127  Acc:  95.3125  Best Score of:  0.99979097\n",
      "227 out of:  2041  With Loss of:  0.5303640961647034  Acc:  87.5  Best Score of:  0.9984366\n",
      "228 out of:  2041  With Loss of:  0.40361854434013367  Acc:  92.1875  Best Score of:  0.9997297\n",
      "229 out of:  2041  With Loss of:  0.6398309469223022  Acc:  76.5625  Best Score of:  0.98682123\n",
      "230 out of:  2041  With Loss of:  0.4936622977256775  Acc:  76.5625  Best Score of:  0.9723174\n",
      "231 out of:  2041  With Loss of:  0.471588134765625  Acc:  89.0625  Best Score of:  0.99425995\n",
      "232 out of:  2041  With Loss of:  0.6776617765426636  Acc:  85.9375  Best Score of:  0.99289143\n",
      "233 out of:  2041  With Loss of:  0.7116566896438599  Acc:  92.1875  Best Score of:  0.70891064\n",
      "234 out of:  2041  With Loss of:  0.5653544068336487  Acc:  90.625  Best Score of:  0.7734766\n",
      "235 out of:  2041  With Loss of:  0.5365737676620483  Acc:  62.5  Best Score of:  0.97230047\n",
      "236 out of:  2041  With Loss of:  0.5365132093429565  Acc:  48.4375  Best Score of:  0.9901799\n",
      "237 out of:  2041  With Loss of:  0.7520606517791748  Acc:  15.625  Best Score of:  0.81169766\n",
      "238 out of:  2041  With Loss of:  0.5213955640792847  Acc:  68.75  Best Score of:  0.8794933\n",
      "239 out of:  2041  With Loss of:  0.39666131138801575  Acc:  93.75  Best Score of:  0.90483165\n",
      "240 out of:  2041  With Loss of:  0.4898202419281006  Acc:  96.875  Best Score of:  0.7618274\n",
      "241 out of:  2041  With Loss of:  0.4863707423210144  Acc:  93.75  Best Score of:  0.7501715\n",
      "242 out of:  2041  With Loss of:  0.7615847587585449  Acc:  93.75  Best Score of:  0.80977774\n",
      "243 out of:  2041  With Loss of:  0.63469398021698  Acc:  93.75  Best Score of:  0.9946557\n",
      "244 out of:  2041  With Loss of:  0.4526728391647339  Acc:  87.5  Best Score of:  0.9768578\n",
      "245 out of:  2041  With Loss of:  0.2958117723464966  Acc:  93.75  Best Score of:  0.9929074\n",
      "246 out of:  2041  With Loss of:  0.6506959199905396  Acc:  84.375  Best Score of:  0.866351\n",
      "247 out of:  2041  With Loss of:  0.40413814783096313  Acc:  82.8125  Best Score of:  0.99683374\n",
      "248 out of:  2041  With Loss of:  0.5595521330833435  Acc:  85.9375  Best Score of:  0.88630223\n",
      "249 out of:  2041  With Loss of:  0.5737714767456055  Acc:  84.375  Best Score of:  0.9914942\n",
      "250 out of:  2041  With Loss of:  0.48057666420936584  Acc:  81.25  Best Score of:  0.9714921\n",
      "251 out of:  2041  With Loss of:  0.7162487506866455  Acc:  79.6875  Best Score of:  0.99124676\n",
      "252 out of:  2041  With Loss of:  0.4366908073425293  Acc:  87.5  Best Score of:  0.9426216\n",
      "253 out of:  2041  With Loss of:  0.4956461191177368  Acc:  81.25  Best Score of:  0.98661685\n",
      "254 out of:  2041  With Loss of:  0.3585956394672394  Acc:  90.625  Best Score of:  0.92323536\n",
      "255 out of:  2041  With Loss of:  0.46928098797798157  Acc:  93.75  Best Score of:  0.64698696\n",
      "256 out of:  2041  With Loss of:  0.584459125995636  Acc:  92.1875  Best Score of:  0.84573936\n",
      "257 out of:  2041  With Loss of:  0.46466028690338135  Acc:  96.875  Best Score of:  0.96099335\n",
      "258 out of:  2041  With Loss of:  0.6253631711006165  Acc:  89.0625  Best Score of:  0.99695957\n",
      "259 out of:  2041  With Loss of:  0.4138105809688568  Acc:  95.3125  Best Score of:  0.9382274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260 out of:  2041  With Loss of:  0.4994264841079712  Acc:  84.375  Best Score of:  0.9821862\n",
      "261 out of:  2041  With Loss of:  0.629545271396637  Acc:  73.4375  Best Score of:  0.99884987\n",
      "262 out of:  2041  With Loss of:  0.592332124710083  Acc:  64.0625  Best Score of:  0.9997265\n",
      "263 out of:  2041  With Loss of:  0.4294728636741638  Acc:  68.75  Best Score of:  0.99533963\n",
      "264 out of:  2041  With Loss of:  0.4726932644844055  Acc:  64.0625  Best Score of:  0.99976665\n",
      "265 out of:  2041  With Loss of:  0.7706127166748047  Acc:  67.1875  Best Score of:  0.9888636\n",
      "266 out of:  2041  With Loss of:  0.5069707036018372  Acc:  78.125  Best Score of:  0.97740763\n",
      "267 out of:  2041  With Loss of:  0.6662675738334656  Acc:  92.1875  Best Score of:  0.65820074\n",
      "268 out of:  2041  With Loss of:  0.6456685066223145  Acc:  95.3125  Best Score of:  0.6947252\n",
      "269 out of:  2041  With Loss of:  0.4146828055381775  Acc:  98.4375  Best Score of:  0.77422595\n",
      "270 out of:  2041  With Loss of:  0.4774835407733917  Acc:  93.75  Best Score of:  0.8094303\n",
      "271 out of:  2041  With Loss of:  0.5087711811065674  Acc:  93.75  Best Score of:  0.940865\n",
      "272 out of:  2041  With Loss of:  0.3661879599094391  Acc:  95.3125  Best Score of:  0.924099\n",
      "273 out of:  2041  With Loss of:  0.4126191735267639  Acc:  92.1875  Best Score of:  0.8705813\n",
      "274 out of:  2041  With Loss of:  0.49730390310287476  Acc:  87.5  Best Score of:  0.96386117\n",
      "275 out of:  2041  With Loss of:  0.427093505859375  Acc:  92.1875  Best Score of:  0.9372731\n",
      "276 out of:  2041  With Loss of:  0.5748074054718018  Acc:  78.125  Best Score of:  0.9650163\n",
      "277 out of:  2041  With Loss of:  0.3298262059688568  Acc:  89.0625  Best Score of:  0.93883246\n",
      "278 out of:  2041  With Loss of:  0.40511345863342285  Acc:  87.5  Best Score of:  0.9631423\n",
      "279 out of:  2041  With Loss of:  0.46264079213142395  Acc:  89.0625  Best Score of:  0.96758366\n",
      "280 out of:  2041  With Loss of:  0.4436497390270233  Acc:  81.25  Best Score of:  0.96752924\n",
      "281 out of:  2041  With Loss of:  0.5279186964035034  Acc:  93.75  Best Score of:  0.97113085\n",
      "282 out of:  2041  With Loss of:  0.6264674663543701  Acc:  93.75  Best Score of:  0.9749069\n",
      "283 out of:  2041  With Loss of:  0.43706801533699036  Acc:  89.0625  Best Score of:  0.9660546\n",
      "284 out of:  2041  With Loss of:  0.4948256015777588  Acc:  87.5  Best Score of:  0.9727078\n",
      "285 out of:  2041  With Loss of:  0.6438443660736084  Acc:  67.1875  Best Score of:  0.9992792\n",
      "286 out of:  2041  With Loss of:  0.6297851800918579  Acc:  62.5  Best Score of:  0.9973912\n",
      "287 out of:  2041  With Loss of:  0.6505255103111267  Acc:  71.875  Best Score of:  0.9997155\n",
      "288 out of:  2041  With Loss of:  0.6796942949295044  Acc:  54.6875  Best Score of:  0.9978242\n",
      "289 out of:  2041  With Loss of:  0.3214624524116516  Acc:  92.1875  Best Score of:  0.9981468\n",
      "290 out of:  2041  With Loss of:  0.40394046902656555  Acc:  93.75  Best Score of:  0.9905008\n",
      "291 out of:  2041  With Loss of:  0.42189377546310425  Acc:  92.1875  Best Score of:  0.9834381\n",
      "292 out of:  2041  With Loss of:  0.46636590361595154  Acc:  90.625  Best Score of:  0.74176204\n",
      "293 out of:  2041  With Loss of:  0.43245965242385864  Acc:  87.5  Best Score of:  0.99897516\n",
      "294 out of:  2041  With Loss of:  0.618967592716217  Acc:  84.375  Best Score of:  0.98376346\n",
      "295 out of:  2041  With Loss of:  0.6134437322616577  Acc:  75.0  Best Score of:  0.99989974\n",
      "296 out of:  2041  With Loss of:  0.3754909038543701  Acc:  78.125  Best Score of:  0.99896395\n",
      "297 out of:  2041  With Loss of:  0.47878655791282654  Acc:  68.75  Best Score of:  0.9964194\n",
      "298 out of:  2041  With Loss of:  0.6309337019920349  Acc:  78.125  Best Score of:  0.8890012\n",
      "299 out of:  2041  With Loss of:  0.5627875924110413  Acc:  79.6875  Best Score of:  0.99340075\n",
      "300 out of:  2041  With Loss of:  0.5548443794250488  Acc:  85.9375  Best Score of:  0.8534294\n",
      "301 out of:  2041  With Loss of:  0.3896147906780243  Acc:  89.0625  Best Score of:  0.8489552\n",
      "302 out of:  2041  With Loss of:  0.4875381886959076  Acc:  92.1875  Best Score of:  0.99393153\n",
      "303 out of:  2041  With Loss of:  0.36573487520217896  Acc:  96.875  Best Score of:  0.6844026\n",
      "304 out of:  2041  With Loss of:  0.4942876696586609  Acc:  93.75  Best Score of:  0.8663714\n",
      "305 out of:  2041  With Loss of:  0.7449418902397156  Acc:  82.8125  Best Score of:  0.91618556\n",
      "306 out of:  2041  With Loss of:  0.5113491415977478  Acc:  76.5625  Best Score of:  0.98932916\n",
      "307 out of:  2041  With Loss of:  0.3344612717628479  Acc:  79.6875  Best Score of:  0.99962807\n",
      "308 out of:  2041  With Loss of:  0.40740200877189636  Acc:  76.5625  Best Score of:  0.9950765\n",
      "309 out of:  2041  With Loss of:  0.5905720591545105  Acc:  70.3125  Best Score of:  0.9997613\n",
      "310 out of:  2041  With Loss of:  0.6312848329544067  Acc:  73.4375  Best Score of:  0.99764866\n",
      "311 out of:  2041  With Loss of:  0.3185056447982788  Acc:  90.625  Best Score of:  0.99039334\n",
      "312 out of:  2041  With Loss of:  0.5024521946907043  Acc:  75.0  Best Score of:  0.9602917\n",
      "313 out of:  2041  With Loss of:  0.8002927899360657  Acc:  75.0  Best Score of:  0.9994849\n",
      "314 out of:  2041  With Loss of:  0.3986172080039978  Acc:  82.8125  Best Score of:  0.99895334\n",
      "315 out of:  2041  With Loss of:  0.44865843653678894  Acc:  87.5  Best Score of:  0.98024166\n",
      "316 out of:  2041  With Loss of:  0.4340241551399231  Acc:  92.1875  Best Score of:  0.998343\n",
      "317 out of:  2041  With Loss of:  0.4103076159954071  Acc:  89.0625  Best Score of:  0.99134034\n",
      "318 out of:  2041  With Loss of:  0.28250664472579956  Acc:  93.75  Best Score of:  0.937167\n",
      "319 out of:  2041  With Loss of:  0.3935680091381073  Acc:  90.625  Best Score of:  0.92987955\n",
      "320 out of:  2041  With Loss of:  0.5391491651535034  Acc:  96.875  Best Score of:  0.8471108\n",
      "321 out of:  2041  With Loss of:  0.6825268268585205  Acc:  95.3125  Best Score of:  0.8150661\n",
      "322 out of:  2041  With Loss of:  0.5656659007072449  Acc:  96.875  Best Score of:  0.7260519\n",
      "323 out of:  2041  With Loss of:  0.8524848818778992  Acc:  85.9375  Best Score of:  0.79162097\n",
      "324 out of:  2041  With Loss of:  0.2637869417667389  Acc:  85.9375  Best Score of:  0.96988624\n",
      "325 out of:  2041  With Loss of:  0.3542271554470062  Acc:  87.5  Best Score of:  0.9235881\n",
      "326 out of:  2041  With Loss of:  0.5379868149757385  Acc:  73.4375  Best Score of:  0.9831035\n",
      "327 out of:  2041  With Loss of:  0.5912443399429321  Acc:  76.5625  Best Score of:  0.9130773\n",
      "328 out of:  2041  With Loss of:  0.45211300253868103  Acc:  89.0625  Best Score of:  0.97009844\n",
      "329 out of:  2041  With Loss of:  0.44381406903266907  Acc:  78.125  Best Score of:  0.8831337\n",
      "330 out of:  2041  With Loss of:  0.3415490686893463  Acc:  95.3125  Best Score of:  0.95600843\n",
      "331 out of:  2041  With Loss of:  0.3623715937137604  Acc:  92.1875  Best Score of:  0.82861966\n",
      "332 out of:  2041  With Loss of:  0.6188985109329224  Acc:  85.9375  Best Score of:  0.9048799\n",
      "333 out of:  2041  With Loss of:  0.5907323956489563  Acc:  87.5  Best Score of:  0.963816\n",
      "334 out of:  2041  With Loss of:  0.37287425994873047  Acc:  92.1875  Best Score of:  0.894373\n",
      "335 out of:  2041  With Loss of:  0.36235177516937256  Acc:  87.5  Best Score of:  0.9605811\n",
      "336 out of:  2041  With Loss of:  0.3010326325893402  Acc:  92.1875  Best Score of:  0.9301724\n",
      "337 out of:  2041  With Loss of:  0.4356420636177063  Acc:  90.625  Best Score of:  0.94757205\n",
      "338 out of:  2041  With Loss of:  0.5107966661453247  Acc:  92.1875  Best Score of:  0.91483504\n",
      "339 out of:  2041  With Loss of:  0.44148629903793335  Acc:  89.0625  Best Score of:  0.99716574\n",
      "340 out of:  2041  With Loss of:  0.17518065869808197  Acc:  90.625  Best Score of:  0.99876475\n",
      "341 out of:  2041  With Loss of:  0.4791252315044403  Acc:  84.375  Best Score of:  0.9999883\n",
      "342 out of:  2041  With Loss of:  0.3058073818683624  Acc:  85.9375  Best Score of:  0.9993142\n",
      "343 out of:  2041  With Loss of:  0.20918111503124237  Acc:  89.0625  Best Score of:  0.9972785\n",
      "344 out of:  2041  With Loss of:  0.5693280696868896  Acc:  87.5  Best Score of:  0.9977769\n",
      "345 out of:  2041  With Loss of:  0.564633846282959  Acc:  82.8125  Best Score of:  0.9972031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346 out of:  2041  With Loss of:  0.4532528519630432  Acc:  84.375  Best Score of:  0.9423546\n",
      "347 out of:  2041  With Loss of:  0.297214537858963  Acc:  92.1875  Best Score of:  0.9839901\n",
      "348 out of:  2041  With Loss of:  0.44197866320610046  Acc:  79.6875  Best Score of:  0.94339585\n",
      "349 out of:  2041  With Loss of:  0.42947643995285034  Acc:  84.375  Best Score of:  0.9702804\n",
      "350 out of:  2041  With Loss of:  0.6615301966667175  Acc:  90.625  Best Score of:  0.7933766\n",
      "351 out of:  2041  With Loss of:  0.3158470094203949  Acc:  87.5  Best Score of:  0.9610365\n",
      "352 out of:  2041  With Loss of:  0.35899680852890015  Acc:  90.625  Best Score of:  0.99977785\n",
      "353 out of:  2041  With Loss of:  0.26283663511276245  Acc:  84.375  Best Score of:  0.9999367\n",
      "354 out of:  2041  With Loss of:  0.4863319396972656  Acc:  85.9375  Best Score of:  0.9862629\n",
      "355 out of:  2041  With Loss of:  0.45385533571243286  Acc:  82.8125  Best Score of:  0.9995271\n",
      "356 out of:  2041  With Loss of:  0.3135332763195038  Acc:  84.375  Best Score of:  0.99318933\n",
      "357 out of:  2041  With Loss of:  0.24391722679138184  Acc:  90.625  Best Score of:  0.9848647\n",
      "358 out of:  2041  With Loss of:  0.4098167419433594  Acc:  85.9375  Best Score of:  0.9999386\n",
      "359 out of:  2041  With Loss of:  0.5814986228942871  Acc:  87.5  Best Score of:  0.98567724\n",
      "360 out of:  2041  With Loss of:  0.3553836941719055  Acc:  89.0625  Best Score of:  0.9958599\n",
      "361 out of:  2041  With Loss of:  0.6165878772735596  Acc:  85.9375  Best Score of:  0.959386\n",
      "362 out of:  2041  With Loss of:  0.2959716320037842  Acc:  90.625  Best Score of:  0.99883336\n",
      "363 out of:  2041  With Loss of:  0.4022986590862274  Acc:  82.8125  Best Score of:  0.99009526\n",
      "364 out of:  2041  With Loss of:  0.6290007829666138  Acc:  68.75  Best Score of:  0.9998807\n",
      "365 out of:  2041  With Loss of:  0.9524291753768921  Acc:  48.4375  Best Score of:  0.9998301\n",
      "366 out of:  2041  With Loss of:  0.487224280834198  Acc:  76.5625  Best Score of:  0.99994457\n",
      "367 out of:  2041  With Loss of:  0.4040709137916565  Acc:  76.5625  Best Score of:  0.9974401\n",
      "368 out of:  2041  With Loss of:  0.45493894815444946  Acc:  84.375  Best Score of:  0.9998292\n",
      "369 out of:  2041  With Loss of:  0.36797457933425903  Acc:  96.875  Best Score of:  0.9559675\n",
      "370 out of:  2041  With Loss of:  0.6932165026664734  Acc:  90.625  Best Score of:  0.8848103\n",
      "371 out of:  2041  With Loss of:  0.5112442970275879  Acc:  95.3125  Best Score of:  0.77397907\n",
      "372 out of:  2041  With Loss of:  0.6695598363876343  Acc:  96.875  Best Score of:  0.6376366\n",
      "373 out of:  2041  With Loss of:  0.28013572096824646  Acc:  90.625  Best Score of:  0.9419255\n",
      "374 out of:  2041  With Loss of:  0.36901599168777466  Acc:  89.0625  Best Score of:  0.94928676\n",
      "375 out of:  2041  With Loss of:  0.477042019367218  Acc:  79.6875  Best Score of:  0.9715\n",
      "376 out of:  2041  With Loss of:  0.31213128566741943  Acc:  75.0  Best Score of:  0.9932729\n",
      "377 out of:  2041  With Loss of:  0.5133758187294006  Acc:  81.25  Best Score of:  0.9639705\n",
      "378 out of:  2041  With Loss of:  0.35783347487449646  Acc:  85.9375  Best Score of:  0.9651419\n",
      "379 out of:  2041  With Loss of:  0.47482362389564514  Acc:  73.4375  Best Score of:  0.99746394\n",
      "380 out of:  2041  With Loss of:  0.5247490406036377  Acc:  76.5625  Best Score of:  0.94052327\n",
      "381 out of:  2041  With Loss of:  0.2741200029850006  Acc:  81.25  Best Score of:  0.9935154\n",
      "382 out of:  2041  With Loss of:  0.38158977031707764  Acc:  81.25  Best Score of:  0.99888045\n",
      "383 out of:  2041  With Loss of:  0.3830002546310425  Acc:  85.9375  Best Score of:  0.90833265\n",
      "384 out of:  2041  With Loss of:  0.4357615113258362  Acc:  90.625  Best Score of:  0.90471715\n",
      "385 out of:  2041  With Loss of:  0.33558717370033264  Acc:  96.875  Best Score of:  0.57927626\n",
      "386 out of:  2041  With Loss of:  0.5427573919296265  Acc:  90.625  Best Score of:  0.9549051\n",
      "387 out of:  2041  With Loss of:  0.4685371518135071  Acc:  79.6875  Best Score of:  0.99540627\n",
      "388 out of:  2041  With Loss of:  0.2811580300331116  Acc:  89.0625  Best Score of:  0.99970883\n",
      "389 out of:  2041  With Loss of:  0.6027243137359619  Acc:  68.75  Best Score of:  0.99961025\n",
      "390 out of:  2041  With Loss of:  0.5085641741752625  Acc:  78.125  Best Score of:  0.9885471\n",
      "391 out of:  2041  With Loss of:  0.32973355054855347  Acc:  70.3125  Best Score of:  0.98390144\n",
      "392 out of:  2041  With Loss of:  0.6100882291793823  Acc:  79.6875  Best Score of:  0.9999908\n",
      "393 out of:  2041  With Loss of:  0.3126688599586487  Acc:  79.6875  Best Score of:  0.99264306\n",
      "394 out of:  2041  With Loss of:  0.5447455048561096  Acc:  82.8125  Best Score of:  0.982535\n",
      "395 out of:  2041  With Loss of:  0.6629375219345093  Acc:  85.9375  Best Score of:  0.99772424\n",
      "396 out of:  2041  With Loss of:  0.2253945767879486  Acc:  85.9375  Best Score of:  0.9961875\n",
      "397 out of:  2041  With Loss of:  0.5418505072593689  Acc:  89.0625  Best Score of:  0.9158259\n",
      "398 out of:  2041  With Loss of:  0.316027969121933  Acc:  85.9375  Best Score of:  0.9575676\n",
      "399 out of:  2041  With Loss of:  0.3019815683364868  Acc:  84.375  Best Score of:  0.99943084\n",
      "400 out of:  2041  With Loss of:  0.23652949929237366  Acc:  92.1875  Best Score of:  0.9969963\n",
      "401 out of:  2041  With Loss of:  0.4984559714794159  Acc:  82.8125  Best Score of:  0.97472817\n",
      "402 out of:  2041  With Loss of:  0.6057811379432678  Acc:  89.0625  Best Score of:  0.9235116\n",
      "403 out of:  2041  With Loss of:  0.2580185830593109  Acc:  89.0625  Best Score of:  0.9997366\n",
      "404 out of:  2041  With Loss of:  0.37005874514579773  Acc:  96.875  Best Score of:  0.98586315\n",
      "405 out of:  2041  With Loss of:  0.5350992679595947  Acc:  82.8125  Best Score of:  0.98431927\n",
      "406 out of:  2041  With Loss of:  0.5354792475700378  Acc:  79.6875  Best Score of:  0.956085\n",
      "407 out of:  2041  With Loss of:  0.48346197605133057  Acc:  87.5  Best Score of:  0.8929176\n",
      "408 out of:  2041  With Loss of:  0.44934535026550293  Acc:  81.25  Best Score of:  0.9997266\n",
      "409 out of:  2041  With Loss of:  0.3146178722381592  Acc:  95.3125  Best Score of:  0.9335626\n",
      "410 out of:  2041  With Loss of:  0.6586132645606995  Acc:  78.125  Best Score of:  0.9410523\n",
      "411 out of:  2041  With Loss of:  0.3577847480773926  Acc:  87.5  Best Score of:  0.88047385\n",
      "412 out of:  2041  With Loss of:  0.6602243185043335  Acc:  90.625  Best Score of:  0.88919246\n",
      "413 out of:  2041  With Loss of:  0.4233226180076599  Acc:  84.375  Best Score of:  0.97384137\n",
      "414 out of:  2041  With Loss of:  0.4966195225715637  Acc:  75.0  Best Score of:  0.98273724\n",
      "415 out of:  2041  With Loss of:  0.4479392170906067  Acc:  79.6875  Best Score of:  0.9894721\n",
      "416 out of:  2041  With Loss of:  0.5381547212600708  Acc:  84.375  Best Score of:  0.9934475\n",
      "417 out of:  2041  With Loss of:  0.6566197276115417  Acc:  89.0625  Best Score of:  0.91035193\n",
      "418 out of:  2041  With Loss of:  0.2945064902305603  Acc:  95.3125  Best Score of:  0.92475003\n",
      "419 out of:  2041  With Loss of:  0.23241174221038818  Acc:  95.3125  Best Score of:  0.72864\n",
      "420 out of:  2041  With Loss of:  0.35374516248703003  Acc:  92.1875  Best Score of:  0.78005445\n",
      "421 out of:  2041  With Loss of:  0.525315523147583  Acc:  89.0625  Best Score of:  0.95067567\n",
      "422 out of:  2041  With Loss of:  0.6625475883483887  Acc:  93.75  Best Score of:  0.7828491\n",
      "423 out of:  2041  With Loss of:  0.4777234196662903  Acc:  90.625  Best Score of:  0.8440289\n",
      "424 out of:  2041  With Loss of:  0.399156779050827  Acc:  81.25  Best Score of:  0.99993503\n",
      "425 out of:  2041  With Loss of:  0.3190353512763977  Acc:  87.5  Best Score of:  0.9613808\n",
      "426 out of:  2041  With Loss of:  0.29420459270477295  Acc:  89.0625  Best Score of:  0.98901373\n",
      "427 out of:  2041  With Loss of:  0.7001134753227234  Acc:  75.0  Best Score of:  0.99971586\n",
      "428 out of:  2041  With Loss of:  0.47095048427581787  Acc:  73.4375  Best Score of:  0.991411\n",
      "429 out of:  2041  With Loss of:  0.6710797548294067  Acc:  76.5625  Best Score of:  0.991577\n",
      "430 out of:  2041  With Loss of:  0.23564957082271576  Acc:  84.375  Best Score of:  0.9959214\n",
      "431 out of:  2041  With Loss of:  0.43692129850387573  Acc:  79.6875  Best Score of:  0.9933622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432 out of:  2041  With Loss of:  0.3681561350822449  Acc:  87.5  Best Score of:  0.9930957\n",
      "433 out of:  2041  With Loss of:  0.8353694081306458  Acc:  85.9375  Best Score of:  0.9859736\n",
      "434 out of:  2041  With Loss of:  0.24965697526931763  Acc:  95.3125  Best Score of:  0.9475817\n",
      "435 out of:  2041  With Loss of:  0.4807330369949341  Acc:  85.9375  Best Score of:  0.98353475\n",
      "436 out of:  2041  With Loss of:  0.28817206621170044  Acc:  87.5  Best Score of:  0.9977806\n",
      "437 out of:  2041  With Loss of:  0.3956453502178192  Acc:  78.125  Best Score of:  0.99691796\n",
      "438 out of:  2041  With Loss of:  0.39699843525886536  Acc:  64.0625  Best Score of:  0.9999069\n",
      "439 out of:  2041  With Loss of:  0.38655251264572144  Acc:  75.0  Best Score of:  0.99893147\n",
      "440 out of:  2041  With Loss of:  0.47094282507896423  Acc:  75.0  Best Score of:  0.9992656\n",
      "441 out of:  2041  With Loss of:  0.3617097735404968  Acc:  87.5  Best Score of:  0.9819005\n",
      "442 out of:  2041  With Loss of:  0.3469240069389343  Acc:  85.9375  Best Score of:  0.9927805\n",
      "443 out of:  2041  With Loss of:  0.36019590497016907  Acc:  90.625  Best Score of:  0.99718386\n",
      "444 out of:  2041  With Loss of:  0.25938889384269714  Acc:  87.5  Best Score of:  0.9991591\n",
      "445 out of:  2041  With Loss of:  0.11098197102546692  Acc:  98.4375  Best Score of:  0.9572544\n",
      "446 out of:  2041  With Loss of:  0.7974839210510254  Acc:  95.3125  Best Score of:  0.8744676\n",
      "447 out of:  2041  With Loss of:  0.4157637357711792  Acc:  93.75  Best Score of:  0.8691997\n",
      "448 out of:  2041  With Loss of:  0.6502794027328491  Acc:  95.3125  Best Score of:  0.87716633\n",
      "449 out of:  2041  With Loss of:  0.5446178317070007  Acc:  90.625  Best Score of:  0.9990482\n",
      "450 out of:  2041  With Loss of:  0.3024328351020813  Acc:  89.0625  Best Score of:  0.94889975\n",
      "451 out of:  2041  With Loss of:  0.2617812752723694  Acc:  87.5  Best Score of:  0.99940836\n",
      "452 out of:  2041  With Loss of:  0.5356881022453308  Acc:  78.125  Best Score of:  0.99980694\n",
      "453 out of:  2041  With Loss of:  0.43883177638053894  Acc:  81.25  Best Score of:  0.99385035\n",
      "454 out of:  2041  With Loss of:  0.7157195210456848  Acc:  73.4375  Best Score of:  0.9942905\n",
      "455 out of:  2041  With Loss of:  0.585443377494812  Acc:  67.1875  Best Score of:  0.9989225\n",
      "456 out of:  2041  With Loss of:  0.43044033646583557  Acc:  79.6875  Best Score of:  0.9992204\n",
      "457 out of:  2041  With Loss of:  0.5437777042388916  Acc:  70.3125  Best Score of:  0.9978446\n",
      "458 out of:  2041  With Loss of:  0.30862826108932495  Acc:  87.5  Best Score of:  0.994456\n",
      "459 out of:  2041  With Loss of:  0.35183483362197876  Acc:  73.4375  Best Score of:  0.9948784\n",
      "460 out of:  2041  With Loss of:  0.46069079637527466  Acc:  81.25  Best Score of:  0.98111707\n",
      "461 out of:  2041  With Loss of:  0.4672882854938507  Acc:  92.1875  Best Score of:  0.99886036\n",
      "462 out of:  2041  With Loss of:  0.6152842044830322  Acc:  87.5  Best Score of:  0.9632073\n",
      "463 out of:  2041  With Loss of:  0.38351181149482727  Acc:  93.75  Best Score of:  0.9504636\n",
      "464 out of:  2041  With Loss of:  0.23886433243751526  Acc:  93.75  Best Score of:  0.9621976\n",
      "465 out of:  2041  With Loss of:  0.284797340631485  Acc:  96.875  Best Score of:  0.58960605\n",
      "466 out of:  2041  With Loss of:  0.19326020777225494  Acc:  95.3125  Best Score of:  0.96824676\n",
      "467 out of:  2041  With Loss of:  0.5582365989685059  Acc:  93.75  Best Score of:  0.85510176\n",
      "468 out of:  2041  With Loss of:  0.4970581531524658  Acc:  93.75  Best Score of:  0.89077425\n",
      "469 out of:  2041  With Loss of:  0.2904171347618103  Acc:  92.1875  Best Score of:  0.9622198\n",
      "470 out of:  2041  With Loss of:  0.5166901350021362  Acc:  89.0625  Best Score of:  0.901688\n",
      "471 out of:  2041  With Loss of:  0.34079423546791077  Acc:  76.5625  Best Score of:  0.99978477\n",
      "472 out of:  2041  With Loss of:  0.3419738709926605  Acc:  75.0  Best Score of:  0.9977976\n",
      "473 out of:  2041  With Loss of:  0.33364003896713257  Acc:  78.125  Best Score of:  0.9998648\n",
      "474 out of:  2041  With Loss of:  0.5654997229576111  Acc:  75.0  Best Score of:  0.9999424\n",
      "475 out of:  2041  With Loss of:  0.3407529592514038  Acc:  71.875  Best Score of:  0.9991321\n",
      "476 out of:  2041  With Loss of:  0.18674592673778534  Acc:  90.625  Best Score of:  0.9935454\n",
      "477 out of:  2041  With Loss of:  0.3000129759311676  Acc:  78.125  Best Score of:  0.999199\n",
      "478 out of:  2041  With Loss of:  0.3489323556423187  Acc:  93.75  Best Score of:  0.96200716\n",
      "479 out of:  2041  With Loss of:  0.598524808883667  Acc:  87.5  Best Score of:  0.9917548\n",
      "480 out of:  2041  With Loss of:  0.3092738389968872  Acc:  90.625  Best Score of:  0.9824531\n",
      "481 out of:  2041  With Loss of:  0.3903329372406006  Acc:  82.8125  Best Score of:  0.9993555\n",
      "482 out of:  2041  With Loss of:  0.6110489368438721  Acc:  87.5  Best Score of:  0.84033364\n",
      "483 out of:  2041  With Loss of:  0.22484110295772552  Acc:  92.1875  Best Score of:  0.9983388\n",
      "484 out of:  2041  With Loss of:  0.7880610227584839  Acc:  84.375  Best Score of:  0.99874616\n",
      "485 out of:  2041  With Loss of:  0.58255535364151  Acc:  85.9375  Best Score of:  0.9991617\n",
      "486 out of:  2041  With Loss of:  0.6263783574104309  Acc:  79.6875  Best Score of:  0.99496293\n",
      "487 out of:  2041  With Loss of:  0.477901428937912  Acc:  82.8125  Best Score of:  0.99025005\n",
      "488 out of:  2041  With Loss of:  0.20919959247112274  Acc:  90.625  Best Score of:  0.9995695\n",
      "489 out of:  2041  With Loss of:  0.3473038077354431  Acc:  78.125  Best Score of:  0.999933\n",
      "490 out of:  2041  With Loss of:  0.25981462001800537  Acc:  85.9375  Best Score of:  0.9884729\n",
      "491 out of:  2041  With Loss of:  0.3887234330177307  Acc:  84.375  Best Score of:  0.9886057\n",
      "492 out of:  2041  With Loss of:  0.4982295036315918  Acc:  90.625  Best Score of:  0.99918395\n",
      "493 out of:  2041  With Loss of:  0.4790598750114441  Acc:  90.625  Best Score of:  0.9997845\n",
      "494 out of:  2041  With Loss of:  0.24000060558319092  Acc:  95.3125  Best Score of:  0.96353644\n",
      "495 out of:  2041  With Loss of:  0.3469483256340027  Acc:  87.5  Best Score of:  0.929728\n",
      "496 out of:  2041  With Loss of:  0.2592342793941498  Acc:  93.75  Best Score of:  0.8802064\n",
      "497 out of:  2041  With Loss of:  0.6648460626602173  Acc:  87.5  Best Score of:  0.9862856\n",
      "498 out of:  2041  With Loss of:  0.695934534072876  Acc:  89.0625  Best Score of:  0.99771297\n",
      "499 out of:  2041  With Loss of:  0.1947656273841858  Acc:  93.75  Best Score of:  0.995453\n",
      "500 out of:  2041  With Loss of:  0.643582284450531  Acc:  87.5  Best Score of:  0.84653556\n",
      "501 out of:  2041  With Loss of:  0.34914156794548035  Acc:  89.0625  Best Score of:  0.9244131\n",
      "502 out of:  2041  With Loss of:  0.4946191906929016  Acc:  93.75  Best Score of:  0.84126085\n",
      "503 out of:  2041  With Loss of:  0.3920760750770569  Acc:  84.375  Best Score of:  0.99911565\n",
      "504 out of:  2041  With Loss of:  0.33735573291778564  Acc:  87.5  Best Score of:  0.99358845\n",
      "505 out of:  2041  With Loss of:  0.4948115944862366  Acc:  90.625  Best Score of:  0.90806574\n",
      "506 out of:  2041  With Loss of:  0.34504956007003784  Acc:  81.25  Best Score of:  0.98697156\n",
      "507 out of:  2041  With Loss of:  0.47142642736434937  Acc:  87.5  Best Score of:  0.98956394\n",
      "508 out of:  2041  With Loss of:  0.38015004992485046  Acc:  73.4375  Best Score of:  0.99864227\n",
      "509 out of:  2041  With Loss of:  0.5793330669403076  Acc:  81.25  Best Score of:  0.9959187\n",
      "510 out of:  2041  With Loss of:  0.3488995432853699  Acc:  78.125  Best Score of:  0.9321615\n",
      "511 out of:  2041  With Loss of:  0.3313033878803253  Acc:  81.25  Best Score of:  0.9990375\n",
      "512 out of:  2041  With Loss of:  0.45274078845977783  Acc:  78.125  Best Score of:  0.98975205\n",
      "513 out of:  2041  With Loss of:  0.3520185351371765  Acc:  84.375  Best Score of:  0.9967378\n",
      "514 out of:  2041  With Loss of:  0.973164439201355  Acc:  81.25  Best Score of:  0.99778986\n",
      "515 out of:  2041  With Loss of:  0.2875094711780548  Acc:  89.0625  Best Score of:  0.99650335\n",
      "516 out of:  2041  With Loss of:  0.38706082105636597  Acc:  81.25  Best Score of:  0.9720853\n",
      "517 out of:  2041  With Loss of:  0.430469274520874  Acc:  85.9375  Best Score of:  0.98523235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518 out of:  2041  With Loss of:  0.5648680925369263  Acc:  85.9375  Best Score of:  0.9796143\n",
      "519 out of:  2041  With Loss of:  0.20378394424915314  Acc:  89.0625  Best Score of:  0.9793133\n",
      "520 out of:  2041  With Loss of:  0.4246576130390167  Acc:  90.625  Best Score of:  0.99664706\n",
      "521 out of:  2041  With Loss of:  0.2862871289253235  Acc:  92.1875  Best Score of:  0.99717087\n",
      "522 out of:  2041  With Loss of:  0.33644986152648926  Acc:  84.375  Best Score of:  0.99202406\n",
      "523 out of:  2041  With Loss of:  0.3810104727745056  Acc:  87.5  Best Score of:  0.99430263\n",
      "524 out of:  2041  With Loss of:  0.28380709886550903  Acc:  90.625  Best Score of:  0.925777\n",
      "525 out of:  2041  With Loss of:  0.4355870485305786  Acc:  95.3125  Best Score of:  0.9992817\n",
      "526 out of:  2041  With Loss of:  0.3138003349304199  Acc:  78.125  Best Score of:  0.9999417\n",
      "527 out of:  2041  With Loss of:  0.25606122612953186  Acc:  93.75  Best Score of:  0.9995851\n",
      "528 out of:  2041  With Loss of:  0.2395121157169342  Acc:  87.5  Best Score of:  0.9997881\n",
      "529 out of:  2041  With Loss of:  0.306677907705307  Acc:  84.375  Best Score of:  0.9997998\n",
      "530 out of:  2041  With Loss of:  0.4842934012413025  Acc:  85.9375  Best Score of:  0.93010235\n",
      "531 out of:  2041  With Loss of:  0.4987615644931793  Acc:  84.375  Best Score of:  0.92260087\n",
      "532 out of:  2041  With Loss of:  0.3254603147506714  Acc:  82.8125  Best Score of:  0.99998415\n",
      "533 out of:  2041  With Loss of:  0.580803394317627  Acc:  89.0625  Best Score of:  1.0\n",
      "534 out of:  2041  With Loss of:  0.3625398874282837  Acc:  81.25  Best Score of:  0.9990615\n",
      "535 out of:  2041  With Loss of:  0.33776167035102844  Acc:  75.0  Best Score of:  0.99955946\n",
      "536 out of:  2041  With Loss of:  0.35467854142189026  Acc:  84.375  Best Score of:  0.99971944\n",
      "537 out of:  2041  With Loss of:  0.19808350503444672  Acc:  90.625  Best Score of:  0.9927481\n",
      "538 out of:  2041  With Loss of:  0.37326401472091675  Acc:  90.625  Best Score of:  0.7335429\n",
      "539 out of:  2041  With Loss of:  0.3130694329738617  Acc:  93.75  Best Score of:  0.73494476\n",
      "540 out of:  2041  With Loss of:  0.5159652829170227  Acc:  78.125  Best Score of:  0.99926955\n",
      "541 out of:  2041  With Loss of:  0.7267026901245117  Acc:  82.8125  Best Score of:  0.94495827\n",
      "542 out of:  2041  With Loss of:  0.36624622344970703  Acc:  79.6875  Best Score of:  0.9899451\n",
      "543 out of:  2041  With Loss of:  0.3254702091217041  Acc:  76.5625  Best Score of:  0.9979583\n",
      "544 out of:  2041  With Loss of:  0.3993322551250458  Acc:  81.25  Best Score of:  0.99690163\n",
      "545 out of:  2041  With Loss of:  0.435186505317688  Acc:  79.6875  Best Score of:  0.99751794\n",
      "546 out of:  2041  With Loss of:  0.3177712857723236  Acc:  84.375  Best Score of:  0.9948598\n",
      "547 out of:  2041  With Loss of:  0.3354308009147644  Acc:  89.0625  Best Score of:  0.9531728\n",
      "548 out of:  2041  With Loss of:  0.4702119827270508  Acc:  90.625  Best Score of:  0.9448815\n",
      "549 out of:  2041  With Loss of:  0.49583399295806885  Acc:  89.0625  Best Score of:  0.9824279\n",
      "550 out of:  2041  With Loss of:  0.5999512076377869  Acc:  87.5  Best Score of:  0.95800596\n",
      "551 out of:  2041  With Loss of:  0.5032334923744202  Acc:  78.125  Best Score of:  0.9962863\n",
      "552 out of:  2041  With Loss of:  0.2728790044784546  Acc:  82.8125  Best Score of:  0.99742705\n",
      "553 out of:  2041  With Loss of:  0.5168946981430054  Acc:  81.25  Best Score of:  0.99792\n",
      "554 out of:  2041  With Loss of:  0.2795339822769165  Acc:  93.75  Best Score of:  0.8976461\n",
      "555 out of:  2041  With Loss of:  0.529026210308075  Acc:  82.8125  Best Score of:  0.99312794\n",
      "556 out of:  2041  With Loss of:  0.2985095679759979  Acc:  92.1875  Best Score of:  0.96972233\n",
      "557 out of:  2041  With Loss of:  0.3167795240879059  Acc:  87.5  Best Score of:  0.9996333\n",
      "558 out of:  2041  With Loss of:  0.409086138010025  Acc:  87.5  Best Score of:  0.9280599\n",
      "559 out of:  2041  With Loss of:  0.24050351977348328  Acc:  87.5  Best Score of:  0.990272\n",
      "560 out of:  2041  With Loss of:  0.34896597266197205  Acc:  81.25  Best Score of:  0.98840386\n",
      "561 out of:  2041  With Loss of:  0.25848138332366943  Acc:  82.8125  Best Score of:  0.98732835\n",
      "562 out of:  2041  With Loss of:  0.6472743153572083  Acc:  84.375  Best Score of:  0.99569607\n",
      "563 out of:  2041  With Loss of:  0.47049853205680847  Acc:  89.0625  Best Score of:  0.9430731\n",
      "564 out of:  2041  With Loss of:  0.4142754077911377  Acc:  82.8125  Best Score of:  0.98468256\n",
      "565 out of:  2041  With Loss of:  0.34700268507003784  Acc:  76.5625  Best Score of:  0.9758808\n",
      "566 out of:  2041  With Loss of:  0.40505656599998474  Acc:  87.5  Best Score of:  0.9849936\n",
      "567 out of:  2041  With Loss of:  0.3448067903518677  Acc:  84.375  Best Score of:  0.98822\n",
      "568 out of:  2041  With Loss of:  0.3617589473724365  Acc:  82.8125  Best Score of:  0.98167795\n",
      "569 out of:  2041  With Loss of:  0.4594690501689911  Acc:  79.6875  Best Score of:  0.9303429\n",
      "570 out of:  2041  With Loss of:  0.36541903018951416  Acc:  92.1875  Best Score of:  0.9890593\n",
      "571 out of:  2041  With Loss of:  0.3542320132255554  Acc:  89.0625  Best Score of:  0.9919184\n",
      "572 out of:  2041  With Loss of:  0.3750150799751282  Acc:  84.375  Best Score of:  0.97756106\n",
      "573 out of:  2041  With Loss of:  0.32939550280570984  Acc:  87.5  Best Score of:  0.9767049\n",
      "574 out of:  2041  With Loss of:  0.4943809509277344  Acc:  87.5  Best Score of:  0.98339003\n",
      "575 out of:  2041  With Loss of:  0.5821145176887512  Acc:  90.625  Best Score of:  0.7100738\n",
      "576 out of:  2041  With Loss of:  0.7485267519950867  Acc:  84.375  Best Score of:  0.9812749\n",
      "577 out of:  2041  With Loss of:  0.4137905538082123  Acc:  89.0625  Best Score of:  0.9923718\n",
      "578 out of:  2041  With Loss of:  0.5276646018028259  Acc:  62.5  Best Score of:  0.99995923\n",
      "579 out of:  2041  With Loss of:  0.3193119168281555  Acc:  68.75  Best Score of:  0.9865238\n",
      "580 out of:  2041  With Loss of:  0.4817100167274475  Acc:  81.25  Best Score of:  0.9970028\n",
      "581 out of:  2041  With Loss of:  0.34291326999664307  Acc:  76.5625  Best Score of:  0.99931216\n",
      "582 out of:  2041  With Loss of:  0.4160315990447998  Acc:  87.5  Best Score of:  0.89169234\n",
      "583 out of:  2041  With Loss of:  0.30131441354751587  Acc:  81.25  Best Score of:  0.9944404\n",
      "584 out of:  2041  With Loss of:  0.39095067977905273  Acc:  81.25  Best Score of:  0.9475117\n",
      "585 out of:  2041  With Loss of:  0.4520505666732788  Acc:  81.25  Best Score of:  0.9803042\n",
      "586 out of:  2041  With Loss of:  0.4830308258533478  Acc:  79.6875  Best Score of:  0.99979025\n",
      "587 out of:  2041  With Loss of:  0.26792699098587036  Acc:  87.5  Best Score of:  0.9958754\n",
      "588 out of:  2041  With Loss of:  0.28314992785453796  Acc:  92.1875  Best Score of:  0.8597618\n",
      "589 out of:  2041  With Loss of:  0.45931166410446167  Acc:  90.625  Best Score of:  0.8818798\n",
      "590 out of:  2041  With Loss of:  0.23056408762931824  Acc:  92.1875  Best Score of:  0.99927443\n",
      "591 out of:  2041  With Loss of:  0.4219249486923218  Acc:  81.25  Best Score of:  0.9925005\n",
      "592 out of:  2041  With Loss of:  0.19621768593788147  Acc:  98.4375  Best Score of:  0.98895276\n",
      "593 out of:  2041  With Loss of:  0.29964444041252136  Acc:  89.0625  Best Score of:  0.8879444\n",
      "594 out of:  2041  With Loss of:  0.5657157897949219  Acc:  93.75  Best Score of:  0.9891238\n",
      "595 out of:  2041  With Loss of:  0.45963382720947266  Acc:  78.125  Best Score of:  0.99959046\n",
      "596 out of:  2041  With Loss of:  0.8066130876541138  Acc:  82.8125  Best Score of:  0.94086283\n",
      "597 out of:  2041  With Loss of:  0.6791697144508362  Acc:  73.4375  Best Score of:  0.9871405\n",
      "598 out of:  2041  With Loss of:  0.4585742950439453  Acc:  73.4375  Best Score of:  0.99930716\n",
      "599 out of:  2041  With Loss of:  0.394843727350235  Acc:  81.25  Best Score of:  0.9893055\n",
      "600 out of:  2041  With Loss of:  0.33537277579307556  Acc:  90.625  Best Score of:  0.99608165\n",
      "601 out of:  2041  With Loss of:  0.6840158700942993  Acc:  84.375  Best Score of:  0.976484\n",
      "602 out of:  2041  With Loss of:  0.2986873686313629  Acc:  92.1875  Best Score of:  0.9617774\n",
      "603 out of:  2041  With Loss of:  0.3201882839202881  Acc:  89.0625  Best Score of:  0.9671537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604 out of:  2041  With Loss of:  0.3073616921901703  Acc:  93.75  Best Score of:  0.90843916\n",
      "605 out of:  2041  With Loss of:  0.39757663011550903  Acc:  82.8125  Best Score of:  0.9508864\n",
      "606 out of:  2041  With Loss of:  0.5469489693641663  Acc:  84.375  Best Score of:  0.9057373\n",
      "607 out of:  2041  With Loss of:  0.3844195604324341  Acc:  90.625  Best Score of:  0.69376737\n",
      "608 out of:  2041  With Loss of:  0.2475457489490509  Acc:  81.25  Best Score of:  0.9704709\n",
      "609 out of:  2041  With Loss of:  0.4157937169075012  Acc:  81.25  Best Score of:  0.8731809\n",
      "610 out of:  2041  With Loss of:  0.43031060695648193  Acc:  79.6875  Best Score of:  0.9943948\n",
      "611 out of:  2041  With Loss of:  0.43576836585998535  Acc:  82.8125  Best Score of:  0.9973978\n",
      "612 out of:  2041  With Loss of:  0.4888651371002197  Acc:  76.5625  Best Score of:  0.98134345\n",
      "613 out of:  2041  With Loss of:  0.2784624695777893  Acc:  76.5625  Best Score of:  0.99761367\n",
      "614 out of:  2041  With Loss of:  0.4464101195335388  Acc:  78.125  Best Score of:  0.8485638\n",
      "615 out of:  2041  With Loss of:  0.36212554574012756  Acc:  85.9375  Best Score of:  0.99105513\n",
      "616 out of:  2041  With Loss of:  0.5030816197395325  Acc:  82.8125  Best Score of:  0.97784024\n",
      "617 out of:  2041  With Loss of:  0.3650544583797455  Acc:  82.8125  Best Score of:  0.9813726\n",
      "618 out of:  2041  With Loss of:  0.6270610690116882  Acc:  84.375  Best Score of:  0.97273886\n",
      "619 out of:  2041  With Loss of:  0.4120854437351227  Acc:  89.0625  Best Score of:  0.9484515\n",
      "620 out of:  2041  With Loss of:  0.27548152208328247  Acc:  81.25  Best Score of:  0.9882742\n",
      "621 out of:  2041  With Loss of:  0.38791516423225403  Acc:  89.0625  Best Score of:  0.94805974\n",
      "622 out of:  2041  With Loss of:  0.28829070925712585  Acc:  87.5  Best Score of:  0.997696\n",
      "623 out of:  2041  With Loss of:  0.5558420419692993  Acc:  76.5625  Best Score of:  0.99112916\n",
      "624 out of:  2041  With Loss of:  0.31124231219291687  Acc:  78.125  Best Score of:  0.9999217\n",
      "625 out of:  2041  With Loss of:  0.18002775311470032  Acc:  89.0625  Best Score of:  0.97610575\n",
      "626 out of:  2041  With Loss of:  0.31031638383865356  Acc:  87.5  Best Score of:  0.99396497\n",
      "627 out of:  2041  With Loss of:  0.3180199861526489  Acc:  95.3125  Best Score of:  0.97147614\n",
      "628 out of:  2041  With Loss of:  0.3866913914680481  Acc:  90.625  Best Score of:  0.91845983\n",
      "629 out of:  2041  With Loss of:  0.5422131419181824  Acc:  85.9375  Best Score of:  0.9343967\n",
      "630 out of:  2041  With Loss of:  0.46728023886680603  Acc:  75.0  Best Score of:  0.98462754\n",
      "631 out of:  2041  With Loss of:  0.1440613716840744  Acc:  96.875  Best Score of:  0.99763846\n",
      "632 out of:  2041  With Loss of:  0.7006964683532715  Acc:  90.625  Best Score of:  0.99355936\n",
      "633 out of:  2041  With Loss of:  0.2584497630596161  Acc:  84.375  Best Score of:  0.9846709\n",
      "634 out of:  2041  With Loss of:  0.2926383316516876  Acc:  89.0625  Best Score of:  0.99777967\n",
      "635 out of:  2041  With Loss of:  0.46980586647987366  Acc:  92.1875  Best Score of:  0.980594\n",
      "636 out of:  2041  With Loss of:  0.6140276789665222  Acc:  92.1875  Best Score of:  0.6122264\n",
      "637 out of:  2041  With Loss of:  0.38083770871162415  Acc:  89.0625  Best Score of:  0.998376\n",
      "638 out of:  2041  With Loss of:  0.4941818118095398  Acc:  75.0  Best Score of:  0.9952356\n",
      "639 out of:  2041  With Loss of:  0.29420650005340576  Acc:  85.9375  Best Score of:  0.9951449\n",
      "640 out of:  2041  With Loss of:  0.47956421971321106  Acc:  84.375  Best Score of:  0.9990823\n",
      "641 out of:  2041  With Loss of:  0.5218715071678162  Acc:  89.0625  Best Score of:  0.98238933\n",
      "642 out of:  2041  With Loss of:  0.43212613463401794  Acc:  81.25  Best Score of:  0.9766908\n",
      "643 out of:  2041  With Loss of:  0.435669481754303  Acc:  73.4375  Best Score of:  0.99730456\n",
      "644 out of:  2041  With Loss of:  0.34990954399108887  Acc:  87.5  Best Score of:  0.90906644\n",
      "645 out of:  2041  With Loss of:  0.28120484948158264  Acc:  82.8125  Best Score of:  0.9989267\n",
      "646 out of:  2041  With Loss of:  0.24742205440998077  Acc:  95.3125  Best Score of:  0.8626345\n",
      "647 out of:  2041  With Loss of:  0.5483097434043884  Acc:  92.1875  Best Score of:  0.9940805\n",
      "648 out of:  2041  With Loss of:  0.4645141363143921  Acc:  76.5625  Best Score of:  0.97382516\n",
      "649 out of:  2041  With Loss of:  0.25207334756851196  Acc:  90.625  Best Score of:  0.98563033\n",
      "650 out of:  2041  With Loss of:  0.3908044695854187  Acc:  90.625  Best Score of:  0.98748094\n",
      "651 out of:  2041  With Loss of:  0.32593828439712524  Acc:  89.0625  Best Score of:  0.9524684\n",
      "652 out of:  2041  With Loss of:  0.3340628147125244  Acc:  90.625  Best Score of:  0.80727124\n",
      "653 out of:  2041  With Loss of:  0.3463261127471924  Acc:  87.5  Best Score of:  0.9226668\n",
      "654 out of:  2041  With Loss of:  0.3101707100868225  Acc:  96.875  Best Score of:  0.83386344\n",
      "655 out of:  2041  With Loss of:  0.37442153692245483  Acc:  82.8125  Best Score of:  0.99509513\n",
      "656 out of:  2041  With Loss of:  0.23996379971504211  Acc:  92.1875  Best Score of:  0.98181677\n",
      "657 out of:  2041  With Loss of:  0.3682188391685486  Acc:  89.0625  Best Score of:  0.9920244\n",
      "658 out of:  2041  With Loss of:  0.2810535728931427  Acc:  87.5  Best Score of:  0.99939907\n",
      "659 out of:  2041  With Loss of:  0.21413448452949524  Acc:  98.4375  Best Score of:  0.86505413\n",
      "660 out of:  2041  With Loss of:  0.6746699810028076  Acc:  90.625  Best Score of:  0.9979254\n",
      "661 out of:  2041  With Loss of:  0.36973682045936584  Acc:  92.1875  Best Score of:  0.96469283\n",
      "662 out of:  2041  With Loss of:  0.3584403395652771  Acc:  87.5  Best Score of:  0.9935912\n",
      "663 out of:  2041  With Loss of:  0.6023278832435608  Acc:  78.125  Best Score of:  0.99996877\n",
      "664 out of:  2041  With Loss of:  0.17065834999084473  Acc:  90.625  Best Score of:  0.99962413\n",
      "665 out of:  2041  With Loss of:  0.17362505197525024  Acc:  89.0625  Best Score of:  0.998061\n",
      "666 out of:  2041  With Loss of:  0.4669331908226013  Acc:  76.5625  Best Score of:  0.9999975\n",
      "667 out of:  2041  With Loss of:  0.405628502368927  Acc:  82.8125  Best Score of:  0.9999887\n",
      "668 out of:  2041  With Loss of:  0.40522485971450806  Acc:  79.6875  Best Score of:  0.9777533\n",
      "669 out of:  2041  With Loss of:  0.3191674053668976  Acc:  84.375  Best Score of:  0.9996319\n",
      "670 out of:  2041  With Loss of:  0.6729989051818848  Acc:  76.5625  Best Score of:  0.9968388\n",
      "671 out of:  2041  With Loss of:  0.5774917006492615  Acc:  89.0625  Best Score of:  0.98209375\n",
      "672 out of:  2041  With Loss of:  0.32156142592430115  Acc:  87.5  Best Score of:  0.9339394\n",
      "673 out of:  2041  With Loss of:  0.5197806358337402  Acc:  82.8125  Best Score of:  0.9522184\n",
      "674 out of:  2041  With Loss of:  0.22608445584774017  Acc:  93.75  Best Score of:  0.9559534\n",
      "675 out of:  2041  With Loss of:  0.5352283716201782  Acc:  84.375  Best Score of:  0.89503384\n",
      "676 out of:  2041  With Loss of:  0.32792845368385315  Acc:  76.5625  Best Score of:  0.9855165\n",
      "677 out of:  2041  With Loss of:  0.2815327048301697  Acc:  92.1875  Best Score of:  0.9984394\n",
      "678 out of:  2041  With Loss of:  0.2544900178909302  Acc:  92.1875  Best Score of:  0.9153366\n",
      "679 out of:  2041  With Loss of:  0.48217466473579407  Acc:  93.75  Best Score of:  0.99617296\n",
      "680 out of:  2041  With Loss of:  0.311818391084671  Acc:  82.8125  Best Score of:  0.9944062\n",
      "681 out of:  2041  With Loss of:  0.29574233293533325  Acc:  95.3125  Best Score of:  0.99831945\n",
      "682 out of:  2041  With Loss of:  0.524641215801239  Acc:  96.875  Best Score of:  0.8591085\n",
      "683 out of:  2041  With Loss of:  0.3815809488296509  Acc:  92.1875  Best Score of:  0.58091235\n",
      "684 out of:  2041  With Loss of:  0.5765867829322815  Acc:  87.5  Best Score of:  0.9965127\n",
      "685 out of:  2041  With Loss of:  0.38434842228889465  Acc:  93.75  Best Score of:  0.80784553\n",
      "686 out of:  2041  With Loss of:  0.2689022123813629  Acc:  87.5  Best Score of:  0.9991499\n",
      "687 out of:  2041  With Loss of:  0.685417652130127  Acc:  82.8125  Best Score of:  0.9998406\n",
      "688 out of:  2041  With Loss of:  0.40104231238365173  Acc:  81.25  Best Score of:  0.9996076\n",
      "689 out of:  2041  With Loss of:  0.27868369221687317  Acc:  81.25  Best Score of:  0.99996233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690 out of:  2041  With Loss of:  0.29949551820755005  Acc:  76.5625  Best Score of:  0.99287015\n",
      "691 out of:  2041  With Loss of:  0.40103963017463684  Acc:  81.25  Best Score of:  0.96923864\n",
      "692 out of:  2041  With Loss of:  0.37752509117126465  Acc:  89.0625  Best Score of:  0.8903845\n",
      "693 out of:  2041  With Loss of:  0.35929441452026367  Acc:  78.125  Best Score of:  0.9907967\n",
      "694 out of:  2041  With Loss of:  0.48023006319999695  Acc:  84.375  Best Score of:  0.9800007\n",
      "695 out of:  2041  With Loss of:  0.25915876030921936  Acc:  79.6875  Best Score of:  0.9947919\n",
      "696 out of:  2041  With Loss of:  0.2559148967266083  Acc:  90.625  Best Score of:  0.98980695\n",
      "697 out of:  2041  With Loss of:  0.30162906646728516  Acc:  76.5625  Best Score of:  0.99998903\n",
      "698 out of:  2041  With Loss of:  0.23357263207435608  Acc:  93.75  Best Score of:  0.9840545\n",
      "699 out of:  2041  With Loss of:  0.3420146405696869  Acc:  93.75  Best Score of:  0.9858636\n",
      "700 out of:  2041  With Loss of:  1.1503199338912964  Acc:  85.9375  Best Score of:  0.9942352\n",
      "701 out of:  2041  With Loss of:  0.38439860939979553  Acc:  92.1875  Best Score of:  0.9569505\n",
      "702 out of:  2041  With Loss of:  0.2038680911064148  Acc:  90.625  Best Score of:  0.9854204\n",
      "703 out of:  2041  With Loss of:  0.36177751421928406  Acc:  71.875  Best Score of:  0.99685967\n",
      "704 out of:  2041  With Loss of:  0.2563144564628601  Acc:  92.1875  Best Score of:  0.96519876\n",
      "705 out of:  2041  With Loss of:  0.33381029963493347  Acc:  76.5625  Best Score of:  0.9986883\n",
      "706 out of:  2041  With Loss of:  0.44331464171409607  Acc:  89.0625  Best Score of:  0.9940891\n",
      "707 out of:  2041  With Loss of:  0.2956216037273407  Acc:  79.6875  Best Score of:  0.99654883\n",
      "708 out of:  2041  With Loss of:  0.5707336664199829  Acc:  68.75  Best Score of:  0.9993291\n",
      "709 out of:  2041  With Loss of:  0.5758897066116333  Acc:  70.3125  Best Score of:  0.99893457\n",
      "710 out of:  2041  With Loss of:  0.4897051453590393  Acc:  90.625  Best Score of:  0.9918452\n",
      "711 out of:  2041  With Loss of:  0.1776902675628662  Acc:  93.75  Best Score of:  0.8158699\n",
      "712 out of:  2041  With Loss of:  0.32264024019241333  Acc:  90.625  Best Score of:  0.974756\n",
      "713 out of:  2041  With Loss of:  0.20509764552116394  Acc:  93.75  Best Score of:  0.99937314\n",
      "714 out of:  2041  With Loss of:  0.4793669581413269  Acc:  87.5  Best Score of:  0.97541577\n",
      "715 out of:  2041  With Loss of:  0.7652016282081604  Acc:  90.625  Best Score of:  0.9208564\n",
      "716 out of:  2041  With Loss of:  0.48481130599975586  Acc:  95.3125  Best Score of:  0.85509074\n",
      "717 out of:  2041  With Loss of:  0.5366857647895813  Acc:  87.5  Best Score of:  0.96173185\n",
      "718 out of:  2041  With Loss of:  0.3111439347267151  Acc:  87.5  Best Score of:  0.9976412\n",
      "719 out of:  2041  With Loss of:  0.41326087713241577  Acc:  87.5  Best Score of:  0.9632834\n",
      "720 out of:  2041  With Loss of:  0.29774224758148193  Acc:  78.125  Best Score of:  0.99981993\n",
      "721 out of:  2041  With Loss of:  0.49755460023880005  Acc:  75.0  Best Score of:  0.996979\n",
      "722 out of:  2041  With Loss of:  0.7499256134033203  Acc:  68.75  Best Score of:  0.9993554\n",
      "723 out of:  2041  With Loss of:  0.4013059735298157  Acc:  76.5625  Best Score of:  0.98618513\n",
      "724 out of:  2041  With Loss of:  0.3722573518753052  Acc:  82.8125  Best Score of:  0.9453383\n",
      "725 out of:  2041  With Loss of:  0.41421616077423096  Acc:  73.4375  Best Score of:  0.99923515\n",
      "726 out of:  2041  With Loss of:  0.6578088998794556  Acc:  70.3125  Best Score of:  0.9265346\n",
      "727 out of:  2041  With Loss of:  0.3987918496131897  Acc:  71.875  Best Score of:  0.9842248\n",
      "728 out of:  2041  With Loss of:  0.33897632360458374  Acc:  76.5625  Best Score of:  0.98685515\n",
      "729 out of:  2041  With Loss of:  0.2849622070789337  Acc:  82.8125  Best Score of:  0.99604493\n",
      "730 out of:  2041  With Loss of:  0.2712791860103607  Acc:  87.5  Best Score of:  0.97190416\n",
      "731 out of:  2041  With Loss of:  0.1621454954147339  Acc:  95.3125  Best Score of:  0.9942075\n",
      "732 out of:  2041  With Loss of:  0.6586400270462036  Acc:  92.1875  Best Score of:  0.7438098\n",
      "733 out of:  2041  With Loss of:  0.45178329944610596  Acc:  92.1875  Best Score of:  0.68256366\n",
      "734 out of:  2041  With Loss of:  0.3023326098918915  Acc:  96.875  Best Score of:  0.5182217\n",
      "735 out of:  2041  With Loss of:  0.34776097536087036  Acc:  92.1875  Best Score of:  0.8122092\n",
      "736 out of:  2041  With Loss of:  0.6767861247062683  Acc:  90.625  Best Score of:  0.9699395\n",
      "737 out of:  2041  With Loss of:  0.3166060745716095  Acc:  93.75  Best Score of:  0.9798884\n",
      "738 out of:  2041  With Loss of:  0.21367964148521423  Acc:  95.3125  Best Score of:  0.8502755\n",
      "739 out of:  2041  With Loss of:  0.23250900208950043  Acc:  90.625  Best Score of:  0.93728983\n",
      "740 out of:  2041  With Loss of:  0.2526054084300995  Acc:  89.0625  Best Score of:  0.98595595\n",
      "741 out of:  2041  With Loss of:  0.33271729946136475  Acc:  89.0625  Best Score of:  0.99287754\n",
      "742 out of:  2041  With Loss of:  0.27717435359954834  Acc:  82.8125  Best Score of:  0.9838922\n",
      "743 out of:  2041  With Loss of:  0.21559523046016693  Acc:  87.5  Best Score of:  0.99978083\n",
      "744 out of:  2041  With Loss of:  0.27586349844932556  Acc:  76.5625  Best Score of:  0.9997787\n",
      "745 out of:  2041  With Loss of:  0.23927024006843567  Acc:  89.0625  Best Score of:  0.9959125\n",
      "746 out of:  2041  With Loss of:  0.27785488963127136  Acc:  78.125  Best Score of:  0.9999975\n",
      "747 out of:  2041  With Loss of:  0.3671957552433014  Acc:  79.6875  Best Score of:  0.99892163\n",
      "748 out of:  2041  With Loss of:  0.5488948822021484  Acc:  79.6875  Best Score of:  0.9985123\n",
      "749 out of:  2041  With Loss of:  0.8619717359542847  Acc:  81.25  Best Score of:  0.9886395\n",
      "750 out of:  2041  With Loss of:  0.2742326557636261  Acc:  90.625  Best Score of:  0.99924564\n",
      "751 out of:  2041  With Loss of:  0.33385345339775085  Acc:  79.6875  Best Score of:  0.99985707\n",
      "752 out of:  2041  With Loss of:  0.47036248445510864  Acc:  89.0625  Best Score of:  0.9998572\n",
      "753 out of:  2041  With Loss of:  0.30941495299339294  Acc:  81.25  Best Score of:  0.99986804\n",
      "754 out of:  2041  With Loss of:  0.44843876361846924  Acc:  76.5625  Best Score of:  0.99489367\n",
      "755 out of:  2041  With Loss of:  0.3893415033817291  Acc:  82.8125  Best Score of:  0.9924736\n",
      "756 out of:  2041  With Loss of:  0.4052443206310272  Acc:  85.9375  Best Score of:  0.99907124\n",
      "757 out of:  2041  With Loss of:  0.6305249929428101  Acc:  81.25  Best Score of:  0.9998356\n",
      "758 out of:  2041  With Loss of:  0.5212783217430115  Acc:  73.4375  Best Score of:  0.99990046\n",
      "759 out of:  2041  With Loss of:  0.6027650833129883  Acc:  76.5625  Best Score of:  0.9413259\n",
      "760 out of:  2041  With Loss of:  0.29687079787254333  Acc:  87.5  Best Score of:  0.9162869\n",
      "761 out of:  2041  With Loss of:  0.41582006216049194  Acc:  85.9375  Best Score of:  0.8929353\n",
      "762 out of:  2041  With Loss of:  0.34724605083465576  Acc:  90.625  Best Score of:  0.88743097\n",
      "763 out of:  2041  With Loss of:  0.38469401001930237  Acc:  87.5  Best Score of:  0.9116326\n",
      "764 out of:  2041  With Loss of:  0.6464577913284302  Acc:  95.3125  Best Score of:  0.94439685\n",
      "765 out of:  2041  With Loss of:  0.484986811876297  Acc:  82.8125  Best Score of:  0.96586543\n",
      "766 out of:  2041  With Loss of:  0.44391918182373047  Acc:  90.625  Best Score of:  0.99668306\n",
      "767 out of:  2041  With Loss of:  0.2629329562187195  Acc:  92.1875  Best Score of:  0.8834734\n",
      "768 out of:  2041  With Loss of:  0.3909449577331543  Acc:  87.5  Best Score of:  0.95908225\n",
      "769 out of:  2041  With Loss of:  0.24662524461746216  Acc:  85.9375  Best Score of:  0.9505941\n",
      "770 out of:  2041  With Loss of:  0.41289639472961426  Acc:  75.0  Best Score of:  0.9985598\n",
      "771 out of:  2041  With Loss of:  0.6832597255706787  Acc:  79.6875  Best Score of:  0.99262285\n",
      "772 out of:  2041  With Loss of:  0.2840689420700073  Acc:  85.9375  Best Score of:  0.99730325\n",
      "773 out of:  2041  With Loss of:  0.504500687122345  Acc:  84.375  Best Score of:  0.9920878\n",
      "774 out of:  2041  With Loss of:  0.2980732321739197  Acc:  89.0625  Best Score of:  0.9785905\n",
      "775 out of:  2041  With Loss of:  0.3331203758716583  Acc:  90.625  Best Score of:  0.9957377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776 out of:  2041  With Loss of:  0.5089450478553772  Acc:  82.8125  Best Score of:  0.99410063\n",
      "777 out of:  2041  With Loss of:  0.48848390579223633  Acc:  87.5  Best Score of:  0.9222748\n",
      "778 out of:  2041  With Loss of:  0.35328543186187744  Acc:  85.9375  Best Score of:  0.9937211\n",
      "779 out of:  2041  With Loss of:  0.26933181285858154  Acc:  89.0625  Best Score of:  0.99046564\n",
      "780 out of:  2041  With Loss of:  0.32796230912208557  Acc:  87.5  Best Score of:  0.9943884\n",
      "781 out of:  2041  With Loss of:  0.3836430311203003  Acc:  90.625  Best Score of:  0.9991835\n",
      "782 out of:  2041  With Loss of:  0.20015090703964233  Acc:  90.625  Best Score of:  0.9915692\n",
      "783 out of:  2041  With Loss of:  0.2812194228172302  Acc:  92.1875  Best Score of:  0.9949221\n",
      "784 out of:  2041  With Loss of:  0.25701552629470825  Acc:  93.75  Best Score of:  0.9620011\n",
      "785 out of:  2041  With Loss of:  0.41612130403518677  Acc:  87.5  Best Score of:  0.9930137\n",
      "786 out of:  2041  With Loss of:  0.4986672103404999  Acc:  95.3125  Best Score of:  0.94001085\n",
      "787 out of:  2041  With Loss of:  0.5757944583892822  Acc:  90.625  Best Score of:  0.7645503\n",
      "788 out of:  2041  With Loss of:  0.2535501718521118  Acc:  92.1875  Best Score of:  0.99999833\n",
      "789 out of:  2041  With Loss of:  0.5259814858436584  Acc:  89.0625  Best Score of:  0.9994456\n",
      "790 out of:  2041  With Loss of:  0.4577533006668091  Acc:  79.6875  Best Score of:  0.99938047\n",
      "791 out of:  2041  With Loss of:  0.32476869225502014  Acc:  89.0625  Best Score of:  0.99998415\n",
      "792 out of:  2041  With Loss of:  0.30426639318466187  Acc:  92.1875  Best Score of:  0.9068739\n",
      "793 out of:  2041  With Loss of:  0.2847393751144409  Acc:  90.625  Best Score of:  0.9917442\n",
      "794 out of:  2041  With Loss of:  0.3953802287578583  Acc:  92.1875  Best Score of:  0.99872977\n",
      "795 out of:  2041  With Loss of:  0.11301631480455399  Acc:  95.3125  Best Score of:  0.9932046\n",
      "796 out of:  2041  With Loss of:  0.3225633203983307  Acc:  93.75  Best Score of:  0.7586683\n",
      "797 out of:  2041  With Loss of:  0.672060489654541  Acc:  82.8125  Best Score of:  0.9190119\n",
      "798 out of:  2041  With Loss of:  0.6621167659759521  Acc:  84.375  Best Score of:  0.9985197\n",
      "799 out of:  2041  With Loss of:  0.2372877299785614  Acc:  85.9375  Best Score of:  0.99535286\n",
      "800 out of:  2041  With Loss of:  0.7074605226516724  Acc:  79.6875  Best Score of:  0.9994023\n",
      "801 out of:  2041  With Loss of:  0.46796277165412903  Acc:  78.125  Best Score of:  0.99350303\n",
      "802 out of:  2041  With Loss of:  0.39168232679367065  Acc:  73.4375  Best Score of:  0.99999833\n",
      "803 out of:  2041  With Loss of:  0.7175686359405518  Acc:  71.875  Best Score of:  0.9996395\n",
      "804 out of:  2041  With Loss of:  0.4704819321632385  Acc:  62.5  Best Score of:  0.99995816\n",
      "805 out of:  2041  With Loss of:  0.3572632968425751  Acc:  71.875  Best Score of:  0.9970763\n",
      "806 out of:  2041  With Loss of:  0.3097991943359375  Acc:  78.125  Best Score of:  0.9603769\n",
      "807 out of:  2041  With Loss of:  0.34638991951942444  Acc:  81.25  Best Score of:  0.9999436\n",
      "808 out of:  2041  With Loss of:  0.37528157234191895  Acc:  89.0625  Best Score of:  0.97980624\n",
      "809 out of:  2041  With Loss of:  0.30137282609939575  Acc:  96.875  Best Score of:  0.99007434\n",
      "810 out of:  2041  With Loss of:  0.6989280581474304  Acc:  90.625  Best Score of:  0.8196513\n",
      "811 out of:  2041  With Loss of:  0.18496757745742798  Acc:  93.75  Best Score of:  0.9645971\n",
      "812 out of:  2041  With Loss of:  0.5015727877616882  Acc:  95.3125  Best Score of:  0.8389946\n",
      "813 out of:  2041  With Loss of:  0.17781922221183777  Acc:  92.1875  Best Score of:  0.95458513\n",
      "814 out of:  2041  With Loss of:  0.3885757029056549  Acc:  82.8125  Best Score of:  0.9973814\n",
      "815 out of:  2041  With Loss of:  0.3989194929599762  Acc:  90.625  Best Score of:  0.9668223\n",
      "816 out of:  2041  With Loss of:  0.26699575781822205  Acc:  100.0  Best Score of:  0.7669588\n",
      "817 out of:  2041  With Loss of:  0.555426836013794  Acc:  93.75  Best Score of:  0.99708194\n",
      "818 out of:  2041  With Loss of:  0.31069281697273254  Acc:  92.1875  Best Score of:  0.92530984\n",
      "819 out of:  2041  With Loss of:  0.6710867881774902  Acc:  71.875  Best Score of:  0.9511478\n",
      "820 out of:  2041  With Loss of:  0.686465859413147  Acc:  79.6875  Best Score of:  0.99777824\n",
      "821 out of:  2041  With Loss of:  0.3424619436264038  Acc:  84.375  Best Score of:  0.99297214\n",
      "822 out of:  2041  With Loss of:  0.5037267208099365  Acc:  87.5  Best Score of:  0.9472786\n",
      "823 out of:  2041  With Loss of:  0.35468044877052307  Acc:  79.6875  Best Score of:  0.9793778\n",
      "824 out of:  2041  With Loss of:  0.6010558605194092  Acc:  73.4375  Best Score of:  0.98989046\n",
      "825 out of:  2041  With Loss of:  0.3039413392543793  Acc:  79.6875  Best Score of:  0.98419696\n",
      "826 out of:  2041  With Loss of:  0.3075368106365204  Acc:  79.6875  Best Score of:  0.99890816\n",
      "827 out of:  2041  With Loss of:  0.38841861486434937  Acc:  89.0625  Best Score of:  0.99296826\n",
      "828 out of:  2041  With Loss of:  0.4188777506351471  Acc:  92.1875  Best Score of:  0.99463946\n",
      "829 out of:  2041  With Loss of:  0.6525184512138367  Acc:  84.375  Best Score of:  0.97415507\n",
      "830 out of:  2041  With Loss of:  0.4242071509361267  Acc:  89.0625  Best Score of:  0.9844044\n",
      "831 out of:  2041  With Loss of:  0.6812887787818909  Acc:  90.625  Best Score of:  0.93632585\n",
      "832 out of:  2041  With Loss of:  0.5005864500999451  Acc:  95.3125  Best Score of:  0.8591145\n",
      "833 out of:  2041  With Loss of:  0.2503768801689148  Acc:  92.1875  Best Score of:  0.8172388\n",
      "834 out of:  2041  With Loss of:  0.49490123987197876  Acc:  90.625  Best Score of:  0.9963871\n",
      "835 out of:  2041  With Loss of:  0.35943713784217834  Acc:  81.25  Best Score of:  0.9657746\n",
      "836 out of:  2041  With Loss of:  0.30559003353118896  Acc:  81.25  Best Score of:  0.9389688\n",
      "837 out of:  2041  With Loss of:  0.43676456809043884  Acc:  89.0625  Best Score of:  0.9878086\n",
      "838 out of:  2041  With Loss of:  0.4219614267349243  Acc:  93.75  Best Score of:  0.84363747\n",
      "839 out of:  2041  With Loss of:  0.4311663508415222  Acc:  85.9375  Best Score of:  0.98268086\n",
      "840 out of:  2041  With Loss of:  0.2022518515586853  Acc:  87.5  Best Score of:  0.99197257\n",
      "841 out of:  2041  With Loss of:  0.3887879252433777  Acc:  96.875  Best Score of:  0.7494612\n",
      "842 out of:  2041  With Loss of:  0.3119134306907654  Acc:  87.5  Best Score of:  0.9898168\n",
      "843 out of:  2041  With Loss of:  0.32258889079093933  Acc:  90.625  Best Score of:  0.99980503\n",
      "844 out of:  2041  With Loss of:  0.22993329167366028  Acc:  84.375  Best Score of:  0.99939704\n",
      "845 out of:  2041  With Loss of:  0.3135109841823578  Acc:  84.375  Best Score of:  0.999521\n",
      "846 out of:  2041  With Loss of:  0.17147289216518402  Acc:  96.875  Best Score of:  0.98818177\n",
      "847 out of:  2041  With Loss of:  0.36574023962020874  Acc:  76.5625  Best Score of:  0.99745935\n",
      "848 out of:  2041  With Loss of:  0.3113477826118469  Acc:  90.625  Best Score of:  0.9756427\n",
      "849 out of:  2041  With Loss of:  0.20491591095924377  Acc:  87.5  Best Score of:  0.9852374\n",
      "850 out of:  2041  With Loss of:  0.3653944730758667  Acc:  82.8125  Best Score of:  0.98600113\n",
      "851 out of:  2041  With Loss of:  0.173916295170784  Acc:  90.625  Best Score of:  0.99996114\n",
      "852 out of:  2041  With Loss of:  0.1979384422302246  Acc:  92.1875  Best Score of:  0.9999193\n",
      "853 out of:  2041  With Loss of:  0.5322022438049316  Acc:  87.5  Best Score of:  0.95601\n",
      "854 out of:  2041  With Loss of:  0.33370155096054077  Acc:  84.375  Best Score of:  0.9997955\n",
      "855 out of:  2041  With Loss of:  0.9713014960289001  Acc:  89.0625  Best Score of:  0.999894\n",
      "856 out of:  2041  With Loss of:  0.3587528467178345  Acc:  95.3125  Best Score of:  0.9480683\n",
      "857 out of:  2041  With Loss of:  0.465260773897171  Acc:  81.25  Best Score of:  0.94222814\n",
      "858 out of:  2041  With Loss of:  0.2818327844142914  Acc:  84.375  Best Score of:  0.99850583\n",
      "859 out of:  2041  With Loss of:  0.2429097443819046  Acc:  92.1875  Best Score of:  0.9881378\n",
      "860 out of:  2041  With Loss of:  0.22950546443462372  Acc:  90.625  Best Score of:  0.9649952\n",
      "861 out of:  2041  With Loss of:  0.11480895429849625  Acc:  95.3125  Best Score of:  0.99967027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862 out of:  2041  With Loss of:  0.48481109738349915  Acc:  90.625  Best Score of:  0.8300822\n",
      "863 out of:  2041  With Loss of:  0.2850256562232971  Acc:  87.5  Best Score of:  0.99440444\n",
      "864 out of:  2041  With Loss of:  0.3713218569755554  Acc:  78.125  Best Score of:  0.9965507\n",
      "865 out of:  2041  With Loss of:  0.22162768244743347  Acc:  92.1875  Best Score of:  0.9684967\n",
      "866 out of:  2041  With Loss of:  0.3280627727508545  Acc:  84.375  Best Score of:  0.95769745\n",
      "867 out of:  2041  With Loss of:  0.3224148750305176  Acc:  96.875  Best Score of:  0.71248513\n",
      "868 out of:  2041  With Loss of:  0.2113727182149887  Acc:  93.75  Best Score of:  0.9975788\n",
      "869 out of:  2041  With Loss of:  0.2982932925224304  Acc:  92.1875  Best Score of:  0.80603063\n",
      "870 out of:  2041  With Loss of:  0.6621786952018738  Acc:  90.625  Best Score of:  0.99823785\n",
      "871 out of:  2041  With Loss of:  0.7787222266197205  Acc:  92.1875  Best Score of:  0.84961766\n",
      "872 out of:  2041  With Loss of:  0.3295760452747345  Acc:  93.75  Best Score of:  0.97949356\n",
      "873 out of:  2041  With Loss of:  0.7097198963165283  Acc:  87.5  Best Score of:  0.8305882\n",
      "874 out of:  2041  With Loss of:  0.32181063294410706  Acc:  85.9375  Best Score of:  0.9358133\n",
      "875 out of:  2041  With Loss of:  0.5362328290939331  Acc:  85.9375  Best Score of:  0.9998343\n",
      "876 out of:  2041  With Loss of:  0.4735729396343231  Acc:  92.1875  Best Score of:  0.9858978\n",
      "877 out of:  2041  With Loss of:  0.5192572474479675  Acc:  73.4375  Best Score of:  0.9984901\n",
      "878 out of:  2041  With Loss of:  0.3175453841686249  Acc:  81.25  Best Score of:  0.99383825\n",
      "879 out of:  2041  With Loss of:  0.3760753571987152  Acc:  93.75  Best Score of:  0.99421906\n",
      "880 out of:  2041  With Loss of:  0.3135504126548767  Acc:  84.375  Best Score of:  0.99745554\n",
      "881 out of:  2041  With Loss of:  0.2608526945114136  Acc:  87.5  Best Score of:  0.99484646\n",
      "882 out of:  2041  With Loss of:  0.4327145516872406  Acc:  85.9375  Best Score of:  0.9276473\n",
      "883 out of:  2041  With Loss of:  0.430547297000885  Acc:  85.9375  Best Score of:  0.9619544\n",
      "884 out of:  2041  With Loss of:  0.3494393527507782  Acc:  84.375  Best Score of:  0.99368507\n",
      "885 out of:  2041  With Loss of:  0.5914697051048279  Acc:  84.375  Best Score of:  0.87396663\n",
      "886 out of:  2041  With Loss of:  0.6416887044906616  Acc:  95.3125  Best Score of:  0.90758055\n",
      "887 out of:  2041  With Loss of:  0.258577436208725  Acc:  90.625  Best Score of:  0.9865607\n",
      "888 out of:  2041  With Loss of:  0.4481589198112488  Acc:  84.375  Best Score of:  0.8304946\n",
      "889 out of:  2041  With Loss of:  0.39607203006744385  Acc:  82.8125  Best Score of:  0.9816836\n",
      "890 out of:  2041  With Loss of:  0.4773358106613159  Acc:  85.9375  Best Score of:  0.9806128\n",
      "891 out of:  2041  With Loss of:  0.30757641792297363  Acc:  76.5625  Best Score of:  0.99626046\n",
      "892 out of:  2041  With Loss of:  0.47671645879745483  Acc:  84.375  Best Score of:  0.99507916\n",
      "893 out of:  2041  With Loss of:  0.3022429943084717  Acc:  85.9375  Best Score of:  0.99696654\n",
      "894 out of:  2041  With Loss of:  0.6057422757148743  Acc:  87.5  Best Score of:  0.9771383\n",
      "895 out of:  2041  With Loss of:  0.5378943085670471  Acc:  89.0625  Best Score of:  0.9942742\n",
      "896 out of:  2041  With Loss of:  0.4124736487865448  Acc:  90.625  Best Score of:  0.8409666\n",
      "897 out of:  2041  With Loss of:  0.29920390248298645  Acc:  84.375  Best Score of:  0.99021065\n",
      "898 out of:  2041  With Loss of:  0.22193853557109833  Acc:  84.375  Best Score of:  0.9954651\n",
      "899 out of:  2041  With Loss of:  0.6530958414077759  Acc:  78.125  Best Score of:  0.9034453\n",
      "900 out of:  2041  With Loss of:  0.40093228220939636  Acc:  87.5  Best Score of:  0.9900074\n",
      "901 out of:  2041  With Loss of:  0.21873679757118225  Acc:  92.1875  Best Score of:  0.9965653\n",
      "902 out of:  2041  With Loss of:  0.7025236487388611  Acc:  82.8125  Best Score of:  0.98549277\n",
      "903 out of:  2041  With Loss of:  0.270510733127594  Acc:  87.5  Best Score of:  0.9947249\n",
      "904 out of:  2041  With Loss of:  0.3791172504425049  Acc:  78.125  Best Score of:  0.99065405\n",
      "905 out of:  2041  With Loss of:  0.3746524453163147  Acc:  78.125  Best Score of:  0.99731296\n",
      "906 out of:  2041  With Loss of:  0.4462340772151947  Acc:  84.375  Best Score of:  0.9669107\n",
      "907 out of:  2041  With Loss of:  0.38182663917541504  Acc:  90.625  Best Score of:  0.91844225\n",
      "908 out of:  2041  With Loss of:  0.477114737033844  Acc:  85.9375  Best Score of:  0.9030203\n",
      "909 out of:  2041  With Loss of:  0.27962297201156616  Acc:  82.8125  Best Score of:  0.99012065\n",
      "910 out of:  2041  With Loss of:  0.2854112684726715  Acc:  84.375  Best Score of:  0.8924618\n",
      "911 out of:  2041  With Loss of:  0.49995309114456177  Acc:  87.5  Best Score of:  0.8794805\n",
      "912 out of:  2041  With Loss of:  0.7064942121505737  Acc:  81.25  Best Score of:  0.9929575\n",
      "913 out of:  2041  With Loss of:  0.41737842559814453  Acc:  79.6875  Best Score of:  0.98709226\n",
      "914 out of:  2041  With Loss of:  0.27596110105514526  Acc:  89.0625  Best Score of:  0.9675115\n",
      "915 out of:  2041  With Loss of:  0.40488386154174805  Acc:  81.25  Best Score of:  0.95825964\n",
      "916 out of:  2041  With Loss of:  0.3483831286430359  Acc:  82.8125  Best Score of:  0.99758804\n",
      "917 out of:  2041  With Loss of:  0.3763977587223053  Acc:  81.25  Best Score of:  0.9848605\n",
      "918 out of:  2041  With Loss of:  0.7863489389419556  Acc:  78.125  Best Score of:  0.8975844\n",
      "919 out of:  2041  With Loss of:  0.4280329942703247  Acc:  76.5625  Best Score of:  0.99810565\n",
      "920 out of:  2041  With Loss of:  0.5443319082260132  Acc:  79.6875  Best Score of:  0.96715957\n",
      "921 out of:  2041  With Loss of:  0.241266667842865  Acc:  84.375  Best Score of:  0.9970682\n",
      "922 out of:  2041  With Loss of:  0.5688525438308716  Acc:  76.5625  Best Score of:  0.97380674\n"
     ]
    }
   ],
   "source": [
    "validate = True\n",
    "\n",
    "checking = next(iter(train_iter))\n",
    "inputs_check = checking.question_text\n",
    "target_check = checking.target\n",
    "\n",
    "\n",
    "f1_train = []\n",
    "auc_train = []\n",
    "train_accu = []\n",
    "\n",
    "avg_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = inputs_check\n",
    "    score1 = model(inputs)\n",
    "    #print(score)\n",
    "    \n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    training_set = iter(train_iter)\n",
    "    model.zero_grad()\n",
    "    all_preds = []\n",
    "    all_actual = []\n",
    "    for batch_num in range(len(train_iter)):\n",
    "        \n",
    "        sentence_data = next(training_set)\n",
    "        sentence_in = sentence_data.question_text\n",
    "        #target -> target.unsqueeze(1)\n",
    "        if sigBCE:\n",
    "            targets = sentence_data.target.float()#.unsqueeze(1)\n",
    "        else:\n",
    "            targets = torch.LongTensor(sentence_data.target)#.unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        target_scores = model(sentence_in)\n",
    "        \n",
    "        loss = loss_function(target_scores.squeeze(1), targets)#.squeeze(1))#.reshape(64,1).squeeze(1))\n",
    "        loss = (loss * (targets*14+1)/(targets*14+1).sum()).sum()#.mean()\n",
    "        loss.backward()\n",
    "        \n",
    "        if sigBCE:\n",
    "            prediction = np.asarray(target_scores.squeeze(1)>0.5,dtype=float)\n",
    "            accuracy = np.asarray(prediction == np.asarray(targets)).sum()/len(targets)*100\n",
    "        else:\n",
    "            prediction = target_scores.data.max(1)[1]\n",
    "            accuracy = np.asarray(prediction==targets).sum()/len(targets)*100\n",
    "        train_accu.append(accuracy)\n",
    "        \n",
    "        if sigBCE:\n",
    "            all_preds.extend(target_scores.tolist())#torch.round(target_scores).tolist())\n",
    "        else:\n",
    "            all_preds.extend(torch.exp(target_scores).tolist())#torch.round(target_scores).tolist())\n",
    "        all_actual.extend(targets.tolist())\n",
    "        \n",
    "        #print(np.asarray(target_scores)[:,1].max())\n",
    "        \n",
    "        #targets -> targets.reshape(64,1).squeeze(1)\n",
    "        \n",
    "        avg_loss += loss.item()\n",
    "        \n",
    "        #print(loss)\n",
    "        if sigBCE:\n",
    "            print(batch_num+1,\"out of: \",len(train_iter), \" With Loss of: \",loss.item(),\" Acc: \",accuracy, \" Best Score of: \", np.asarray(target_scores).max())\n",
    "        else:\n",
    "            print(batch_num+1,\"out of: \",len(train_iter), \" With Loss of: \",loss.item(),\" Acc: \",accuracy, \" Best Score of: \", np.asarray(torch.exp(target_scores))[:,1].max())\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    #model.eval()\n",
    "    avg_loss = avg_loss/len(train_iter)\n",
    "    \n",
    "    #if validate:\n",
    "        #avg_val\n",
    "    \n",
    "    \n",
    "    #else:\n",
    "    print(\"Training loss for epoch {} is {}\".format(epoch + 1, avg_loss))\n",
    "    \n",
    "    \n",
    "    #train_confusion_matrix = confusion_matrix(all_actual, all_preds).ravel()\n",
    "    if sigBCE:\n",
    "        pred_Clas = (np.asarray(all_preds) > 0.5).astype(int)#-2.65).astype(int)#\n",
    "    else:\n",
    "        pred_Clas = (np.asarray(all_preds)[:,1] > np.asarray(all_preds)[:,0]).astype(int)#-2.65).astype(int)#\n",
    "    train_confusion_matrix = confusion_matrix(all_actual, pred_Clas).ravel()\n",
    "    \n",
    "    print(\"Confusion matrix for epoch {}, tn={}, fp={}, fn={}, tp={}\".format(epoch + 1, *train_confusion_matrix))\n",
    "    f1_train.append(f1_score(all_actual, pred_Clas))\n",
    "    print(\"F1 score for epoch {} is {}\".format(epoch + 1, f1_train[-1]))\n",
    "    fpr, tpr, _ = roc_curve(all_actual, pred_Clas, pos_label=1)\n",
    "    auc_train.append(auc(fpr, tpr))\n",
    "    print(\"AUC for epoch {} is {}\\n\".format(epoch + 1, auc_train[-1]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = inputs_check\n",
    "    score2 = model(inputs)\n",
    "    #print(score)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "nb_classes = 2\n",
    "target = torch.LongTensor(batch_size).random_(nb_classes)\n",
    "#print((targets*5+1))\n",
    "print((loss * (targets*5+1)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5147],\n",
       "        [0.5146],\n",
       "        [0.5146],\n",
       "        [0.5147],\n",
       "        [0.5146],\n",
       "        [0.5147],\n",
       "        [0.5147],\n",
       "        [0.5148],\n",
       "        [0.5147],\n",
       "        [0.5147],\n",
       "        [0.5148],\n",
       "        [0.5146],\n",
       "        [0.5146],\n",
       "        [0.5146],\n",
       "        [0.5147],\n",
       "        [0.5146],\n",
       "        [0.5147],\n",
       "        [0.5152],\n",
       "        [0.5148],\n",
       "        [0.5145],\n",
       "        [0.5146],\n",
       "        [0.5147],\n",
       "        [0.5146],\n",
       "        [0.5147],\n",
       "        [0.5147],\n",
       "        [0.5148],\n",
       "        [0.5147],\n",
       "        [0.5147],\n",
       "        [0.5148],\n",
       "        [0.5148],\n",
       "        [0.5146],\n",
       "        [0.5146],\n",
       "        [0.5146],\n",
       "        [0.5147],\n",
       "        [0.5148],\n",
       "        [0.5147],\n",
       "        [0.5146],\n",
       "        [0.5146],\n",
       "        [0.5146],\n",
       "        [0.5146],\n",
       "        [0.5148],\n",
       "        [0.5147],\n",
       "        [0.5147],\n",
       "        [0.5145],\n",
       "        [0.5148],\n",
       "        [0.5147],\n",
       "        [0.5147],\n",
       "        [0.5147],\n",
       "        [0.5147],\n",
       "        [0.5148],\n",
       "        [0.5146],\n",
       "        [0.5147],\n",
       "        [0.5146],\n",
       "        [0.5147],\n",
       "        [0.5147],\n",
       "        [0.5146],\n",
       "        [0.5147],\n",
       "        [0.5146],\n",
       "        [0.5147],\n",
       "        [0.5146],\n",
       "        [0.5146],\n",
       "        [0.5146],\n",
       "        [0.5148],\n",
       "        [0.5146]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = np.asarray(target_scores.squeeze(1)>0.5,dtype=float)\n",
    "target_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preddd = np.asarray(all_preds)[:,1]\n",
    "#preddd_clas = (preddd>-0.45).astype(int)\n",
    "#train_confusion_matrix = confusion_matrix(all_actual, preddd_clas).ravel()\n",
    "#print(\"Confusion matrix for epoch {}, tn={}, fp={}, fn={}, tp={}\".format(epoch + 1, *train_confusion_matrix))\n",
    "#pred_Clas = (np.asarray(all_preds)[:,1] > np.asarray(all_preds)[:,0]).astype(int)\n",
    "#f1_score(all_actual, pred_Clas)\n",
    "#pred_Clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preddd = (np.asarray(all_preds)[:,1] > np.asarray(all_preds)[:,0]).astype(int)\n",
    "len(preddd[preddd>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_search(y_true, y_proba, plot=False):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    thresholds = np.append(thresholds, 1.001) \n",
    "    F = 2 / (1/precision + 1/recall)\n",
    "    best_score = np.max(F)\n",
    "    best_th = thresholds[np.argmax(F)]\n",
    "    if plot:\n",
    "        plt.plot(thresholds, F, '-b')\n",
    "        plt.plot([best_th], [best_score], '*r')\n",
    "        plt.show()\n",
    "    search_result = {'threshold': best_th , 'f1': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_search_2(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(100)], disable=True):\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#threshold_search_2(all_actual, preddd)\n",
    "np.asarray(all_preds)[:,1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preddd = np.asarray(all_preds)[:,1]\n",
    "preddd_clas = (preddd>-0.45).astype(int)\n",
    "preddd.max()\n",
    "len(preddd_clas[preddd_clas == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr = threshold_search_2(all_actual, np.asarray(all_preds));thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr = threshold_search(all_actual, np.asarray(all_preds), plot=True);thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_final_preds = []\n",
    "for batch_num in range(len(test_iter)):\n",
    "    final_preds = model(next(iter(test_iter)).question_text)\n",
    "    all_final_preds.extend(final_preds.tolist())\n",
    "\n",
    "#probab = np.asarray(final_preds)[:,1]\n",
    "\n",
    "#final_preds_clas = (probab>thr['threshold']).astype(int)\n",
    "\n",
    "#final_preds_clas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr['threshold']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def train_model(model, train_data, epochs, show=False):\n",
    "    \n",
    "    #optimizer = torch.optim.Adam(model.parameters())\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "    #loss_function = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    print(\"The loss function being used is {}\".format(loss_function))\n",
    "    errors = []\n",
    "    eval_errors = []\n",
    "    f1_train = []\n",
    "    auc_train = []\n",
    "    f1_eval = []\n",
    "    auc_eval = []\n",
    "    \n",
    "    num_training_batches = len(train_data)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {}\".format(epoch + 1))\n",
    "        print(\"Training mode\")\n",
    "        model.train()\n",
    "        train_iter = iter(train_data)\n",
    "        model.zero_grad()\n",
    "        train_loss = 0\n",
    "        all_preds = []\n",
    "        all_actual = []\n",
    "        for batch_num in range(num_training_batches):\n",
    "            print(\"Batch {}\".format(batch_num + 1))\n",
    "            print(\"Out of {}\".format(num_training_batches))\n",
    "            batch = next(train_iter)\n",
    "            text, class_vector = batch.question_text.transpose(0,1),batch.target.unsqueeze(1)\n",
    "            #print(batch)\n",
    "            #print(text.shape)\n",
    "            #optimizer.zero_grad()\n",
    "            text_pred = model(text)\n",
    "            #print(text_pred)\n",
    "            print(class_vector.shape)\n",
    "            print(torch.max(class_vector,1)[0].shape)\n",
    "            loss = loss_function(text_pred,class_vector.reshape(64,1).squeeze(1)) #torch.max(class_vector,1)[0])\n",
    "            print(loss)\n",
    "            print(text_pred.squeeze(1))\n",
    "            print(torch.max(class_vector,1)[0])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#train_model(net, train_iter,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_iters = iter(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wee = next(train_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wee.question_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
