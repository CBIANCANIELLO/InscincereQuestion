{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastai.vision import *\n",
    "from fastai import * \n",
    "from fastai.text import *\n",
    "from pathlib import Path \n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"my bar!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = True\n",
    "clean = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('../input/embeddings'),\n",
       " WindowsPath('../input/glove.840B.300d'),\n",
       " WindowsPath('../input/sample_submission.csv'),\n",
       " WindowsPath('../input/test.csv'),\n",
       " WindowsPath('../input/tmp_lm'),\n",
       " WindowsPath('../input/train.csv')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if lm: path = Path('../input'); \n",
    "list(path.iterdir())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_df = pd.read_csv(path/'train.csv')\n",
    "test_df = pd.read_csv(path/'test.csv')\n",
    "allText_df = train_df.copy().append(test_df.copy(), sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clean:\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "    allText_df[\"question_text\"] = allText_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "\n",
    "    train_df['question_text'] = train_df['question_text'].apply(lambda x: clean_text(x))\n",
    "    test_df['question_text'] = test_df['question_text'].apply(lambda x: clean_text(x))\n",
    "    allText_df[\"question_text\"] = allText_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "\n",
    "    train_df['question_text'] = train_df['question_text'].apply(lambda x: clean_numbers(x))\n",
    "    test_df['question_text'] = test_df['question_text'].apply(lambda x:clean_numbers(x))\n",
    "    allText_df[\"question_text\"] = allText_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "\n",
    "    train_df['question_text'] = train_df['question_text'].apply(lambda x: replace_typical_mispell(x))\n",
    "    test_df['question_text'] = test_df['question_text'].apply(lambda x: replace_typical_mispell(x))\n",
    "    allText_df[\"question_text\"] = allText_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "\n",
    "#train_x = train_df['question_text'].fillna('_##_').values\n",
    "#test_x = test_df['question_text'].fillna('_##_').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import zipfile\n",
    "zf = zipfile.ZipFile(path/'test.csv.zip')\n",
    "df_test = pd.read_csv(zf.open('test.csv'))\n",
    "zf = zipfile.ZipFile(path/'train.csv.zip')\n",
    "df_train = pd.read_csv(zf.open('train.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_train.head(3))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences():\n",
    "    tokenizer = lambda text: text.split() # the function above is the function we will be using to tokenize the text\n",
    "    TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer, lower=True)\n",
    "    LABEL = torchtext.data.Field(sequential=False, use_vocab=False) # sequential and use_vocab=False since no text (binary)\n",
    "    train_datafields = [(\"qid\", None), (\"question_text\", TEXT), (\"target\", LABEL)]\n",
    "    train = torchtext.data.TabularDataset( # If we had a validation set as well, we would add an additional .splits(...)\n",
    "                        # path=\"data/train_cleaned_v2.csv\", # the root directory where the data lies\n",
    "                        path = path/'train.csv',\n",
    "                        format='csv',\n",
    "                        # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "                        skip_header=True, \n",
    "                        fields=train_datafields)\n",
    "    test_datafields = [(\"qid\", None),\n",
    "                     (\"question_text\", TEXT), (\"target\", LABEL)] \n",
    "    test = torchtext.data.TabularDataset( \n",
    "                path=path/'test.csv',\n",
    "                format=\"csv\",\n",
    "                skip_header=True,\n",
    "                fields=test_datafields)\n",
    "    return TEXT, LABEL, train, test\n",
    "# Chat Conversation End\n",
    "# Type a message...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT,LABEL,train,test = prepare_sequences()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.batch_first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, test, vectors = \"glove.6B.50d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,_ = train.split(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "train_iter = BucketIterator(\n",
    "    train, # we pass in the datasets we want the iterator to draw data from\n",
    "    batch_size=256, \n",
    "    sort_key= lambda x: len(x.question_text), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "    sort_within_batch=False, # sorting would add bias\n",
    "    repeat=False)\n",
    "test_iter = Iterator(test, batch_size = 256, device = torch.device(0), sort = False,  sort_within_batch = False, repeat = False) #Changed device from -1 to None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#iter(train_iter).__next__().question_text\n",
    "len(train_iter)\n",
    "#train_iter = iter(train_iter)\n",
    "#train_iter.__next__().question_text\n",
    "#train_iter.__next__().target\n",
    "#iter(train_iter).__next__().question_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaller,_ = train.split(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaller.examples[0].question_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.examples[0].question_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([527626, 50])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'how': 369907,\n",
       "         'did': 52902,\n",
       "         'quebec': 122,\n",
       "         'nationalists': 132,\n",
       "         'see': 11665,\n",
       "         'their': 44817,\n",
       "         'province': 174,\n",
       "         'as': 77136,\n",
       "         'a': 523949,\n",
       "         'nation': 1073,\n",
       "         'in': 480479,\n",
       "         'the': 853487,\n",
       "         '1960s?': 68,\n",
       "         'do': 312432,\n",
       "         'you': 241782,\n",
       "         'have': 116324,\n",
       "         'an': 86247,\n",
       "         'adopted': 403,\n",
       "         'dog,': 106,\n",
       "         'would': 78740,\n",
       "         'encourage': 476,\n",
       "         'people': 63109,\n",
       "         'to': 521322,\n",
       "         'adopt': 574,\n",
       "         'and': 329249,\n",
       "         'not': 54759,\n",
       "         'shop?': 244,\n",
       "         'why': 200208,\n",
       "         'does': 110586,\n",
       "         'velocity': 821,\n",
       "         'affect': 5640,\n",
       "         'time?': 7418,\n",
       "         'space': 3121,\n",
       "         'geometry?': 111,\n",
       "         'otto': 62,\n",
       "         'von': 145,\n",
       "         'guericke': 3,\n",
       "         'used': 15907,\n",
       "         'magdeburg': 2,\n",
       "         'hemispheres?': 6,\n",
       "         'can': 227414,\n",
       "         'i': 394196,\n",
       "         'convert': 1953,\n",
       "         'montra': 5,\n",
       "         'helicon': 1,\n",
       "         'd': 635,\n",
       "         'mountain': 500,\n",
       "         'bike': 1040,\n",
       "         'by': 54499,\n",
       "         'just': 21247,\n",
       "         'changing': 1333,\n",
       "         'tyres?': 35,\n",
       "         'is': 568393,\n",
       "         'gaza': 84,\n",
       "         'slowly': 333,\n",
       "         'becoming': 2942,\n",
       "         'auschwitz,': 4,\n",
       "         'dachau': 5,\n",
       "         'or': 118523,\n",
       "         'treblinka': 1,\n",
       "         'for': 256476,\n",
       "         'palestinians?': 151,\n",
       "         'quora': 12847,\n",
       "         'automatically': 672,\n",
       "         'ban': 1165,\n",
       "         'conservative': 1089,\n",
       "         'opinions': 638,\n",
       "         'when': 80484,\n",
       "         'reported,': 10,\n",
       "         'but': 35443,\n",
       "         'same': 12984,\n",
       "         'liberal': 1509,\n",
       "         'views?': 440,\n",
       "         'it': 159359,\n",
       "         'crazy': 751,\n",
       "         'if': 126250,\n",
       "         'wash': 607,\n",
       "         'wipe': 232,\n",
       "         'my': 144828,\n",
       "         'groceries': 55,\n",
       "         'off?': 1083,\n",
       "         'germs': 56,\n",
       "         'are': 311729,\n",
       "         'everywhere.': 38,\n",
       "         'there': 65762,\n",
       "         'such': 9261,\n",
       "         'thing': 9216,\n",
       "         'dressing': 164,\n",
       "         'moderately,': 1,\n",
       "         'so,': 3528,\n",
       "         'that': 133804,\n",
       "         'different': 14609,\n",
       "         'than': 30767,\n",
       "         'modestly?': 3,\n",
       "         'me': 34062,\n",
       "         'ever': 22072,\n",
       "         'been': 17660,\n",
       "         'this': 30830,\n",
       "         'phase': 758,\n",
       "         'wherein': 54,\n",
       "         'became': 1587,\n",
       "         'ignorant': 446,\n",
       "         'once': 2737,\n",
       "         'loved,': 21,\n",
       "         'completely': 2189,\n",
       "         'disregarding': 34,\n",
       "         'feelings/lives': 1,\n",
       "         'so': 44838,\n",
       "         'get': 79867,\n",
       "         'something': 9015,\n",
       "         'go': 17771,\n",
       "         'your': 86932,\n",
       "         'way': 24411,\n",
       "         'feel': 20353,\n",
       "         'temporarily': 155,\n",
       "         'at': 60750,\n",
       "         'ease.': 3,\n",
       "         'things': 12365,\n",
       "         'change?': 1110,\n",
       "         'what': 585115,\n",
       "         'say': 11660,\n",
       "         'about': 55688,\n",
       "         'feminism?': 193,\n",
       "         'were': 23029,\n",
       "         'calgary': 39,\n",
       "         'flames': 111,\n",
       "         'founded?': 383,\n",
       "         'dumbest,': 3,\n",
       "         'yet': 2427,\n",
       "         'possibly': 728,\n",
       "         'true': 8053,\n",
       "         'explanation': 605,\n",
       "         'trump': 13659,\n",
       "         'being': 24045,\n",
       "         'elected?': 120,\n",
       "         'we': 51139,\n",
       "         'use': 26083,\n",
       "         'our': 14658,\n",
       "         'external': 770,\n",
       "         'hard': 6183,\n",
       "         'disk': 238,\n",
       "         'os': 522,\n",
       "         'well': 4460,\n",
       "         'data': 7137,\n",
       "         'storage.will': 1,\n",
       "         'be': 115003,\n",
       "         'affected?': 76,\n",
       "         'am': 25965,\n",
       "         '30,': 58,\n",
       "         'living': 5852,\n",
       "         'home': 5346,\n",
       "         'no': 17580,\n",
       "         'boyfriend.': 131,\n",
       "         'love': 11745,\n",
       "         'boyfriend': 3323,\n",
       "         'own': 9716,\n",
       "         'home.': 152,\n",
       "         'progress': 457,\n",
       "         'situation?': 663,\n",
       "         'know': 24089,\n",
       "         'bram': 16,\n",
       "         'fischer': 20,\n",
       "         'rivonia': 1,\n",
       "         'trial?': 107,\n",
       "         'difficult': 3320,\n",
       "         'find': 22273,\n",
       "         'good': 46397,\n",
       "         'instructor': 88,\n",
       "         'take': 23031,\n",
       "         'class': 7515,\n",
       "         'near': 2445,\n",
       "         'you?': 8106,\n",
       "         'licked': 61,\n",
       "         'skin': 2330,\n",
       "         'of': 426654,\n",
       "         'corpse?': 22,\n",
       "         'think': 32772,\n",
       "         'amazon': 2440,\n",
       "         'will': 64773,\n",
       "         'house': 3996,\n",
       "         'approach': 1592,\n",
       "         'manufacturing': 1172,\n",
       "         'similar': 3632,\n",
       "         'tesla': 651,\n",
       "         'x': 1909,\n",
       "         'business': 10755,\n",
       "         'models?': 233,\n",
       "         'many': 31256,\n",
       "         'baronies': 1,\n",
       "         'might': 2678,\n",
       "         'exist': 1910,\n",
       "         'within': 2936,\n",
       "         'county': 244,\n",
       "         'palatine?': 1,\n",
       "         'whether': 1884,\n",
       "         'girl': 8701,\n",
       "         'had': 15459,\n",
       "         'done': 6538,\n",
       "         'sex': 8476,\n",
       "         'before': 12956,\n",
       "         'with': 120958,\n",
       "         'me?': 8028,\n",
       "         'become': 21702,\n",
       "         'fast': 2292,\n",
       "         'learner': 67,\n",
       "         'both': 5922,\n",
       "         'professional': 2741,\n",
       "         'career': 6307,\n",
       "         'personal': 3663,\n",
       "         'life?': 10046,\n",
       "         'has': 40576,\n",
       "         'united': 5964,\n",
       "         'states': 4606,\n",
       "         'largest': 1310,\n",
       "         'dictatorship': 127,\n",
       "         'world?': 7119,\n",
       "         'strangest': 383,\n",
       "         'phenomenon': 190,\n",
       "         'of,': 149,\n",
       "         'witnessed': 264,\n",
       "         'generated': 277,\n",
       "         'area': 2455,\n",
       "         'electronics': 1066,\n",
       "         'terms': 3195,\n",
       "         'modern': 3707,\n",
       "         'physics?': 1240,\n",
       "         'should': 82888,\n",
       "         'leave': 3889,\n",
       "         'friends': 5236,\n",
       "         'new': 18303,\n",
       "         'ones?': 741,\n",
       "         'make': 35546,\n",
       "         'alexa': 155,\n",
       "         'trigger': 259,\n",
       "         'events': 1678,\n",
       "         'browser?': 175,\n",
       "         \"haven't\": 1416,\n",
       "         'two': 10249,\n",
       "         'democracies': 51,\n",
       "         'never': 8609,\n",
       "         'went': 1815,\n",
       "         'full': 4539,\n",
       "         'fledged': 13,\n",
       "         'war?': 2540,\n",
       "         'stops': 482,\n",
       "         'them?': 7914,\n",
       "         'top': 6998,\n",
       "         'cbse': 2030,\n",
       "         '6': 3044,\n",
       "         'months?': 1373,\n",
       "         'visiting': 1048,\n",
       "         'mcleodganj': 5,\n",
       "         'doing': 9069,\n",
       "         'triund': 4,\n",
       "         'trek?': 68,\n",
       "         'military': 3591,\n",
       "         'submarines': 87,\n",
       "         'reduce': 2395,\n",
       "         'noise': 429,\n",
       "         'achieve': 1486,\n",
       "         'stealth?': 12,\n",
       "         'which': 84147,\n",
       "         'babies': 554,\n",
       "         'more': 32926,\n",
       "         'sweeter': 15,\n",
       "         'parents?': 1214,\n",
       "         'dark': 2299,\n",
       "         'light': 3778,\n",
       "         'babies?': 261,\n",
       "         'remove': 2971,\n",
       "         'black': 8747,\n",
       "         'heads': 250,\n",
       "         'all': 31388,\n",
       "         'over': 13723,\n",
       "         'nose?': 293,\n",
       "         'lightsabers': 11,\n",
       "         'created': 1958,\n",
       "         'individual': 1255,\n",
       "         'wielders,': 1,\n",
       "         'each': 6814,\n",
       "         'saber': 19,\n",
       "         'unique': 1193,\n",
       "         'powers/abilities?': 1,\n",
       "         'anyone': 10835,\n",
       "         'still': 15498,\n",
       "         'using': 12228,\n",
       "         'visual': 687,\n",
       "         'basic?': 52,\n",
       "         'worth': 5056,\n",
       "         'learning': 6173,\n",
       "         '2018?': 4618,\n",
       "         'jobs': 4421,\n",
       "         'basic': 2747,\n",
       "         'programmers': 321,\n",
       "         '2018-19-20?': 1,\n",
       "         'sykes': 12,\n",
       "         'enterprises': 75,\n",
       "         'about?': 2330,\n",
       "         'any': 51128,\n",
       "         'clear': 2098,\n",
       "         'relations': 732,\n",
       "         'between': 31268,\n",
       "         'number': 7309,\n",
       "         'nodes/dofs': 1,\n",
       "         'computational': 215,\n",
       "         'performances': 56,\n",
       "         'requirements': 1248,\n",
       "         'fea': 13,\n",
       "         'cfd': 34,\n",
       "         'analyses': 18,\n",
       "         '(for': 805,\n",
       "         'ansys': 45,\n",
       "         'solutions': 729,\n",
       "         'particular)?': 4,\n",
       "         'package': 1120,\n",
       "         'isc': 417,\n",
       "         'since': 6207,\n",
       "         'may': 3052,\n",
       "         '31,2017': 1,\n",
       "         \"don't\": 20276,\n",
       "         'updated?': 42,\n",
       "         'great': 5750,\n",
       "         'wit': 29,\n",
       "         'mean?': 5116,\n",
       "         'experience': 7082,\n",
       "         'working': 6943,\n",
       "         'realtors,': 2,\n",
       "         'wish': 1940,\n",
       "         'realtors': 19,\n",
       "         'better?': 1613,\n",
       "         'charge': 2157,\n",
       "         'contact?': 207,\n",
       "         'public': 4293,\n",
       "         'school': 9878,\n",
       "         'teachers': 1218,\n",
       "         'vacation': 579,\n",
       "         'whenever': 993,\n",
       "         'they': 53932,\n",
       "         'ask': 6369,\n",
       "         'it?': 16957,\n",
       "         'role': 3746,\n",
       "         'technology': 3484,\n",
       "         'resource?': 82,\n",
       "         'opt': 837,\n",
       "         'jaypee': 42,\n",
       "         'university': 8055,\n",
       "         'guna': 13,\n",
       "         'mechanical': 3613,\n",
       "         'engineering?': 3821,\n",
       "         'where': 36174,\n",
       "         'download': 3134,\n",
       "         'microsoft': 1229,\n",
       "         'word': 4383,\n",
       "         'windows': 2087,\n",
       "         '2.0': 140,\n",
       "         'hungarian?': 9,\n",
       "         'need': 16420,\n",
       "         'buying': 2191,\n",
       "         'car': 5650,\n",
       "         'south': 5051,\n",
       "         'africa': 1155,\n",
       "         'american?': 276,\n",
       "         'someone': 26513,\n",
       "         'who': 65244,\n",
       "         \"didn't\": 4244,\n",
       "         'enjoy': 2030,\n",
       "         'harry': 1665,\n",
       "         'potter': 904,\n",
       "         'order': 5229,\n",
       "         'phoenix': 127,\n",
       "         'movie,': 220,\n",
       "         'least': 2742,\n",
       "         'book': 10596,\n",
       "         'writing': 4793,\n",
       "         'style': 1580,\n",
       "         '\"how': 265,\n",
       "         'resist': 216,\n",
       "         'prince': 676,\n",
       "         'charming\"': 1,\n",
       "         'linda': 64,\n",
       "         'kage?': 3,\n",
       "         'mother': 2526,\n",
       "         'expects': 67,\n",
       "         'memorize': 196,\n",
       "         'her': 13636,\n",
       "         'usernames': 22,\n",
       "         'passwords.': 3,\n",
       "         'responsible': 1360,\n",
       "         'them': 13702,\n",
       "         'going': 10186,\n",
       "         'college': 10300,\n",
       "         'one': 39158,\n",
       "         'year?': 3632,\n",
       "         'movie': 4746,\n",
       "         'kid': 919,\n",
       "         'fooled': 90,\n",
       "         'into': 20533,\n",
       "         'thinking': 2499,\n",
       "         'kill': 4087,\n",
       "         'him': 7141,\n",
       "         'he': 26309,\n",
       "         'lives': 1424,\n",
       "         'bubble': 293,\n",
       "         'most': 36034,\n",
       "         'his': 21721,\n",
       "         'life,': 1163,\n",
       "         'then': 11072,\n",
       "         'decides': 308,\n",
       "         'travel': 4606,\n",
       "         'world': 12994,\n",
       "         'portable': 204,\n",
       "         'germ-free': 2,\n",
       "         'bubble?': 75,\n",
       "         'computer': 7659,\n",
       "         'science': 7308,\n",
       "         'student': 8220,\n",
       "         'buy': 10354,\n",
       "         'final': 1905,\n",
       "         'year': 14236,\n",
       "         'project': 2597,\n",
       "         'from': 87311,\n",
       "         'outside': 2486,\n",
       "         'rather': 3662,\n",
       "         'own,': 110,\n",
       "         'education': 3345,\n",
       "         'system': 5629,\n",
       "         'really': 14984,\n",
       "         'week?': 969,\n",
       "         'some': 70202,\n",
       "         'ways': 8858,\n",
       "         'shorten': 69,\n",
       "         'period,': 118,\n",
       "         'risks': 1149,\n",
       "         'calead': 1,\n",
       "         'leap': 116,\n",
       "         'year.?': 10,\n",
       "         'days': 4346,\n",
       "         'rid': 3409,\n",
       "         'spleen': 19,\n",
       "         'enlargement?': 18,\n",
       "         'machine': 2848,\n",
       "         'techniques': 1545,\n",
       "         'extract': 312,\n",
       "         'metadata': 23,\n",
       "         '(font': 1,\n",
       "         'color,': 74,\n",
       "         'size,': 84,\n",
       "         'indentation': 7,\n",
       "         'alignment)': 1,\n",
       "         'document': 498,\n",
       "         '(.docx)': 1,\n",
       "         'file,': 37,\n",
       "         'integrated': 474,\n",
       "         'web': 3592,\n",
       "         'application?': 870,\n",
       "         'work': 17694,\n",
       "         'girls': 6766,\n",
       "         'hitch': 7,\n",
       "         'smith': 343,\n",
       "         'asks': 763,\n",
       "         'dance': 667,\n",
       "         'too': 6900,\n",
       "         'much?': 1799,\n",
       "         'india': 17693,\n",
       "         'act': 2860,\n",
       "         '1935': 15,\n",
       "         'was': 53098,\n",
       "         'special?': 249,\n",
       "         'sports': 1513,\n",
       "         'like?': 5723,\n",
       "         'dna': 979,\n",
       "         'rna': 79,\n",
       "         'compare': 1888,\n",
       "         'contrast?': 65,\n",
       "         'breaks': 316,\n",
       "         'shoot': 741,\n",
       "         'armed': 622,\n",
       "         'only': 17059,\n",
       "         'knife': 278,\n",
       "         'happens': 6925,\n",
       "         'now?': 4310,\n",
       "         'write': 7623,\n",
       "         'biography': 760,\n",
       "         'gianni': 7,\n",
       "         'versace?': 3,\n",
       "         'extroverted': 49,\n",
       "         'better': 17409,\n",
       "         'faster': 1308,\n",
       "         'processing': 560,\n",
       "         'expelling': 22,\n",
       "         'information': 3397,\n",
       "         'introverts?': 69,\n",
       "         'recognized': 337,\n",
       "         'place': 6540,\n",
       "         'very': 9230,\n",
       "         'far': 2494,\n",
       "         'home?': 2298,\n",
       "         'price': 2728,\n",
       "         'comparison': 448,\n",
       "         'websites': 2048,\n",
       "         'financial': 2761,\n",
       "         'services?': 1155,\n",
       "         'ragging': 80,\n",
       "         'happen': 9589,\n",
       "         'nift': 340,\n",
       "         'bangalore?': 2115,\n",
       "         'bad': 10199,\n",
       "         'reviews': 521,\n",
       "         'bahubali': 108,\n",
       "         '2': 8552,\n",
       "         'on': 118512,\n",
       "         'imdb?': 12,\n",
       "         'swallowing': 76,\n",
       "         'listerine': 13,\n",
       "         'dangerous?': 408,\n",
       "         'theories': 581,\n",
       "         'critical': 681,\n",
       "         'thinking?': 460,\n",
       "         'biggest': 3409,\n",
       "         'problems,': 123,\n",
       "         'questions,': 202,\n",
       "         'doubts': 105,\n",
       "         'come': 9361,\n",
       "         'across': 1514,\n",
       "         'trying': 3590,\n",
       "         'choose': 5246,\n",
       "         'paint': 681,\n",
       "         'color': 1765,\n",
       "         'room?': 614,\n",
       "         'cheap': 1619,\n",
       "         'flights': 368,\n",
       "         'edinburgh?': 49,\n",
       "         \"china's\": 515,\n",
       "         'chick?': 19,\n",
       "         'send': 3002,\n",
       "         'large': 2845,\n",
       "         'picture': 1320,\n",
       "         'files': 909,\n",
       "         'through': 9736,\n",
       "         'email?': 345,\n",
       "         \"doesn't\": 7740,\n",
       "         'ebay': 278,\n",
       "         'allow': 2729,\n",
       "         'sale': 531,\n",
       "         'wwii': 269,\n",
       "         'purple': 193,\n",
       "         'heart': 1987,\n",
       "         'medals': 53,\n",
       "         'even': 13768,\n",
       "         'though': 3640,\n",
       "         'categories': 192,\n",
       "         'specifically': 680,\n",
       "         'medals?': 11,\n",
       "         'characteristics': 2562,\n",
       "         'define': 1113,\n",
       "         'isovolumetric': 1,\n",
       "         'relaxation?': 16,\n",
       "         'cybersecurity': 72,\n",
       "         'overseas?': 170,\n",
       "         'developer': 1559,\n",
       "         'refer': 876,\n",
       "         'w3c': 4,\n",
       "         'standard': 1839,\n",
       "         'practice?': 409,\n",
       "         'internet': 2765,\n",
       "         'problem?': 1292,\n",
       "         'rely': 256,\n",
       "         'everything?': 653,\n",
       "         'itc': 54,\n",
       "         'charges': 766,\n",
       "         'levied': 30,\n",
       "         'banks?': 316,\n",
       "         'muhammed': 52,\n",
       "         'services': 2482,\n",
       "         'provided': 795,\n",
       "         'food': 5792,\n",
       "         'testing': 892,\n",
       "         'lab': 350,\n",
       "         'certified': 400,\n",
       "         'requirements?': 182,\n",
       "         'ok': 2194,\n",
       "         'solo': 487,\n",
       "         'whole': 2371,\n",
       "         'life': 16367,\n",
       "         'pretend': 452,\n",
       "         'exists?': 286,\n",
       "         'unwanted?': 5,\n",
       "         'change': 10622,\n",
       "         'owner': 657,\n",
       "         'current': 5495,\n",
       "         'youtube': 3245,\n",
       "         'account?': 2433,\n",
       "         'original': 1781,\n",
       "         'let': 3691,\n",
       "         'access': 1878,\n",
       "         'anything.': 163,\n",
       "         'archer': 30,\n",
       "         'characters': 1511,\n",
       "         'animated': 399,\n",
       "         'based': 4468,\n",
       "         'celebrities?': 92,\n",
       "         'best': 79116,\n",
       "         'propose': 288,\n",
       "         'without': 21120,\n",
       "         'annoying': 525,\n",
       "         'her?': 2066,\n",
       "         '$500': 102,\n",
       "         'million': 1515,\n",
       "         'solar': 1739,\n",
       "         'power?': 1191,\n",
       "         'recommended': 794,\n",
       "         '2d': 194,\n",
       "         'game': 4326,\n",
       "         'engines': 364,\n",
       "         'beginning': 703,\n",
       "         'python': 937,\n",
       "         'programmer?': 329,\n",
       "         'teleological': 3,\n",
       "         'pantheism?': 6,\n",
       "         'aircraft': 809,\n",
       "         'propulsion(m.s)?': 1,\n",
       "         'countries': 7074,\n",
       "         'freedom': 1161,\n",
       "         'speech': 1153,\n",
       "         'par': 119,\n",
       "         'us?': 3963,\n",
       "         'lead': 2096,\n",
       "         'red': 2817,\n",
       "         'terror': 284,\n",
       "         'ethiopia?': 57,\n",
       "         'pen': 442,\n",
       "         'low': 3948,\n",
       "         'cost': 5502,\n",
       "         'sorts': 94,\n",
       "         'things?': 1106,\n",
       "         'trust': 1640,\n",
       "         'facebook': 4427,\n",
       "         'page': 1401,\n",
       "         'its': 12959,\n",
       "         'website?': 1957,\n",
       "         'improve': 4875,\n",
       "         'piano': 479,\n",
       "         'skill': 713,\n",
       "         'keep': 8778,\n",
       "         'practicing': 372,\n",
       "         'pieces?': 81,\n",
       "         'effective': 3406,\n",
       "         'way?': 1800,\n",
       "         'wear': 3929,\n",
       "         'insulin': 110,\n",
       "         'pump,': 13,\n",
       "         'lot': 4360,\n",
       "         'like': 55520,\n",
       "         'it.': 2032,\n",
       "         'nick': 133,\n",
       "         'jonas': 31,\n",
       "         'wears': 192,\n",
       "         'dated': 282,\n",
       "         'selena': 68,\n",
       "         'gomez.': 1,\n",
       "         'difference': 15724,\n",
       "         'several': 1038,\n",
       "         'zeros': 45,\n",
       "         'missing': 710,\n",
       "         'bank': 4490,\n",
       "         'turkey': 948,\n",
       "         'seperated': 29,\n",
       "         'give': 13074,\n",
       "         'land': 2146,\n",
       "         'kurds': 165,\n",
       "         'because': 12232,\n",
       "         'foreign': 3574,\n",
       "         'powers': 611,\n",
       "         'usa': 4087,\n",
       "         'future?': 2531,\n",
       "         'r': 742,\n",
       "         'sq.': 19,\n",
       "         'cos-1': 1,\n",
       "         'r=18.5': 1,\n",
       "         'equals?': 16,\n",
       "         'sings': 44,\n",
       "         'song': 2803,\n",
       "         'head?': 588,\n",
       "         'chronicled': 10,\n",
       "         'replace': 1475,\n",
       "         'supply': 967,\n",
       "         'chain?': 156,\n",
       "         'solve': 2461,\n",
       "         'equation…': 2,\n",
       "         'sin(a*t)-b*t=0?': 1,\n",
       "         'demerits': 132,\n",
       "         'excellence': 48,\n",
       "         'academic': 1225,\n",
       "         'pursuits?': 5,\n",
       "         'indians': 4693,\n",
       "         'melbourne,': 46,\n",
       "         'australia?': 1647,\n",
       "         'physicists,': 10,\n",
       "         'mathematicians,': 8,\n",
       "         'scientists': 959,\n",
       "         'philosophers': 145,\n",
       "         'david': 784,\n",
       "         \"deutsch's\": 1,\n",
       "         \"'constructor\": 1,\n",
       "         \"theory'?\": 3,\n",
       "         'old': 11555,\n",
       "         'scriptures': 77,\n",
       "         'eastern': 914,\n",
       "         'cultures': 446,\n",
       "         'appear': 1665,\n",
       "         'lost': 2982,\n",
       "         'culture?': 1359,\n",
       "         'i.q,': 1,\n",
       "         'hate': 6355,\n",
       "         'numbers?': 605,\n",
       "         'up': 20368,\n",
       "         'mind': 3143,\n",
       "         'habits': 704,\n",
       "         'procrastination?': 93,\n",
       "         'relationship': 5465,\n",
       "         'napoleon': 154,\n",
       "         'ali': 372,\n",
       "         'pasha': 7,\n",
       "         'tepelene?': 1,\n",
       "         'presynaptic': 1,\n",
       "         'neurons': 123,\n",
       "         'found?': 293,\n",
       "         'narcissist': 1539,\n",
       "         'punish': 366,\n",
       "         'child': 4573,\n",
       "         'contact': 2446,\n",
       "         'goes': 1643,\n",
       "         'back': 9560,\n",
       "         'start': 15401,\n",
       "         'freelancing': 186,\n",
       "         'after': 39133,\n",
       "         'finishing': 223,\n",
       "         \"udacity's\": 15,\n",
       "         'android': 3815,\n",
       "         'nanodegree?': 10,\n",
       "         'reason': 4492,\n",
       "         'bitcoin?': 501,\n",
       "         'songs': 1715,\n",
       "         'long': 13508,\n",
       "         'journey?': 173,\n",
       "         'blacks': 700,\n",
       "         'support': 5011,\n",
       "         'choice': 1312,\n",
       "         'mandatory': 557,\n",
       "         'sentencing': 29,\n",
       "         'criminals': 387,\n",
       "         'vote': 1408,\n",
       "         'republican?': 74,\n",
       "         'added': 960,\n",
       "         'thrice': 49,\n",
       "         'rational': 394,\n",
       "         '-8/9': 2,\n",
       "         '4/7?': 1,\n",
       "         'four': 1484,\n",
       "         'movement': 812,\n",
       "         'symphony,': 2,\n",
       "         'typically': 581,\n",
       "         'form': 4321,\n",
       "         'second': 3461,\n",
       "         'movement?': 465,\n",
       "         'various': 1515,\n",
       "         'stock': 2906,\n",
       "         'exchanges?': 45,\n",
       "         'gay': 2765,\n",
       "         'boy': 2044,\n",
       "         'cousin': 393,\n",
       "         '(boy).': 3,\n",
       "         'sexy,': 19,\n",
       "         'dont': 1233,\n",
       "         'do.': 246,\n",
       "         'hot,': 52,\n",
       "         'want': 23648,\n",
       "         'di**.': 1,\n",
       "         'do?': 11227,\n",
       "         'races': 336,\n",
       "         'smallest': 522,\n",
       "         'penis?': 457,\n",
       "         'architecture,': 39,\n",
       "         'microarchitecture': 3,\n",
       "         'instruction': 111,\n",
       "         'set': 3414,\n",
       "         'architecture(isa)': 1,\n",
       "         'iphone': 2637,\n",
       "         'users': 1263,\n",
       "         'psychologically': 134,\n",
       "         'trapped': 228,\n",
       "         'brand': 1540,\n",
       "         'naming': 153,\n",
       "         'avoiding': 205,\n",
       "         'complain': 631,\n",
       "         'reasonable': 649,\n",
       "         'features': 1594,\n",
       "         'battery': 1299,\n",
       "         'performance,': 44,\n",
       "         'lacks': 111,\n",
       "         'while': 14708,\n",
       "         'other': 26885,\n",
       "         'flagship': 41,\n",
       "         '2017-2018': 38,\n",
       "         'much': 27919,\n",
       "         'semester': 763,\n",
       "         'ontario?': 95,\n",
       "         'females': 651,\n",
       "         'penises': 82,\n",
       "         'ugly?': 336,\n",
       "         'add': 2844,\n",
       "         'flour': 138,\n",
       "         'homemade': 502,\n",
       "         'chili?': 14,\n",
       "         'advice': 3320,\n",
       "         'wishes': 221,\n",
       "         'accomplish': 242,\n",
       "         'have?': 2451,\n",
       "         'overcome': 2067,\n",
       "         'extreme': 798,\n",
       "         'fear': 1982,\n",
       "         'insects': 217,\n",
       "         'bugs?': 84,\n",
       "         'examples': 5681,\n",
       "         'durable': 104,\n",
       "         'gloves?': 56,\n",
       "         'cars': 1564,\n",
       "         'move': 4660,\n",
       "         'miles': 495,\n",
       "         'towards': 3467,\n",
       "         '8': 2003,\n",
       "         'oppostie': 1,\n",
       "         'direction,': 38,\n",
       "         'distance': 2073,\n",
       "         'apps': 2235,\n",
       "         'given': 5872,\n",
       "         'day?': 3738,\n",
       "         'maximum': 1677,\n",
       "         '100': 1912,\n",
       "         'headers': 16,\n",
       "         'dataset?': 35,\n",
       "         'noticed': 616,\n",
       "         'print': 700,\n",
       "         'first': 15165,\n",
       "         '100.': 14,\n",
       "         'anyway': 315,\n",
       "         'that?': 2558,\n",
       "         'marry': 2332,\n",
       "         'american': 8590,\n",
       "         'woman': 4769,\n",
       "         'green': 2015,\n",
       "         'card?': 1393,\n",
       "         'charge?': 322,\n",
       "         'companies': 5673,\n",
       "         'look': 9585,\n",
       "         'recruiting': 169,\n",
       "         'expat?': 34,\n",
       "         'nationality': 148,\n",
       "         'candidate': 970,\n",
       "         'play': 5648,\n",
       "         'huge': 1316,\n",
       "         'role?': 286,\n",
       "         'transcranial': 5,\n",
       "         'magnetic': 919,\n",
       "         'stimulation': 42,\n",
       "         '(tms)': 1,\n",
       "         'increase': 4492,\n",
       "         'cognitive': 503,\n",
       "         'performance?': 419,\n",
       "         'but,': 138,\n",
       "         'george': 973,\n",
       "         'washington': 529,\n",
       "         'free': 7718,\n",
       "         'slaves': 403,\n",
       "         'happened': 3918,\n",
       "         'revolutionary': 198,\n",
       "         'time': 20447,\n",
       "         'one-child': 16,\n",
       "         'policy?': 585,\n",
       "         'us': 16429,\n",
       "         'embassy': 244,\n",
       "         'deny': 608,\n",
       "         'citizen': 1097,\n",
       "         'entry': 1132,\n",
       "         'embassy?': 59,\n",
       "         '1920s': 11,\n",
       "         'party?': 920,\n",
       "         'test': 4834,\n",
       "         'energy': 3808,\n",
       "         'meter': 261,\n",
       "         'periodically?': 19,\n",
       "         'stationery': 27,\n",
       "         'office': 1859,\n",
       "         'vs': 2885,\n",
       "         'regular': 1691,\n",
       "         'office?': 825,\n",
       "         'genuine': 689,\n",
       "         'aviation': 244,\n",
       "         'jobs?': 1197,\n",
       "         'nonprofit': 45,\n",
       "         'organizations': 659,\n",
       "         'treat': 3509,\n",
       "         'employees': 1185,\n",
       "         'badly?': 181,\n",
       "         'procedure': 2005,\n",
       "         'invest': 3142,\n",
       "         'mutual': 1111,\n",
       "         'fund?': 255,\n",
       "         'osn?': 1,\n",
       "         'aesthetically': 32,\n",
       "         'pleasing': 54,\n",
       "         'shade': 97,\n",
       "         'green?': 139,\n",
       "         'wise': 809,\n",
       "         'm.com': 47,\n",
       "         '(2017-2019)': 1,\n",
       "         'calcutta': 142,\n",
       "         'preparing': 1588,\n",
       "         'cat': 3002,\n",
       "         'politician': 284,\n",
       "         'leech?': 4,\n",
       "         'china': 5684,\n",
       "         'sponsor': 316,\n",
       "         'regime': 273,\n",
       "         'north': 5457,\n",
       "         'korea?': 1212,\n",
       "         'europeans': 1005,\n",
       "         \"they're\": 1556,\n",
       "         'superior': 803,\n",
       "         'race,': 180,\n",
       "         'fact': 2412,\n",
       "         'took': 1669,\n",
       "         '2,000': 44,\n",
       "         'years': 11931,\n",
       "         'until': 1947,\n",
       "         'mid': 420,\n",
       "         '19th': 244,\n",
       "         'century': 623,\n",
       "         'surpass': 134,\n",
       "         'economy?': 983,\n",
       "         'cytoplasm': 17,\n",
       "         'built': 1010,\n",
       "         'inside': 2229,\n",
       "         'lip?': 80,\n",
       "         \"i'm\": 16480,\n",
       "         'creating': 1318,\n",
       "         'app': 5450,\n",
       "         'multiple': 2261,\n",
       "         'programming': 3356,\n",
       "         'languages,': 120,\n",
       "         'ide?': 23,\n",
       "         'neighborhoods': 788,\n",
       "         'chuncheon,': 3,\n",
       "         'cpss/pabt': 2,\n",
       "         '4afsb': 1,\n",
       "         'varanasi?': 88,\n",
       "         'wrongfully': 27,\n",
       "         'jumped': 76,\n",
       "         'conclusions': 64,\n",
       "         'troubleshooting': 29,\n",
       "         'hewlett-packard': 10,\n",
       "         'printer?': 108,\n",
       "         \"isn't\": 4565,\n",
       "         'unfair': 273,\n",
       "         'men': 8908,\n",
       "         'duty': 492,\n",
       "         'nsf': 9,\n",
       "         'spinoff': 6,\n",
       "         'nsfnet': 1,\n",
       "         'helped': 775,\n",
       "         'sun': 1276,\n",
       "         'microsystem?': 2,\n",
       "         'dangerous': 1499,\n",
       "         'tourist?': 94,\n",
       "         'known': 5527,\n",
       "         'drastically': 115,\n",
       "         'changed?': 286,\n",
       "         'louisville': 9,\n",
       "         'listed': 316,\n",
       "         'qs': 24,\n",
       "         'ranking?': 81,\n",
       "         'julius': 136,\n",
       "         'caesar': 103,\n",
       "         'bring': 2445,\n",
       "         'tyrannosaurus': 22,\n",
       "         'rex': 73,\n",
       "         'campaigns': 108,\n",
       "         'frighten': 22,\n",
       "         'celts': 17,\n",
       "         'submission?': 35,\n",
       "         'mercury': 318,\n",
       "         '(debilitated)': 2,\n",
       "         'dasa': 51,\n",
       "         'simha': 3,\n",
       "         'lagna': 55,\n",
       "         'be?': 3138,\n",
       "         'drivers': 574,\n",
       "         'globalisation?': 25,\n",
       "         'high': 10428,\n",
       "         'emotional': 1265,\n",
       "         ...})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        \n",
    "        hidden_size = 64\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab),embed_size)\n",
    "        self.embedding.weight = nn.Parameter(TEXT.vocab.vectors)\n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size*4,2)\n",
    "        self.relu = nn.ReLU()\n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(16,1)\n",
    "        \n",
    "    def forward(self,train):\n",
    "        #print(train.shape)\n",
    "        h_embedding = self.embedding(train)\n",
    "        #print(h_embedding.shape)\n",
    "        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding,0)))\n",
    "        #print(h_embedding.shape)\n",
    "        \n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        #print(h_lstm.shape)\n",
    "        h_gru, _ = self.gru(h_lstm)\n",
    "        #print(h_gru.shape)\n",
    "        \n",
    "        avg_pool = torch.mean(h_gru,1)\n",
    "        #print(avg_pool.shape)\n",
    "        #avg_pool = avg_pool.view(-1,64)\n",
    "        max_pool, _ = torch.max(h_gru,1)\n",
    "        #print(max_pool.shape)\n",
    "        \n",
    "        \n",
    "        conc = torch.cat((avg_pool, max_pool),1)\n",
    "        print(conc.shape)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        #print(conc.shape)\n",
    "        #conc = self.dropout(conc)\n",
    "        print(conc.shape)\n",
    "        #out = nn.Sigmoid()(conc)\n",
    "        \n",
    "        out = nn.LogSoftmax(dim=1)(conc)\n",
    "        \n",
    "        #out = self.out(conc)\n",
    "        #print(out[:,-1].shape)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "batch = next(iter(train_iter)).question_text\n",
    "net = NeuralNet()\n",
    "#net(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch = next(iter(train_iter)).question_text\n",
    "#batch[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        hidden_size = 6\n",
    "        \n",
    "        self.hidden_dim = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab),embed_size)\n",
    "        self.embedding.weight = nn.Parameter(TEXT.vocab.vectors)\n",
    "        #self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size)\n",
    "        \n",
    "        self.hidden2tag = nn.Linear(hidden_size, 2)\n",
    "    \n",
    "    def forward(self, train):\n",
    "        #print(train.shape)\n",
    "        embeds = self.embedding(train)\n",
    "        #print(embeds.shape)\n",
    "        #print(embeds.view(len(train),64,-1).shape)\n",
    "        #print(embeds.view(len(train),train.shape[1],-1).shape)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(train),train.shape[1],-1))\n",
    "        #print(lstm_out.shape)\n",
    "        avg_pool = torch.mean(lstm_out,0)\n",
    "        #print(avg_pool.shape)\n",
    "        target_space = self.hidden2tag(avg_pool)#.view(len(train),-1))\n",
    "        #print(target_space.shape)\n",
    "        target_score = F.log_softmax(target_space, dim=1)\n",
    "        #print(target_score.shape)\n",
    "        return target_score\n",
    "\n",
    "inputs_check = next(iter(train_iter)).question_text\n",
    "\n",
    "#print(inputs_check.shape)\n",
    "\n",
    "model = LSTMTagger()\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = inputs_check\n",
    "    #print(len(inputs))\n",
    "    score = model(inputs)\n",
    "    #print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9397, -0.4955],\n",
      "        [-0.9360, -0.4979],\n",
      "        [-0.9256, -0.5047],\n",
      "        [-0.9498, -0.4891],\n",
      "        [-0.9271, -0.5037],\n",
      "        [-0.9559, -0.4852],\n",
      "        [-0.9399, -0.4954],\n",
      "        [-0.9254, -0.5048],\n",
      "        [-0.9422, -0.4939],\n",
      "        [-0.9159, -0.5111],\n",
      "        [-0.9479, -0.4903],\n",
      "        [-0.8890, -0.5294],\n",
      "        [-0.9237, -0.5059],\n",
      "        [-0.9445, -0.4924],\n",
      "        [-0.9189, -0.5091],\n",
      "        [-0.9355, -0.4982],\n",
      "        [-0.9516, -0.4880],\n",
      "        [-0.9352, -0.4984],\n",
      "        [-0.9625, -0.4812],\n",
      "        [-0.9124, -0.5134],\n",
      "        [-0.9520, -0.4877],\n",
      "        [-0.8975, -0.5235],\n",
      "        [-0.9156, -0.5113],\n",
      "        [-0.9147, -0.5119],\n",
      "        [-0.9469, -0.4909],\n",
      "        [-0.9699, -0.4766],\n",
      "        [-0.9212, -0.5076],\n",
      "        [-0.9472, -0.4907],\n",
      "        [-0.9660, -0.4790],\n",
      "        [-0.9336, -0.4994],\n",
      "        [-0.9413, -0.4945],\n",
      "        [-0.9315, -0.5008],\n",
      "        [-0.9138, -0.5125],\n",
      "        [-0.9319, -0.5005],\n",
      "        [-0.9033, -0.5196],\n",
      "        [-0.9257, -0.5046],\n",
      "        [-0.9364, -0.4977],\n",
      "        [-0.8820, -0.5344],\n",
      "        [-0.9559, -0.4852],\n",
      "        [-0.9340, -0.4992],\n",
      "        [-0.9781, -0.4717],\n",
      "        [-0.9358, -0.4981],\n",
      "        [-0.9299, -0.5019],\n",
      "        [-0.9706, -0.4762],\n",
      "        [-0.9219, -0.5071],\n",
      "        [-0.8965, -0.5242],\n",
      "        [-0.9085, -0.5160],\n",
      "        [-0.9329, -0.4999],\n",
      "        [-0.9217, -0.5072],\n",
      "        [-0.9140, -0.5123],\n",
      "        [-0.9352, -0.4984],\n",
      "        [-0.9451, -0.4920],\n",
      "        [-0.9308, -0.5013],\n",
      "        [-0.9117, -0.5139],\n",
      "        [-0.9055, -0.5181],\n",
      "        [-0.9178, -0.5098],\n",
      "        [-0.9226, -0.5067],\n",
      "        [-0.9622, -0.4813],\n",
      "        [-0.9671, -0.4784],\n",
      "        [-0.9166, -0.5106],\n",
      "        [-0.9881, -0.4657],\n",
      "        [-0.8992, -0.5224],\n",
      "        [-0.9150, -0.5117],\n",
      "        [-0.8900, -0.5287],\n",
      "        [-0.9581, -0.4839],\n",
      "        [-0.9525, -0.4874],\n",
      "        [-0.9265, -0.5041],\n",
      "        [-0.9167, -0.5105],\n",
      "        [-0.8989, -0.5226],\n",
      "        [-0.9197, -0.5086],\n",
      "        [-0.9249, -0.5051],\n",
      "        [-0.9315, -0.5008],\n",
      "        [-0.9139, -0.5124],\n",
      "        [-0.9533, -0.4869],\n",
      "        [-0.9515, -0.4880],\n",
      "        [-0.9479, -0.4903],\n",
      "        [-0.9246, -0.5053],\n",
      "        [-0.9329, -0.4999],\n",
      "        [-0.9837, -0.4683],\n",
      "        [-0.9124, -0.5135],\n",
      "        [-0.9519, -0.4878],\n",
      "        [-0.9836, -0.4683],\n",
      "        [-0.9339, -0.4993],\n",
      "        [-0.9018, -0.5206],\n",
      "        [-0.9226, -0.5067],\n",
      "        [-0.9108, -0.5145],\n",
      "        [-0.9506, -0.4886],\n",
      "        [-0.9715, -0.4757],\n",
      "        [-0.9040, -0.5191],\n",
      "        [-0.9341, -0.4991],\n",
      "        [-0.9221, -0.5070],\n",
      "        [-0.9688, -0.4773],\n",
      "        [-0.9855, -0.4672],\n",
      "        [-0.9031, -0.5197],\n",
      "        [-0.9527, -0.4873],\n",
      "        [-0.9691, -0.4771],\n",
      "        [-0.9191, -0.5090],\n",
      "        [-0.8762, -0.5385],\n",
      "        [-1.0134, -0.4510],\n",
      "        [-0.9249, -0.5052],\n",
      "        [-0.8928, -0.5268],\n",
      "        [-1.0245, -0.4447],\n",
      "        [-0.9082, -0.5162],\n",
      "        [-0.9543, -0.4863],\n",
      "        [-0.9225, -0.5067],\n",
      "        [-0.9267, -0.5039],\n",
      "        [-0.9267, -0.5039],\n",
      "        [-0.9209, -0.5077],\n",
      "        [-0.9769, -0.4724],\n",
      "        [-0.9461, -0.4914],\n",
      "        [-0.9653, -0.4794],\n",
      "        [-0.8946, -0.5256],\n",
      "        [-0.9277, -0.5033],\n",
      "        [-0.9174, -0.5101],\n",
      "        [-0.9759, -0.4730],\n",
      "        [-0.9793, -0.4709],\n",
      "        [-0.9488, -0.4897],\n",
      "        [-0.9148, -0.5118],\n",
      "        [-0.9437, -0.4929],\n",
      "        [-0.9389, -0.4960],\n",
      "        [-0.9321, -0.5005],\n",
      "        [-0.9334, -0.4996],\n",
      "        [-0.9216, -0.5073],\n",
      "        [-0.9290, -0.5024],\n",
      "        [-0.9224, -0.5068],\n",
      "        [-0.8943, -0.5258],\n",
      "        [-1.0497, -0.4309],\n",
      "        [-0.9139, -0.5124],\n",
      "        [-0.9361, -0.4978],\n",
      "        [-1.0666, -0.4219],\n",
      "        [-0.9680, -0.4778],\n",
      "        [-0.9250, -0.5051],\n",
      "        [-0.9932, -0.4626],\n",
      "        [-0.9097, -0.5153],\n",
      "        [-0.9360, -0.4979],\n",
      "        [-0.9118, -0.5138],\n",
      "        [-0.9219, -0.5071],\n",
      "        [-0.9452, -0.4920],\n",
      "        [-0.9178, -0.5098],\n",
      "        [-0.9276, -0.5034],\n",
      "        [-0.9146, -0.5119],\n",
      "        [-0.9566, -0.4849],\n",
      "        [-0.9299, -0.5019],\n",
      "        [-0.9218, -0.5071],\n",
      "        [-0.9178, -0.5098],\n",
      "        [-0.9911, -0.4639],\n",
      "        [-0.9323, -0.5003],\n",
      "        [-0.9406, -0.4950],\n",
      "        [-0.9345, -0.4988],\n",
      "        [-0.9406, -0.4950],\n",
      "        [-0.9326, -0.5001],\n",
      "        [-0.9281, -0.5030],\n",
      "        [-0.9301, -0.5017],\n",
      "        [-0.9639, -0.4803],\n",
      "        [-0.9253, -0.5049],\n",
      "        [-0.9974, -0.4602],\n",
      "        [-0.9515, -0.4880],\n",
      "        [-0.9194, -0.5087],\n",
      "        [-0.9159, -0.5111],\n",
      "        [-0.9315, -0.5008],\n",
      "        [-0.9385, -0.4963],\n",
      "        [-0.8946, -0.5255],\n",
      "        [-0.9406, -0.4950],\n",
      "        [-0.9243, -0.5055],\n",
      "        [-0.9314, -0.5009],\n",
      "        [-0.9175, -0.5100],\n",
      "        [-0.9035, -0.5195],\n",
      "        [-0.9184, -0.5095],\n",
      "        [-0.9563, -0.4850],\n",
      "        [-0.9801, -0.4704],\n",
      "        [-0.9339, -0.4993],\n",
      "        [-0.9386, -0.4962],\n",
      "        [-0.9172, -0.5102],\n",
      "        [-0.9349, -0.4986],\n",
      "        [-0.9691, -0.4771],\n",
      "        [-0.9395, -0.4956],\n",
      "        [-0.9443, -0.4926],\n",
      "        [-0.9546, -0.4861],\n",
      "        [-0.9286, -0.5027],\n",
      "        [-0.9249, -0.5051],\n",
      "        [-0.9251, -0.5050],\n",
      "        [-0.9385, -0.4963],\n",
      "        [-0.9266, -0.5040],\n",
      "        [-0.9258, -0.5045],\n",
      "        [-0.9906, -0.4642],\n",
      "        [-0.9384, -0.4964],\n",
      "        [-0.9204, -0.5081],\n",
      "        [-1.0122, -0.4517],\n",
      "        [-0.9067, -0.5172],\n",
      "        [-0.9409, -0.4948],\n",
      "        [-1.0136, -0.4509],\n",
      "        [-0.9293, -0.5023],\n",
      "        [-0.9373, -0.4971],\n",
      "        [-0.9699, -0.4766],\n",
      "        [-0.9805, -0.4702],\n",
      "        [-0.9365, -0.4976],\n",
      "        [-1.0662, -0.4221],\n",
      "        [-0.9330, -0.4999],\n",
      "        [-0.9590, -0.4833],\n",
      "        [-0.9499, -0.4890],\n",
      "        [-0.9543, -0.4863],\n",
      "        [-0.9353, -0.4983],\n",
      "        [-0.9082, -0.5163],\n",
      "        [-0.9396, -0.4956],\n",
      "        [-0.9507, -0.4885],\n",
      "        [-0.9597, -0.4829],\n",
      "        [-0.9410, -0.4947],\n",
      "        [-0.9711, -0.4759],\n",
      "        [-0.9201, -0.5083],\n",
      "        [-0.9410, -0.4947],\n",
      "        [-0.8948, -0.5254],\n",
      "        [-0.9291, -0.5024],\n",
      "        [-0.9907, -0.4641],\n",
      "        [-0.9522, -0.4876],\n",
      "        [-0.9593, -0.4831],\n",
      "        [-0.9416, -0.4943],\n",
      "        [-0.9416, -0.4943],\n",
      "        [-0.9112, -0.5142],\n",
      "        [-0.9415, -0.4944],\n",
      "        [-0.9223, -0.5068],\n",
      "        [-0.9366, -0.4975],\n",
      "        [-0.8614, -0.5492],\n",
      "        [-0.9600, -0.4827],\n",
      "        [-0.9416, -0.4943],\n",
      "        [-0.9362, -0.4978],\n",
      "        [-0.9395, -0.4956],\n",
      "        [-0.9347, -0.4987],\n",
      "        [-0.9354, -0.4983],\n",
      "        [-0.9362, -0.4977],\n",
      "        [-0.9128, -0.5131],\n",
      "        [-0.9416, -0.4943],\n",
      "        [-0.9053, -0.5182],\n",
      "        [-0.9337, -0.4994],\n",
      "        [-0.9119, -0.5137],\n",
      "        [-0.9337, -0.4994],\n",
      "        [-0.9916, -0.4636],\n",
      "        [-0.9166, -0.5106],\n",
      "        [-0.9028, -0.5199],\n",
      "        [-0.9338, -0.4994],\n",
      "        [-0.9559, -0.4853],\n",
      "        [-0.8869, -0.5309],\n",
      "        [-0.9412, -0.4945],\n",
      "        [-0.9214, -0.5074],\n",
      "        [-0.9094, -0.5155],\n",
      "        [-0.9273, -0.5036],\n",
      "        [-0.9234, -0.5061],\n",
      "        [-0.9218, -0.5072],\n",
      "        [-1.0342, -0.4393],\n",
      "        [-0.9737, -0.4743],\n",
      "        [-0.9369, -0.4973],\n",
      "        [-0.9550, -0.4858],\n",
      "        [-0.9454, -0.4919],\n",
      "        [-0.9005, -0.5215],\n",
      "        [-0.9238, -0.5058],\n",
      "        [-0.9240, -0.5057],\n",
      "        [-0.9146, -0.5120]])\n",
      "0 out of:  511\n",
      "tensor(0.9145, grad_fn=<NllLossBackward>)\n",
      "1 out of:  511\n",
      "tensor(0.7842, grad_fn=<NllLossBackward>)\n",
      "2 out of:  511\n",
      "tensor(0.6835, grad_fn=<NllLossBackward>)\n",
      "3 out of:  511\n",
      "tensor(0.6137, grad_fn=<NllLossBackward>)\n",
      "4 out of:  511\n",
      "tensor(0.5498, grad_fn=<NllLossBackward>)\n",
      "5 out of:  511\n",
      "tensor(0.4969, grad_fn=<NllLossBackward>)\n",
      "6 out of:  511\n",
      "tensor(0.4476, grad_fn=<NllLossBackward>)\n",
      "7 out of:  511\n",
      "tensor(0.4261, grad_fn=<NllLossBackward>)\n",
      "8 out of:  511\n",
      "tensor(0.3917, grad_fn=<NllLossBackward>)\n",
      "9 out of:  511\n",
      "tensor(0.3545, grad_fn=<NllLossBackward>)\n",
      "10 out of:  511\n",
      "tensor(0.3560, grad_fn=<NllLossBackward>)\n",
      "11 out of:  511\n",
      "tensor(0.3211, grad_fn=<NllLossBackward>)\n",
      "12 out of:  511\n",
      "tensor(0.3260, grad_fn=<NllLossBackward>)\n",
      "13 out of:  511\n",
      "tensor(0.3106, grad_fn=<NllLossBackward>)\n",
      "14 out of:  511\n",
      "tensor(0.3034, grad_fn=<NllLossBackward>)\n",
      "15 out of:  511\n",
      "tensor(0.2653, grad_fn=<NllLossBackward>)\n",
      "16 out of:  511\n",
      "tensor(0.2433, grad_fn=<NllLossBackward>)\n",
      "17 out of:  511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2500, grad_fn=<NllLossBackward>)\n",
      "18 out of:  511\n",
      "tensor(0.2792, grad_fn=<NllLossBackward>)\n",
      "19 out of:  511\n",
      "tensor(0.2333, grad_fn=<NllLossBackward>)\n",
      "20 out of:  511\n",
      "tensor(0.2833, grad_fn=<NllLossBackward>)\n",
      "21 out of:  511\n",
      "tensor(0.2357, grad_fn=<NllLossBackward>)\n",
      "22 out of:  511\n",
      "tensor(0.2512, grad_fn=<NllLossBackward>)\n",
      "23 out of:  511\n",
      "tensor(0.2514, grad_fn=<NllLossBackward>)\n",
      "24 out of:  511\n",
      "tensor(0.2143, grad_fn=<NllLossBackward>)\n",
      "25 out of:  511\n",
      "tensor(0.1777, grad_fn=<NllLossBackward>)\n",
      "26 out of:  511\n",
      "tensor(0.2948, grad_fn=<NllLossBackward>)\n",
      "27 out of:  511\n",
      "tensor(0.2190, grad_fn=<NllLossBackward>)\n",
      "28 out of:  511\n",
      "tensor(0.2172, grad_fn=<NllLossBackward>)\n",
      "29 out of:  511\n",
      "tensor(0.2576, grad_fn=<NllLossBackward>)\n",
      "30 out of:  511\n",
      "tensor(0.2067, grad_fn=<NllLossBackward>)\n",
      "31 out of:  511\n",
      "tensor(0.2557, grad_fn=<NllLossBackward>)\n",
      "32 out of:  511\n",
      "tensor(0.2066, grad_fn=<NllLossBackward>)\n",
      "33 out of:  511\n",
      "tensor(0.2638, grad_fn=<NllLossBackward>)\n",
      "34 out of:  511\n",
      "tensor(0.2879, grad_fn=<NllLossBackward>)\n",
      "35 out of:  511\n",
      "tensor(0.2852, grad_fn=<NllLossBackward>)\n",
      "36 out of:  511\n",
      "tensor(0.2108, grad_fn=<NllLossBackward>)\n",
      "37 out of:  511\n",
      "tensor(0.2286, grad_fn=<NllLossBackward>)\n",
      "38 out of:  511\n",
      "tensor(0.2709, grad_fn=<NllLossBackward>)\n",
      "39 out of:  511\n",
      "tensor(0.1847, grad_fn=<NllLossBackward>)\n",
      "40 out of:  511\n",
      "tensor(0.1976, grad_fn=<NllLossBackward>)\n",
      "41 out of:  511\n",
      "tensor(0.2518, grad_fn=<NllLossBackward>)\n",
      "42 out of:  511\n",
      "tensor(0.3167, grad_fn=<NllLossBackward>)\n",
      "43 out of:  511\n",
      "tensor(0.2518, grad_fn=<NllLossBackward>)\n",
      "44 out of:  511\n",
      "tensor(0.2686, grad_fn=<NllLossBackward>)\n",
      "45 out of:  511\n",
      "tensor(0.2040, grad_fn=<NllLossBackward>)\n",
      "46 out of:  511\n",
      "tensor(0.2746, grad_fn=<NllLossBackward>)\n",
      "47 out of:  511\n",
      "tensor(0.1430, grad_fn=<NllLossBackward>)\n",
      "48 out of:  511\n",
      "tensor(0.2403, grad_fn=<NllLossBackward>)\n",
      "49 out of:  511\n",
      "tensor(0.2017, grad_fn=<NllLossBackward>)\n",
      "50 out of:  511\n",
      "tensor(0.3035, grad_fn=<NllLossBackward>)\n",
      "51 out of:  511\n",
      "tensor(0.1944, grad_fn=<NllLossBackward>)\n",
      "52 out of:  511\n",
      "tensor(0.2476, grad_fn=<NllLossBackward>)\n",
      "53 out of:  511\n",
      "tensor(0.1738, grad_fn=<NllLossBackward>)\n",
      "54 out of:  511\n",
      "tensor(0.2663, grad_fn=<NllLossBackward>)\n",
      "55 out of:  511\n",
      "tensor(0.2302, grad_fn=<NllLossBackward>)\n",
      "56 out of:  511\n",
      "tensor(0.2686, grad_fn=<NllLossBackward>)\n",
      "57 out of:  511\n",
      "tensor(0.2400, grad_fn=<NllLossBackward>)\n",
      "58 out of:  511\n",
      "tensor(0.2815, grad_fn=<NllLossBackward>)\n",
      "59 out of:  511\n",
      "tensor(0.2194, grad_fn=<NllLossBackward>)\n",
      "60 out of:  511\n",
      "tensor(0.2337, grad_fn=<NllLossBackward>)\n",
      "61 out of:  511\n",
      "tensor(0.2411, grad_fn=<NllLossBackward>)\n",
      "62 out of:  511\n",
      "tensor(0.2316, grad_fn=<NllLossBackward>)\n",
      "63 out of:  511\n",
      "tensor(0.2879, grad_fn=<NllLossBackward>)\n",
      "64 out of:  511\n",
      "tensor(0.2099, grad_fn=<NllLossBackward>)\n",
      "65 out of:  511\n",
      "tensor(0.2122, grad_fn=<NllLossBackward>)\n",
      "66 out of:  511\n",
      "tensor(0.1732, grad_fn=<NllLossBackward>)\n",
      "67 out of:  511\n",
      "tensor(0.2109, grad_fn=<NllLossBackward>)\n",
      "68 out of:  511\n",
      "tensor(0.2012, grad_fn=<NllLossBackward>)\n",
      "69 out of:  511\n",
      "tensor(0.2492, grad_fn=<NllLossBackward>)\n",
      "70 out of:  511\n",
      "tensor(0.1671, grad_fn=<NllLossBackward>)\n",
      "71 out of:  511\n",
      "tensor(0.2087, grad_fn=<NllLossBackward>)\n",
      "72 out of:  511\n",
      "tensor(0.2454, grad_fn=<NllLossBackward>)\n",
      "73 out of:  511\n",
      "tensor(0.2243, grad_fn=<NllLossBackward>)\n",
      "74 out of:  511\n",
      "tensor(0.2322, grad_fn=<NllLossBackward>)\n",
      "75 out of:  511\n",
      "tensor(0.1595, grad_fn=<NllLossBackward>)\n",
      "76 out of:  511\n",
      "tensor(0.2004, grad_fn=<NllLossBackward>)\n",
      "77 out of:  511\n",
      "tensor(0.3449, grad_fn=<NllLossBackward>)\n",
      "78 out of:  511\n",
      "tensor(0.2586, grad_fn=<NllLossBackward>)\n",
      "79 out of:  511\n",
      "tensor(0.1884, grad_fn=<NllLossBackward>)\n",
      "80 out of:  511\n",
      "tensor(0.1916, grad_fn=<NllLossBackward>)\n",
      "81 out of:  511\n",
      "tensor(0.1890, grad_fn=<NllLossBackward>)\n",
      "82 out of:  511\n",
      "tensor(0.2990, grad_fn=<NllLossBackward>)\n",
      "83 out of:  511\n",
      "tensor(0.2095, grad_fn=<NllLossBackward>)\n",
      "84 out of:  511\n",
      "tensor(0.1778, grad_fn=<NllLossBackward>)\n",
      "85 out of:  511\n",
      "tensor(0.1753, grad_fn=<NllLossBackward>)\n",
      "86 out of:  511\n",
      "tensor(0.1592, grad_fn=<NllLossBackward>)\n",
      "87 out of:  511\n",
      "tensor(0.2161, grad_fn=<NllLossBackward>)\n",
      "88 out of:  511\n",
      "tensor(0.2291, grad_fn=<NllLossBackward>)\n",
      "89 out of:  511\n",
      "tensor(0.2700, grad_fn=<NllLossBackward>)\n",
      "90 out of:  511\n",
      "tensor(0.2176, grad_fn=<NllLossBackward>)\n",
      "91 out of:  511\n",
      "tensor(0.2671, grad_fn=<NllLossBackward>)\n",
      "92 out of:  511\n",
      "tensor(0.1889, grad_fn=<NllLossBackward>)\n",
      "93 out of:  511\n",
      "tensor(0.1805, grad_fn=<NllLossBackward>)\n",
      "94 out of:  511\n",
      "tensor(0.2609, grad_fn=<NllLossBackward>)\n",
      "95 out of:  511\n",
      "tensor(0.2483, grad_fn=<NllLossBackward>)\n",
      "96 out of:  511\n",
      "tensor(0.2108, grad_fn=<NllLossBackward>)\n",
      "97 out of:  511\n",
      "tensor(0.1253, grad_fn=<NllLossBackward>)\n",
      "98 out of:  511\n",
      "tensor(0.2818, grad_fn=<NllLossBackward>)\n",
      "99 out of:  511\n",
      "tensor(0.2175, grad_fn=<NllLossBackward>)\n",
      "100 out of:  511\n",
      "tensor(0.2507, grad_fn=<NllLossBackward>)\n",
      "101 out of:  511\n",
      "tensor(0.2474, grad_fn=<NllLossBackward>)\n",
      "102 out of:  511\n",
      "tensor(0.2070, grad_fn=<NllLossBackward>)\n",
      "103 out of:  511\n",
      "tensor(0.1979, grad_fn=<NllLossBackward>)\n",
      "104 out of:  511\n",
      "tensor(0.1937, grad_fn=<NllLossBackward>)\n",
      "105 out of:  511\n",
      "tensor(0.2296, grad_fn=<NllLossBackward>)\n",
      "106 out of:  511\n",
      "tensor(0.2049, grad_fn=<NllLossBackward>)\n",
      "107 out of:  511\n",
      "tensor(0.2383, grad_fn=<NllLossBackward>)\n",
      "108 out of:  511\n",
      "tensor(0.2400, grad_fn=<NllLossBackward>)\n",
      "109 out of:  511\n",
      "tensor(0.1957, grad_fn=<NllLossBackward>)\n",
      "110 out of:  511\n",
      "tensor(0.2455, grad_fn=<NllLossBackward>)\n",
      "111 out of:  511\n",
      "tensor(0.2240, grad_fn=<NllLossBackward>)\n",
      "112 out of:  511\n",
      "tensor(0.1949, grad_fn=<NllLossBackward>)\n",
      "113 out of:  511\n",
      "tensor(0.2302, grad_fn=<NllLossBackward>)\n",
      "114 out of:  511\n",
      "tensor(0.2711, grad_fn=<NllLossBackward>)\n",
      "115 out of:  511\n",
      "tensor(0.3011, grad_fn=<NllLossBackward>)\n",
      "116 out of:  511\n",
      "tensor(0.1957, grad_fn=<NllLossBackward>)\n",
      "117 out of:  511\n",
      "tensor(0.1872, grad_fn=<NllLossBackward>)\n",
      "118 out of:  511\n",
      "tensor(0.2566, grad_fn=<NllLossBackward>)\n",
      "119 out of:  511\n",
      "tensor(0.1658, grad_fn=<NllLossBackward>)\n",
      "120 out of:  511\n",
      "tensor(0.2613, grad_fn=<NllLossBackward>)\n",
      "121 out of:  511\n",
      "tensor(0.2314, grad_fn=<NllLossBackward>)\n",
      "122 out of:  511\n",
      "tensor(0.2251, grad_fn=<NllLossBackward>)\n",
      "123 out of:  511\n",
      "tensor(0.2610, grad_fn=<NllLossBackward>)\n",
      "124 out of:  511\n",
      "tensor(0.2973, grad_fn=<NllLossBackward>)\n",
      "125 out of:  511\n",
      "tensor(0.2105, grad_fn=<NllLossBackward>)\n",
      "126 out of:  511\n",
      "tensor(0.2364, grad_fn=<NllLossBackward>)\n",
      "127 out of:  511\n",
      "tensor(0.2346, grad_fn=<NllLossBackward>)\n",
      "128 out of:  511\n",
      "tensor(0.2577, grad_fn=<NllLossBackward>)\n",
      "129 out of:  511\n",
      "tensor(0.1759, grad_fn=<NllLossBackward>)\n",
      "130 out of:  511\n",
      "tensor(0.2386, grad_fn=<NllLossBackward>)\n",
      "131 out of:  511\n",
      "tensor(0.2295, grad_fn=<NllLossBackward>)\n",
      "132 out of:  511\n",
      "tensor(0.2721, grad_fn=<NllLossBackward>)\n",
      "133 out of:  511\n",
      "tensor(0.2184, grad_fn=<NllLossBackward>)\n",
      "134 out of:  511\n",
      "tensor(0.2798, grad_fn=<NllLossBackward>)\n",
      "135 out of:  511\n",
      "tensor(0.2709, grad_fn=<NllLossBackward>)\n",
      "136 out of:  511\n",
      "tensor(0.2189, grad_fn=<NllLossBackward>)\n",
      "137 out of:  511\n",
      "tensor(0.2045, grad_fn=<NllLossBackward>)\n",
      "138 out of:  511\n",
      "tensor(0.1806, grad_fn=<NllLossBackward>)\n",
      "139 out of:  511\n",
      "tensor(0.2149, grad_fn=<NllLossBackward>)\n",
      "140 out of:  511\n",
      "tensor(0.2294, grad_fn=<NllLossBackward>)\n",
      "141 out of:  511\n",
      "tensor(0.1771, grad_fn=<NllLossBackward>)\n",
      "142 out of:  511\n",
      "tensor(0.2256, grad_fn=<NllLossBackward>)\n",
      "143 out of:  511\n",
      "tensor(0.2800, grad_fn=<NllLossBackward>)\n",
      "144 out of:  511\n",
      "tensor(0.1737, grad_fn=<NllLossBackward>)\n",
      "145 out of:  511\n",
      "tensor(0.2679, grad_fn=<NllLossBackward>)\n",
      "146 out of:  511\n",
      "tensor(0.2284, grad_fn=<NllLossBackward>)\n",
      "147 out of:  511\n",
      "tensor(0.2955, grad_fn=<NllLossBackward>)\n",
      "148 out of:  511\n",
      "tensor(0.2779, grad_fn=<NllLossBackward>)\n",
      "149 out of:  511\n",
      "tensor(0.2840, grad_fn=<NllLossBackward>)\n",
      "150 out of:  511\n",
      "tensor(0.2271, grad_fn=<NllLossBackward>)\n",
      "151 out of:  511\n",
      "tensor(0.2098, grad_fn=<NllLossBackward>)\n",
      "152 out of:  511\n",
      "tensor(0.1923, grad_fn=<NllLossBackward>)\n",
      "153 out of:  511\n",
      "tensor(0.1841, grad_fn=<NllLossBackward>)\n",
      "154 out of:  511\n",
      "tensor(0.2212, grad_fn=<NllLossBackward>)\n",
      "155 out of:  511\n",
      "tensor(0.2196, grad_fn=<NllLossBackward>)\n",
      "156 out of:  511\n",
      "tensor(0.2081, grad_fn=<NllLossBackward>)\n",
      "157 out of:  511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3043, grad_fn=<NllLossBackward>)\n",
      "158 out of:  511\n",
      "tensor(0.2726, grad_fn=<NllLossBackward>)\n",
      "159 out of:  511\n",
      "tensor(0.2314, grad_fn=<NllLossBackward>)\n",
      "160 out of:  511\n",
      "tensor(0.2978, grad_fn=<NllLossBackward>)\n",
      "161 out of:  511\n",
      "tensor(0.2995, grad_fn=<NllLossBackward>)\n",
      "162 out of:  511\n",
      "tensor(0.2164, grad_fn=<NllLossBackward>)\n",
      "163 out of:  511\n",
      "tensor(0.1856, grad_fn=<NllLossBackward>)\n",
      "164 out of:  511\n",
      "tensor(0.2376, grad_fn=<NllLossBackward>)\n",
      "165 out of:  511\n",
      "tensor(0.2376, grad_fn=<NllLossBackward>)\n",
      "166 out of:  511\n",
      "tensor(0.2286, grad_fn=<NllLossBackward>)\n",
      "167 out of:  511\n",
      "tensor(0.2198, grad_fn=<NllLossBackward>)\n",
      "168 out of:  511\n",
      "tensor(0.2119, grad_fn=<NllLossBackward>)\n",
      "169 out of:  511\n",
      "tensor(0.1839, grad_fn=<NllLossBackward>)\n",
      "170 out of:  511\n",
      "tensor(0.2057, grad_fn=<NllLossBackward>)\n",
      "171 out of:  511\n",
      "tensor(0.1813, grad_fn=<NllLossBackward>)\n",
      "172 out of:  511\n",
      "tensor(0.1358, grad_fn=<NllLossBackward>)\n",
      "173 out of:  511\n",
      "tensor(0.1448, grad_fn=<NllLossBackward>)\n",
      "174 out of:  511\n",
      "tensor(0.2190, grad_fn=<NllLossBackward>)\n",
      "175 out of:  511\n",
      "tensor(0.2398, grad_fn=<NllLossBackward>)\n",
      "176 out of:  511\n",
      "tensor(0.2184, grad_fn=<NllLossBackward>)\n",
      "177 out of:  511\n",
      "tensor(0.2475, grad_fn=<NllLossBackward>)\n",
      "178 out of:  511\n",
      "tensor(0.1524, grad_fn=<NllLossBackward>)\n",
      "179 out of:  511\n",
      "tensor(0.1824, grad_fn=<NllLossBackward>)\n",
      "180 out of:  511\n",
      "tensor(0.3113, grad_fn=<NllLossBackward>)\n",
      "181 out of:  511\n",
      "tensor(0.1887, grad_fn=<NllLossBackward>)\n",
      "182 out of:  511\n",
      "tensor(0.2287, grad_fn=<NllLossBackward>)\n",
      "183 out of:  511\n",
      "tensor(0.2499, grad_fn=<NllLossBackward>)\n",
      "184 out of:  511\n",
      "tensor(0.2268, grad_fn=<NllLossBackward>)\n",
      "185 out of:  511\n",
      "tensor(0.2462, grad_fn=<NllLossBackward>)\n",
      "186 out of:  511\n",
      "tensor(0.1994, grad_fn=<NllLossBackward>)\n",
      "187 out of:  511\n",
      "tensor(0.2022, grad_fn=<NllLossBackward>)\n",
      "188 out of:  511\n",
      "tensor(0.2285, grad_fn=<NllLossBackward>)\n",
      "189 out of:  511\n",
      "tensor(0.1680, grad_fn=<NllLossBackward>)\n",
      "190 out of:  511\n",
      "tensor(0.1944, grad_fn=<NllLossBackward>)\n",
      "191 out of:  511\n",
      "tensor(0.2151, grad_fn=<NllLossBackward>)\n",
      "192 out of:  511\n",
      "tensor(0.2798, grad_fn=<NllLossBackward>)\n",
      "193 out of:  511\n",
      "tensor(0.1541, grad_fn=<NllLossBackward>)\n",
      "194 out of:  511\n",
      "tensor(0.2301, grad_fn=<NllLossBackward>)\n",
      "195 out of:  511\n",
      "tensor(0.2534, grad_fn=<NllLossBackward>)\n",
      "196 out of:  511\n",
      "tensor(0.2377, grad_fn=<NllLossBackward>)\n",
      "197 out of:  511\n",
      "tensor(0.2052, grad_fn=<NllLossBackward>)\n",
      "198 out of:  511\n",
      "tensor(0.2173, grad_fn=<NllLossBackward>)\n",
      "199 out of:  511\n",
      "tensor(0.2626, grad_fn=<NllLossBackward>)\n",
      "200 out of:  511\n",
      "tensor(0.2579, grad_fn=<NllLossBackward>)\n",
      "201 out of:  511\n",
      "tensor(0.2814, grad_fn=<NllLossBackward>)\n",
      "202 out of:  511\n",
      "tensor(0.2277, grad_fn=<NllLossBackward>)\n",
      "203 out of:  511\n",
      "tensor(0.2854, grad_fn=<NllLossBackward>)\n",
      "204 out of:  511\n",
      "tensor(0.1679, grad_fn=<NllLossBackward>)\n",
      "205 out of:  511\n",
      "tensor(0.2197, grad_fn=<NllLossBackward>)\n",
      "206 out of:  511\n",
      "tensor(0.2482, grad_fn=<NllLossBackward>)\n",
      "207 out of:  511\n",
      "tensor(0.2563, grad_fn=<NllLossBackward>)\n",
      "208 out of:  511\n",
      "tensor(0.2360, grad_fn=<NllLossBackward>)\n",
      "209 out of:  511\n",
      "tensor(0.1448, grad_fn=<NllLossBackward>)\n",
      "210 out of:  511\n",
      "tensor(0.2818, grad_fn=<NllLossBackward>)\n",
      "211 out of:  511\n",
      "tensor(0.2567, grad_fn=<NllLossBackward>)\n",
      "212 out of:  511\n",
      "tensor(0.1386, grad_fn=<NllLossBackward>)\n",
      "213 out of:  511\n",
      "tensor(0.2440, grad_fn=<NllLossBackward>)\n",
      "214 out of:  511\n",
      "tensor(0.2202, grad_fn=<NllLossBackward>)\n",
      "215 out of:  511\n",
      "tensor(0.2085, grad_fn=<NllLossBackward>)\n",
      "216 out of:  511\n",
      "tensor(0.2384, grad_fn=<NllLossBackward>)\n",
      "217 out of:  511\n",
      "tensor(0.2679, grad_fn=<NllLossBackward>)\n",
      "218 out of:  511\n",
      "tensor(0.1745, grad_fn=<NllLossBackward>)\n",
      "219 out of:  511\n",
      "tensor(0.2648, grad_fn=<NllLossBackward>)\n",
      "220 out of:  511\n",
      "tensor(0.2164, grad_fn=<NllLossBackward>)\n",
      "221 out of:  511\n",
      "tensor(0.1653, grad_fn=<NllLossBackward>)\n",
      "222 out of:  511\n",
      "tensor(0.2498, grad_fn=<NllLossBackward>)\n",
      "223 out of:  511\n",
      "tensor(0.1458, grad_fn=<NllLossBackward>)\n",
      "224 out of:  511\n",
      "tensor(0.3212, grad_fn=<NllLossBackward>)\n",
      "225 out of:  511\n",
      "tensor(0.2687, grad_fn=<NllLossBackward>)\n",
      "226 out of:  511\n",
      "tensor(0.1751, grad_fn=<NllLossBackward>)\n",
      "227 out of:  511\n",
      "tensor(0.1952, grad_fn=<NllLossBackward>)\n",
      "228 out of:  511\n",
      "tensor(0.2537, grad_fn=<NllLossBackward>)\n",
      "229 out of:  511\n",
      "tensor(0.1871, grad_fn=<NllLossBackward>)\n",
      "230 out of:  511\n",
      "tensor(0.2248, grad_fn=<NllLossBackward>)\n",
      "231 out of:  511\n",
      "tensor(0.1775, grad_fn=<NllLossBackward>)\n",
      "232 out of:  511\n",
      "tensor(0.2741, grad_fn=<NllLossBackward>)\n",
      "233 out of:  511\n",
      "tensor(0.2112, grad_fn=<NllLossBackward>)\n",
      "234 out of:  511\n",
      "tensor(0.2507, grad_fn=<NllLossBackward>)\n",
      "235 out of:  511\n",
      "tensor(0.2715, grad_fn=<NllLossBackward>)\n",
      "236 out of:  511\n",
      "tensor(0.2061, grad_fn=<NllLossBackward>)\n",
      "237 out of:  511\n",
      "tensor(0.1793, grad_fn=<NllLossBackward>)\n",
      "238 out of:  511\n",
      "tensor(0.2397, grad_fn=<NllLossBackward>)\n",
      "239 out of:  511\n",
      "tensor(0.2280, grad_fn=<NllLossBackward>)\n",
      "240 out of:  511\n",
      "tensor(0.1907, grad_fn=<NllLossBackward>)\n",
      "241 out of:  511\n",
      "tensor(0.2195, grad_fn=<NllLossBackward>)\n",
      "242 out of:  511\n",
      "tensor(0.1726, grad_fn=<NllLossBackward>)\n",
      "243 out of:  511\n",
      "tensor(0.1568, grad_fn=<NllLossBackward>)\n",
      "244 out of:  511\n",
      "tensor(0.2457, grad_fn=<NllLossBackward>)\n",
      "245 out of:  511\n",
      "tensor(0.2103, grad_fn=<NllLossBackward>)\n",
      "246 out of:  511\n",
      "tensor(0.2268, grad_fn=<NllLossBackward>)\n",
      "247 out of:  511\n",
      "tensor(0.2710, grad_fn=<NllLossBackward>)\n",
      "248 out of:  511\n",
      "tensor(0.2592, grad_fn=<NllLossBackward>)\n",
      "249 out of:  511\n",
      "tensor(0.1907, grad_fn=<NllLossBackward>)\n",
      "250 out of:  511\n",
      "tensor(0.2090, grad_fn=<NllLossBackward>)\n",
      "251 out of:  511\n",
      "tensor(0.2401, grad_fn=<NllLossBackward>)\n",
      "252 out of:  511\n",
      "tensor(0.1869, grad_fn=<NllLossBackward>)\n",
      "253 out of:  511\n",
      "tensor(0.1665, grad_fn=<NllLossBackward>)\n",
      "254 out of:  511\n",
      "tensor(0.2218, grad_fn=<NllLossBackward>)\n",
      "255 out of:  511\n",
      "tensor(0.2645, grad_fn=<NllLossBackward>)\n",
      "256 out of:  511\n",
      "tensor(0.1402, grad_fn=<NllLossBackward>)\n",
      "257 out of:  511\n",
      "tensor(0.2482, grad_fn=<NllLossBackward>)\n",
      "258 out of:  511\n",
      "tensor(0.2167, grad_fn=<NllLossBackward>)\n",
      "259 out of:  511\n",
      "tensor(0.2193, grad_fn=<NllLossBackward>)\n",
      "260 out of:  511\n",
      "tensor(0.1913, grad_fn=<NllLossBackward>)\n",
      "261 out of:  511\n",
      "tensor(0.2324, grad_fn=<NllLossBackward>)\n",
      "262 out of:  511\n",
      "tensor(0.2084, grad_fn=<NllLossBackward>)\n",
      "263 out of:  511\n",
      "tensor(0.2937, grad_fn=<NllLossBackward>)\n",
      "264 out of:  511\n",
      "tensor(0.1978, grad_fn=<NllLossBackward>)\n",
      "265 out of:  511\n",
      "tensor(0.2247, grad_fn=<NllLossBackward>)\n",
      "266 out of:  511\n",
      "tensor(0.2422, grad_fn=<NllLossBackward>)\n",
      "267 out of:  511\n",
      "tensor(0.2133, grad_fn=<NllLossBackward>)\n",
      "268 out of:  511\n",
      "tensor(0.2510, grad_fn=<NllLossBackward>)\n",
      "269 out of:  511\n",
      "tensor(0.2048, grad_fn=<NllLossBackward>)\n",
      "270 out of:  511\n",
      "tensor(0.2385, grad_fn=<NllLossBackward>)\n",
      "271 out of:  511\n",
      "tensor(0.2531, grad_fn=<NllLossBackward>)\n",
      "272 out of:  511\n",
      "tensor(0.1528, grad_fn=<NllLossBackward>)\n",
      "273 out of:  511\n",
      "tensor(0.2512, grad_fn=<NllLossBackward>)\n",
      "274 out of:  511\n",
      "tensor(0.2651, grad_fn=<NllLossBackward>)\n",
      "275 out of:  511\n",
      "tensor(0.2703, grad_fn=<NllLossBackward>)\n",
      "276 out of:  511\n",
      "tensor(0.2216, grad_fn=<NllLossBackward>)\n",
      "277 out of:  511\n",
      "tensor(0.2096, grad_fn=<NllLossBackward>)\n",
      "278 out of:  511\n",
      "tensor(0.2183, grad_fn=<NllLossBackward>)\n",
      "279 out of:  511\n",
      "tensor(0.2731, grad_fn=<NllLossBackward>)\n",
      "280 out of:  511\n",
      "tensor(0.2215, grad_fn=<NllLossBackward>)\n",
      "281 out of:  511\n",
      "tensor(0.2101, grad_fn=<NllLossBackward>)\n",
      "282 out of:  511\n",
      "tensor(0.2305, grad_fn=<NllLossBackward>)\n",
      "283 out of:  511\n",
      "tensor(0.2173, grad_fn=<NllLossBackward>)\n",
      "284 out of:  511\n",
      "tensor(0.1775, grad_fn=<NllLossBackward>)\n",
      "285 out of:  511\n",
      "tensor(0.2744, grad_fn=<NllLossBackward>)\n",
      "286 out of:  511\n",
      "tensor(0.2148, grad_fn=<NllLossBackward>)\n",
      "287 out of:  511\n",
      "tensor(0.2364, grad_fn=<NllLossBackward>)\n",
      "288 out of:  511\n",
      "tensor(0.2112, grad_fn=<NllLossBackward>)\n",
      "289 out of:  511\n",
      "tensor(0.2021, grad_fn=<NllLossBackward>)\n",
      "290 out of:  511\n",
      "tensor(0.2258, grad_fn=<NllLossBackward>)\n",
      "291 out of:  511\n",
      "tensor(0.1551, grad_fn=<NllLossBackward>)\n",
      "292 out of:  511\n",
      "tensor(0.1910, grad_fn=<NllLossBackward>)\n",
      "293 out of:  511\n",
      "tensor(0.1891, grad_fn=<NllLossBackward>)\n",
      "294 out of:  511\n",
      "tensor(0.2648, grad_fn=<NllLossBackward>)\n",
      "295 out of:  511\n",
      "tensor(0.2817, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 out of:  511\n",
      "tensor(0.1890, grad_fn=<NllLossBackward>)\n",
      "297 out of:  511\n",
      "tensor(0.1876, grad_fn=<NllLossBackward>)\n",
      "298 out of:  511\n",
      "tensor(0.1802, grad_fn=<NllLossBackward>)\n",
      "299 out of:  511\n",
      "tensor(0.2628, grad_fn=<NllLossBackward>)\n",
      "300 out of:  511\n",
      "tensor(0.2812, grad_fn=<NllLossBackward>)\n",
      "301 out of:  511\n",
      "tensor(0.1917, grad_fn=<NllLossBackward>)\n",
      "302 out of:  511\n",
      "tensor(0.2179, grad_fn=<NllLossBackward>)\n",
      "303 out of:  511\n",
      "tensor(0.2008, grad_fn=<NllLossBackward>)\n",
      "304 out of:  511\n",
      "tensor(0.1381, grad_fn=<NllLossBackward>)\n",
      "305 out of:  511\n",
      "tensor(0.2469, grad_fn=<NllLossBackward>)\n",
      "306 out of:  511\n",
      "tensor(0.2669, grad_fn=<NllLossBackward>)\n",
      "307 out of:  511\n",
      "tensor(0.2762, grad_fn=<NllLossBackward>)\n",
      "308 out of:  511\n",
      "tensor(0.2045, grad_fn=<NllLossBackward>)\n",
      "309 out of:  511\n",
      "tensor(0.2261, grad_fn=<NllLossBackward>)\n",
      "310 out of:  511\n",
      "tensor(0.2067, grad_fn=<NllLossBackward>)\n",
      "311 out of:  511\n",
      "tensor(0.1218, grad_fn=<NllLossBackward>)\n",
      "312 out of:  511\n",
      "tensor(0.2744, grad_fn=<NllLossBackward>)\n",
      "313 out of:  511\n",
      "tensor(0.1418, grad_fn=<NllLossBackward>)\n",
      "314 out of:  511\n",
      "tensor(0.1869, grad_fn=<NllLossBackward>)\n",
      "315 out of:  511\n",
      "tensor(0.1773, grad_fn=<NllLossBackward>)\n",
      "316 out of:  511\n",
      "tensor(0.2141, grad_fn=<NllLossBackward>)\n",
      "317 out of:  511\n",
      "tensor(0.2913, grad_fn=<NllLossBackward>)\n",
      "318 out of:  511\n",
      "tensor(0.2108, grad_fn=<NllLossBackward>)\n",
      "319 out of:  511\n",
      "tensor(0.2470, grad_fn=<NllLossBackward>)\n",
      "320 out of:  511\n",
      "tensor(0.1943, grad_fn=<NllLossBackward>)\n",
      "321 out of:  511\n",
      "tensor(0.2702, grad_fn=<NllLossBackward>)\n",
      "322 out of:  511\n",
      "tensor(0.2098, grad_fn=<NllLossBackward>)\n",
      "323 out of:  511\n",
      "tensor(0.2368, grad_fn=<NllLossBackward>)\n",
      "324 out of:  511\n",
      "tensor(0.1772, grad_fn=<NllLossBackward>)\n",
      "325 out of:  511\n",
      "tensor(0.2693, grad_fn=<NllLossBackward>)\n",
      "326 out of:  511\n",
      "tensor(0.2866, grad_fn=<NllLossBackward>)\n",
      "327 out of:  511\n",
      "tensor(0.2670, grad_fn=<NllLossBackward>)\n",
      "328 out of:  511\n",
      "tensor(0.2599, grad_fn=<NllLossBackward>)\n",
      "329 out of:  511\n",
      "tensor(0.2054, grad_fn=<NllLossBackward>)\n",
      "330 out of:  511\n",
      "tensor(0.2067, grad_fn=<NllLossBackward>)\n",
      "331 out of:  511\n",
      "tensor(0.2534, grad_fn=<NllLossBackward>)\n",
      "332 out of:  511\n",
      "tensor(0.1966, grad_fn=<NllLossBackward>)\n",
      "333 out of:  511\n",
      "tensor(0.2249, grad_fn=<NllLossBackward>)\n",
      "334 out of:  511\n",
      "tensor(0.2138, grad_fn=<NllLossBackward>)\n",
      "335 out of:  511\n",
      "tensor(0.2354, grad_fn=<NllLossBackward>)\n",
      "336 out of:  511\n",
      "tensor(0.2221, grad_fn=<NllLossBackward>)\n",
      "337 out of:  511\n",
      "tensor(0.1461, grad_fn=<NllLossBackward>)\n",
      "338 out of:  511\n",
      "tensor(0.2342, grad_fn=<NllLossBackward>)\n",
      "339 out of:  511\n",
      "tensor(0.2165, grad_fn=<NllLossBackward>)\n",
      "340 out of:  511\n",
      "tensor(0.2126, grad_fn=<NllLossBackward>)\n",
      "341 out of:  511\n",
      "tensor(0.1955, grad_fn=<NllLossBackward>)\n",
      "342 out of:  511\n",
      "tensor(0.1771, grad_fn=<NllLossBackward>)\n",
      "343 out of:  511\n",
      "tensor(0.2131, grad_fn=<NllLossBackward>)\n",
      "344 out of:  511\n",
      "tensor(0.1749, grad_fn=<NllLossBackward>)\n",
      "345 out of:  511\n",
      "tensor(0.2813, grad_fn=<NllLossBackward>)\n",
      "346 out of:  511\n",
      "tensor(0.2029, grad_fn=<NllLossBackward>)\n",
      "347 out of:  511\n",
      "tensor(0.2517, grad_fn=<NllLossBackward>)\n",
      "348 out of:  511\n",
      "tensor(0.2177, grad_fn=<NllLossBackward>)\n",
      "349 out of:  511\n",
      "tensor(0.2686, grad_fn=<NllLossBackward>)\n",
      "350 out of:  511\n",
      "tensor(0.2527, grad_fn=<NllLossBackward>)\n",
      "351 out of:  511\n",
      "tensor(0.2141, grad_fn=<NllLossBackward>)\n",
      "352 out of:  511\n",
      "tensor(0.2267, grad_fn=<NllLossBackward>)\n",
      "353 out of:  511\n",
      "tensor(0.1699, grad_fn=<NllLossBackward>)\n",
      "354 out of:  511\n",
      "tensor(0.1998, grad_fn=<NllLossBackward>)\n",
      "355 out of:  511\n",
      "tensor(0.1581, grad_fn=<NllLossBackward>)\n",
      "356 out of:  511\n",
      "tensor(0.2325, grad_fn=<NllLossBackward>)\n",
      "357 out of:  511\n",
      "tensor(0.1750, grad_fn=<NllLossBackward>)\n",
      "358 out of:  511\n",
      "tensor(0.2363, grad_fn=<NllLossBackward>)\n",
      "359 out of:  511\n",
      "tensor(0.2943, grad_fn=<NllLossBackward>)\n",
      "360 out of:  511\n",
      "tensor(0.1982, grad_fn=<NllLossBackward>)\n",
      "361 out of:  511\n",
      "tensor(0.1264, grad_fn=<NllLossBackward>)\n",
      "362 out of:  511\n",
      "tensor(0.2151, grad_fn=<NllLossBackward>)\n",
      "363 out of:  511\n",
      "tensor(0.2165, grad_fn=<NllLossBackward>)\n",
      "364 out of:  511\n",
      "tensor(0.2675, grad_fn=<NllLossBackward>)\n",
      "365 out of:  511\n",
      "tensor(0.2399, grad_fn=<NllLossBackward>)\n",
      "366 out of:  511\n",
      "tensor(0.1855, grad_fn=<NllLossBackward>)\n",
      "367 out of:  511\n",
      "tensor(0.1781, grad_fn=<NllLossBackward>)\n",
      "368 out of:  511\n",
      "tensor(0.2264, grad_fn=<NllLossBackward>)\n",
      "369 out of:  511\n",
      "tensor(0.2283, grad_fn=<NllLossBackward>)\n",
      "370 out of:  511\n",
      "tensor(0.2917, grad_fn=<NllLossBackward>)\n",
      "371 out of:  511\n",
      "tensor(0.2677, grad_fn=<NllLossBackward>)\n",
      "372 out of:  511\n",
      "tensor(0.1944, grad_fn=<NllLossBackward>)\n",
      "373 out of:  511\n",
      "tensor(0.2335, grad_fn=<NllLossBackward>)\n",
      "374 out of:  511\n",
      "tensor(0.2558, grad_fn=<NllLossBackward>)\n",
      "375 out of:  511\n",
      "tensor(0.2184, grad_fn=<NllLossBackward>)\n",
      "376 out of:  511\n",
      "tensor(0.2821, grad_fn=<NllLossBackward>)\n",
      "377 out of:  511\n",
      "tensor(0.2691, grad_fn=<NllLossBackward>)\n",
      "378 out of:  511\n",
      "tensor(0.2234, grad_fn=<NllLossBackward>)\n",
      "379 out of:  511\n",
      "tensor(0.1872, grad_fn=<NllLossBackward>)\n",
      "380 out of:  511\n",
      "tensor(0.2546, grad_fn=<NllLossBackward>)\n",
      "381 out of:  511\n",
      "tensor(0.2142, grad_fn=<NllLossBackward>)\n",
      "382 out of:  511\n",
      "tensor(0.2071, grad_fn=<NllLossBackward>)\n",
      "383 out of:  511\n",
      "tensor(0.1961, grad_fn=<NllLossBackward>)\n",
      "384 out of:  511\n",
      "tensor(0.2496, grad_fn=<NllLossBackward>)\n",
      "385 out of:  511\n",
      "tensor(0.2244, grad_fn=<NllLossBackward>)\n",
      "386 out of:  511\n",
      "tensor(0.2541, grad_fn=<NllLossBackward>)\n",
      "387 out of:  511\n",
      "tensor(0.1938, grad_fn=<NllLossBackward>)\n",
      "388 out of:  511\n",
      "tensor(0.2407, grad_fn=<NllLossBackward>)\n",
      "389 out of:  511\n",
      "tensor(0.1489, grad_fn=<NllLossBackward>)\n",
      "390 out of:  511\n",
      "tensor(0.2656, grad_fn=<NllLossBackward>)\n",
      "391 out of:  511\n",
      "tensor(0.2331, grad_fn=<NllLossBackward>)\n",
      "392 out of:  511\n",
      "tensor(0.2012, grad_fn=<NllLossBackward>)\n",
      "393 out of:  511\n",
      "tensor(0.2862, grad_fn=<NllLossBackward>)\n",
      "394 out of:  511\n",
      "tensor(0.1831, grad_fn=<NllLossBackward>)\n",
      "395 out of:  511\n",
      "tensor(0.2160, grad_fn=<NllLossBackward>)\n",
      "396 out of:  511\n",
      "tensor(0.2009, grad_fn=<NllLossBackward>)\n",
      "397 out of:  511\n",
      "tensor(0.2204, grad_fn=<NllLossBackward>)\n",
      "398 out of:  511\n",
      "tensor(0.2728, grad_fn=<NllLossBackward>)\n",
      "399 out of:  511\n",
      "tensor(0.2405, grad_fn=<NllLossBackward>)\n",
      "400 out of:  511\n",
      "tensor(0.2734, grad_fn=<NllLossBackward>)\n",
      "401 out of:  511\n",
      "tensor(0.1917, grad_fn=<NllLossBackward>)\n",
      "402 out of:  511\n",
      "tensor(0.2180, grad_fn=<NllLossBackward>)\n",
      "403 out of:  511\n",
      "tensor(0.1925, grad_fn=<NllLossBackward>)\n",
      "404 out of:  511\n",
      "tensor(0.2374, grad_fn=<NllLossBackward>)\n",
      "405 out of:  511\n",
      "tensor(0.2716, grad_fn=<NllLossBackward>)\n",
      "406 out of:  511\n",
      "tensor(0.1895, grad_fn=<NllLossBackward>)\n",
      "407 out of:  511\n",
      "tensor(0.2224, grad_fn=<NllLossBackward>)\n",
      "408 out of:  511\n",
      "tensor(0.2311, grad_fn=<NllLossBackward>)\n",
      "409 out of:  511\n",
      "tensor(0.1752, grad_fn=<NllLossBackward>)\n",
      "410 out of:  511\n",
      "tensor(0.1852, grad_fn=<NllLossBackward>)\n",
      "411 out of:  511\n",
      "tensor(0.2473, grad_fn=<NllLossBackward>)\n",
      "412 out of:  511\n",
      "tensor(0.2179, grad_fn=<NllLossBackward>)\n",
      "413 out of:  511\n",
      "tensor(0.2249, grad_fn=<NllLossBackward>)\n",
      "414 out of:  511\n",
      "tensor(0.1705, grad_fn=<NllLossBackward>)\n",
      "415 out of:  511\n",
      "tensor(0.2341, grad_fn=<NllLossBackward>)\n",
      "416 out of:  511\n",
      "tensor(0.2170, grad_fn=<NllLossBackward>)\n",
      "417 out of:  511\n",
      "tensor(0.3062, grad_fn=<NllLossBackward>)\n",
      "418 out of:  511\n",
      "tensor(0.2668, grad_fn=<NllLossBackward>)\n",
      "419 out of:  511\n",
      "tensor(0.2363, grad_fn=<NllLossBackward>)\n",
      "420 out of:  511\n",
      "tensor(0.2176, grad_fn=<NllLossBackward>)\n",
      "421 out of:  511\n",
      "tensor(0.2066, grad_fn=<NllLossBackward>)\n",
      "422 out of:  511\n",
      "tensor(0.1522, grad_fn=<NllLossBackward>)\n",
      "423 out of:  511\n",
      "tensor(0.1990, grad_fn=<NllLossBackward>)\n",
      "424 out of:  511\n",
      "tensor(0.2070, grad_fn=<NllLossBackward>)\n",
      "425 out of:  511\n",
      "tensor(0.2028, grad_fn=<NllLossBackward>)\n",
      "426 out of:  511\n",
      "tensor(0.1691, grad_fn=<NllLossBackward>)\n",
      "427 out of:  511\n",
      "tensor(0.1960, grad_fn=<NllLossBackward>)\n",
      "428 out of:  511\n",
      "tensor(0.1637, grad_fn=<NllLossBackward>)\n",
      "429 out of:  511\n",
      "tensor(0.1953, grad_fn=<NllLossBackward>)\n",
      "430 out of:  511\n",
      "tensor(0.2003, grad_fn=<NllLossBackward>)\n",
      "431 out of:  511\n",
      "tensor(0.2948, grad_fn=<NllLossBackward>)\n",
      "432 out of:  511\n",
      "tensor(0.1712, grad_fn=<NllLossBackward>)\n",
      "433 out of:  511\n",
      "tensor(0.1594, grad_fn=<NllLossBackward>)\n",
      "434 out of:  511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1592, grad_fn=<NllLossBackward>)\n",
      "435 out of:  511\n",
      "tensor(0.2400, grad_fn=<NllLossBackward>)\n",
      "436 out of:  511\n",
      "tensor(0.2492, grad_fn=<NllLossBackward>)\n",
      "437 out of:  511\n",
      "tensor(0.1939, grad_fn=<NllLossBackward>)\n",
      "438 out of:  511\n",
      "tensor(0.1970, grad_fn=<NllLossBackward>)\n",
      "439 out of:  511\n",
      "tensor(0.1447, grad_fn=<NllLossBackward>)\n",
      "440 out of:  511\n",
      "tensor(0.2820, grad_fn=<NllLossBackward>)\n",
      "441 out of:  511\n",
      "tensor(0.2559, grad_fn=<NllLossBackward>)\n",
      "442 out of:  511\n",
      "tensor(0.2698, grad_fn=<NllLossBackward>)\n",
      "443 out of:  511\n",
      "tensor(0.1422, grad_fn=<NllLossBackward>)\n",
      "444 out of:  511\n",
      "tensor(0.2314, grad_fn=<NllLossBackward>)\n",
      "445 out of:  511\n",
      "tensor(0.2012, grad_fn=<NllLossBackward>)\n",
      "446 out of:  511\n",
      "tensor(0.2550, grad_fn=<NllLossBackward>)\n",
      "447 out of:  511\n",
      "tensor(0.2062, grad_fn=<NllLossBackward>)\n",
      "448 out of:  511\n",
      "tensor(0.1758, grad_fn=<NllLossBackward>)\n",
      "449 out of:  511\n",
      "tensor(0.2009, grad_fn=<NllLossBackward>)\n",
      "450 out of:  511\n",
      "tensor(0.1929, grad_fn=<NllLossBackward>)\n",
      "451 out of:  511\n",
      "tensor(0.1893, grad_fn=<NllLossBackward>)\n",
      "452 out of:  511\n",
      "tensor(0.1964, grad_fn=<NllLossBackward>)\n",
      "453 out of:  511\n",
      "tensor(0.2265, grad_fn=<NllLossBackward>)\n",
      "454 out of:  511\n",
      "tensor(0.2585, grad_fn=<NllLossBackward>)\n",
      "455 out of:  511\n",
      "tensor(0.2213, grad_fn=<NllLossBackward>)\n",
      "456 out of:  511\n",
      "tensor(0.2185, grad_fn=<NllLossBackward>)\n",
      "457 out of:  511\n",
      "tensor(0.2528, grad_fn=<NllLossBackward>)\n",
      "458 out of:  511\n",
      "tensor(0.2683, grad_fn=<NllLossBackward>)\n",
      "459 out of:  511\n",
      "tensor(0.2905, grad_fn=<NllLossBackward>)\n",
      "460 out of:  511\n",
      "tensor(0.2135, grad_fn=<NllLossBackward>)\n",
      "461 out of:  511\n",
      "tensor(0.2257, grad_fn=<NllLossBackward>)\n",
      "462 out of:  511\n",
      "tensor(0.1869, grad_fn=<NllLossBackward>)\n",
      "463 out of:  511\n",
      "tensor(0.2640, grad_fn=<NllLossBackward>)\n",
      "464 out of:  511\n",
      "tensor(0.2093, grad_fn=<NllLossBackward>)\n",
      "465 out of:  511\n",
      "tensor(0.2408, grad_fn=<NllLossBackward>)\n",
      "466 out of:  511\n",
      "tensor(0.2285, grad_fn=<NllLossBackward>)\n",
      "467 out of:  511\n",
      "tensor(0.2113, grad_fn=<NllLossBackward>)\n",
      "468 out of:  511\n",
      "tensor(0.2098, grad_fn=<NllLossBackward>)\n",
      "469 out of:  511\n",
      "tensor(0.2792, grad_fn=<NllLossBackward>)\n",
      "470 out of:  511\n",
      "tensor(0.2219, grad_fn=<NllLossBackward>)\n",
      "471 out of:  511\n",
      "tensor(0.2717, grad_fn=<NllLossBackward>)\n",
      "472 out of:  511\n",
      "tensor(0.2275, grad_fn=<NllLossBackward>)\n",
      "473 out of:  511\n",
      "tensor(0.1646, grad_fn=<NllLossBackward>)\n",
      "474 out of:  511\n",
      "tensor(0.1940, grad_fn=<NllLossBackward>)\n",
      "475 out of:  511\n",
      "tensor(0.2537, grad_fn=<NllLossBackward>)\n",
      "476 out of:  511\n",
      "tensor(0.2295, grad_fn=<NllLossBackward>)\n",
      "477 out of:  511\n",
      "tensor(0.3056, grad_fn=<NllLossBackward>)\n",
      "478 out of:  511\n",
      "tensor(0.2130, grad_fn=<NllLossBackward>)\n",
      "479 out of:  511\n",
      "tensor(0.2202, grad_fn=<NllLossBackward>)\n",
      "480 out of:  511\n",
      "tensor(0.2206, grad_fn=<NllLossBackward>)\n",
      "481 out of:  511\n",
      "tensor(0.2491, grad_fn=<NllLossBackward>)\n",
      "482 out of:  511\n",
      "tensor(0.2084, grad_fn=<NllLossBackward>)\n",
      "483 out of:  511\n",
      "tensor(0.2609, grad_fn=<NllLossBackward>)\n",
      "484 out of:  511\n",
      "tensor(0.2708, grad_fn=<NllLossBackward>)\n",
      "485 out of:  511\n",
      "tensor(0.2073, grad_fn=<NllLossBackward>)\n",
      "486 out of:  511\n",
      "tensor(0.2303, grad_fn=<NllLossBackward>)\n",
      "487 out of:  511\n",
      "tensor(0.2032, grad_fn=<NllLossBackward>)\n",
      "488 out of:  511\n",
      "tensor(0.1976, grad_fn=<NllLossBackward>)\n",
      "489 out of:  511\n",
      "tensor(0.2827, grad_fn=<NllLossBackward>)\n",
      "490 out of:  511\n",
      "tensor(0.2880, grad_fn=<NllLossBackward>)\n",
      "491 out of:  511\n",
      "tensor(0.2424, grad_fn=<NllLossBackward>)\n",
      "492 out of:  511\n",
      "tensor(0.2009, grad_fn=<NllLossBackward>)\n",
      "493 out of:  511\n",
      "tensor(0.2561, grad_fn=<NllLossBackward>)\n",
      "494 out of:  511\n",
      "tensor(0.1949, grad_fn=<NllLossBackward>)\n",
      "495 out of:  511\n",
      "tensor(0.2871, grad_fn=<NllLossBackward>)\n",
      "496 out of:  511\n",
      "tensor(0.2251, grad_fn=<NllLossBackward>)\n",
      "497 out of:  511\n",
      "tensor(0.2384, grad_fn=<NllLossBackward>)\n",
      "498 out of:  511\n",
      "tensor(0.2371, grad_fn=<NllLossBackward>)\n",
      "499 out of:  511\n",
      "tensor(0.1755, grad_fn=<NllLossBackward>)\n",
      "500 out of:  511\n",
      "tensor(0.1895, grad_fn=<NllLossBackward>)\n",
      "501 out of:  511\n",
      "tensor(0.2605, grad_fn=<NllLossBackward>)\n",
      "502 out of:  511\n",
      "tensor(0.1799, grad_fn=<NllLossBackward>)\n",
      "503 out of:  511\n",
      "tensor(0.2243, grad_fn=<NllLossBackward>)\n",
      "504 out of:  511\n",
      "tensor(0.2127, grad_fn=<NllLossBackward>)\n",
      "505 out of:  511\n",
      "tensor(0.2025, grad_fn=<NllLossBackward>)\n",
      "506 out of:  511\n",
      "tensor(0.1671, grad_fn=<NllLossBackward>)\n",
      "507 out of:  511\n",
      "tensor(0.2605, grad_fn=<NllLossBackward>)\n",
      "508 out of:  511\n",
      "tensor(0.3141, grad_fn=<NllLossBackward>)\n",
      "509 out of:  511\n",
      "tensor(0.1785, grad_fn=<NllLossBackward>)\n",
      "510 out of:  511\n",
      "tensor(0.2374, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.1)\n",
    "\n",
    "inputs_check = next(iter(train_iter)).question_text\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = inputs_check\n",
    "    score = model(inputs)\n",
    "    print(score)\n",
    "    \n",
    "for epoch in range(1):\n",
    "    for batch_num in range(len(train_iter)):\n",
    "        model.zero_grad()\n",
    "        print(batch_num,\"out of: \",len(train_iter))\n",
    "        sentence_data = next(iter(train_iter))\n",
    "        sentence_in = sentence_data.question_text\n",
    "        targets = sentence_data.target\n",
    "        \n",
    "        target_scores = model(sentence_in)\n",
    "        \n",
    "        loss = loss_function(target_scores, targets)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = inputs_check\n",
    "    score = model(inputs)\n",
    "    print(score)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0486, -3.0477],\n",
      "        [-0.0643, -2.7756],\n",
      "        [-0.0509, -3.0025],\n",
      "        [-0.0917, -2.4350],\n",
      "        [-0.0505, -3.0100],\n",
      "        [-0.0553, -2.9225],\n",
      "        [-0.0592, -2.8556],\n",
      "        [-0.0450, -3.1234],\n",
      "        [-0.0662, -2.7486],\n",
      "        [-0.0491, -3.0388],\n",
      "        [-0.0738, -2.6431],\n",
      "        [-0.0730, -2.6534],\n",
      "        [-0.0652, -2.7622],\n",
      "        [-0.0556, -2.9177],\n",
      "        [-0.0448, -3.1275],\n",
      "        [-0.0469, -3.0836],\n",
      "        [-0.0517, -2.9875],\n",
      "        [-0.0522, -2.9787],\n",
      "        [-0.0509, -3.0042],\n",
      "        [-0.0437, -3.1512],\n",
      "        [-0.0584, -2.8697],\n",
      "        [-0.0621, -2.8094],\n",
      "        [-0.0460, -3.1028],\n",
      "        [-0.0485, -3.0512],\n",
      "        [-0.0606, -2.8339],\n",
      "        [-0.0868, -2.4873],\n",
      "        [-0.0442, -3.1400],\n",
      "        [-0.0589, -2.8615],\n",
      "        [-0.0723, -2.6631],\n",
      "        [-0.0550, -2.9279],\n",
      "        [-0.0577, -2.8808],\n",
      "        [-0.0473, -3.0745],\n",
      "        [-0.0747, -2.6318],\n",
      "        [-0.0467, -3.0869],\n",
      "        [-0.0580, -2.8754],\n",
      "        [-0.0471, -3.0791],\n",
      "        [-0.0482, -3.0563],\n",
      "        [-0.0966, -2.3855],\n",
      "        [-0.0567, -2.8987],\n",
      "        [-0.1137, -2.2307],\n",
      "        [-0.0985, -2.3670],\n",
      "        [-0.0570, -2.8939],\n",
      "        [-0.0489, -3.0427],\n",
      "        [-0.1054, -2.3018],\n",
      "        [-0.0567, -2.8982],\n",
      "        [-0.0407, -3.2210],\n",
      "        [-0.0495, -3.0312],\n",
      "        [-0.0512, -2.9978],\n",
      "        [-0.0490, -3.0413],\n",
      "        [-0.0466, -3.0898],\n",
      "        [-0.0606, -2.8342],\n",
      "        [-0.0569, -2.8944],\n",
      "        [-0.0605, -2.8351],\n",
      "        [-0.0430, -3.1686],\n",
      "        [-0.0469, -3.0821],\n",
      "        [-0.0402, -3.2339],\n",
      "        [-0.0653, -2.7612],\n",
      "        [-0.0531, -2.9615],\n",
      "        [-0.1307, -2.0994],\n",
      "        [-0.0461, -3.1004],\n",
      "        [-0.0652, -2.7625],\n",
      "        [-0.1129, -2.2372],\n",
      "        [-0.0471, -3.0793],\n",
      "        [-0.0445, -3.1351],\n",
      "        [-0.0692, -2.7050],\n",
      "        [-0.0647, -2.7703],\n",
      "        [-0.0638, -2.7845],\n",
      "        [-0.0450, -3.1244],\n",
      "        [-0.0399, -3.2417],\n",
      "        [-0.0544, -2.9392],\n",
      "        [-0.0714, -2.6744],\n",
      "        [-0.0726, -2.6590],\n",
      "        [-0.0809, -2.5549],\n",
      "        [-0.0705, -2.6872],\n",
      "        [-0.0502, -3.0165],\n",
      "        [-0.0568, -2.8972],\n",
      "        [-0.0484, -3.0530],\n",
      "        [-0.0419, -3.1925],\n",
      "        [-0.0687, -2.7123],\n",
      "        [-0.0495, -3.0314],\n",
      "        [-0.0590, -2.8598],\n",
      "        [-0.0594, -2.8522],\n",
      "        [-0.0601, -2.8409],\n",
      "        [-0.0581, -2.8742],\n",
      "        [-0.0698, -2.6974],\n",
      "        [-0.0647, -2.7699],\n",
      "        [-0.1040, -2.3150],\n",
      "        [-0.0582, -2.8735],\n",
      "        [-0.0603, -2.8384],\n",
      "        [-0.0566, -2.8999],\n",
      "        [-0.0384, -3.2796],\n",
      "        [-0.0711, -2.6792],\n",
      "        [-0.0699, -2.6953],\n",
      "        [-0.0481, -3.0580],\n",
      "        [-0.0678, -2.7243],\n",
      "        [-0.0536, -2.9528],\n",
      "        [-0.0542, -2.9418],\n",
      "        [-0.0463, -3.0953],\n",
      "        [-0.0837, -2.5221],\n",
      "        [-0.0446, -3.1325],\n",
      "        [-0.0833, -2.5266],\n",
      "        [-0.2406, -1.5424],\n",
      "        [-0.0542, -2.9417],\n",
      "        [-0.0637, -2.7847],\n",
      "        [-0.1361, -2.0613],\n",
      "        [-0.0759, -2.6163],\n",
      "        [-0.0563, -2.9056],\n",
      "        [-0.0510, -3.0020],\n",
      "        [-0.1449, -2.0033],\n",
      "        [-0.0471, -3.0796],\n",
      "        [-0.1124, -2.2414],\n",
      "        [-0.0629, -2.7982],\n",
      "        [-0.0449, -3.1268],\n",
      "        [-0.0729, -2.6543],\n",
      "        [-0.1317, -2.0925],\n",
      "        [-0.0655, -2.7589],\n",
      "        [-0.0610, -2.8266],\n",
      "        [-0.0445, -3.1346],\n",
      "        [-0.0544, -2.9385],\n",
      "        [-0.0656, -2.7561],\n",
      "        [-0.1244, -2.1460],\n",
      "        [-0.0684, -2.7169],\n",
      "        [-0.0491, -3.0374],\n",
      "        [-0.0590, -2.8593],\n",
      "        [-0.0506, -3.0086],\n",
      "        [-0.0436, -3.1554],\n",
      "        [-0.3234, -1.2863],\n",
      "        [-0.0812, -2.5516],\n",
      "        [-0.1391, -2.0412],\n",
      "        [-0.3799, -1.1518],\n",
      "        [-0.0585, -2.8672],\n",
      "        [-0.0565, -2.9009],\n",
      "        [-0.0961, -2.3898],\n",
      "        [-0.0676, -2.7280],\n",
      "        [-0.1172, -2.2015],\n",
      "        [-0.0443, -3.1385],\n",
      "        [-0.0448, -3.1289],\n",
      "        [-0.0450, -3.1244],\n",
      "        [-0.0442, -3.1418],\n",
      "        [-0.0504, -3.0125],\n",
      "        [-0.0392, -3.2592],\n",
      "        [-0.0825, -2.5359],\n",
      "        [-0.0795, -2.5719],\n",
      "        [-0.0413, -3.2078],\n",
      "        [-0.0564, -2.9040],\n",
      "        [-0.0733, -2.6499],\n",
      "        [-0.0473, -3.0741],\n",
      "        [-0.0466, -3.0892],\n",
      "        [-0.0536, -2.9537],\n",
      "        [-0.0517, -2.9872],\n",
      "        [-0.0789, -2.5789],\n",
      "        [-0.0469, -3.0833],\n",
      "        [-0.0588, -2.8623],\n",
      "        [-0.0727, -2.6570],\n",
      "        [-0.0501, -3.0189],\n",
      "        [-0.1054, -2.3024],\n",
      "        [-0.0541, -2.9440],\n",
      "        [-0.0582, -2.8729],\n",
      "        [-0.0821, -2.5403],\n",
      "        [-0.0618, -2.8144],\n",
      "        [-0.0462, -3.0983],\n",
      "        [-0.0651, -2.7637],\n",
      "        [-0.0959, -2.3921],\n",
      "        [-0.0554, -2.9215],\n",
      "        [-0.0741, -2.6392],\n",
      "        [-0.0506, -3.0093],\n",
      "        [-0.0526, -2.9716],\n",
      "        [-0.0502, -3.0171],\n",
      "        [-0.0621, -2.8093],\n",
      "        [-0.0756, -2.6196],\n",
      "        [-0.0961, -2.3897],\n",
      "        [-0.0497, -3.0263],\n",
      "        [-0.0522, -2.9792],\n",
      "        [-0.0450, -3.1227],\n",
      "        [-0.0801, -2.5648],\n",
      "        [-0.0580, -2.8769],\n",
      "        [-0.0796, -2.5706],\n",
      "        [-0.0617, -2.8158],\n",
      "        [-0.0504, -3.0122],\n",
      "        [-0.0621, -2.8096],\n",
      "        [-0.0607, -2.8328],\n",
      "        [-0.0538, -2.9502],\n",
      "        [-0.0450, -3.1239],\n",
      "        [-0.0459, -3.1045],\n",
      "        [-0.0744, -2.6347],\n",
      "        [-0.0505, -3.0102],\n",
      "        [-0.0640, -2.7805],\n",
      "        [-0.1171, -2.2024],\n",
      "        [-0.0642, -2.7772],\n",
      "        [-0.0488, -3.0453],\n",
      "        [-0.2353, -1.5622],\n",
      "        [-0.0647, -2.7697],\n",
      "        [-0.0665, -2.7441],\n",
      "        [-0.1147, -2.2225],\n",
      "        [-0.0844, -2.5145],\n",
      "        [-0.0614, -2.8202],\n",
      "        [-0.3487, -1.2229],\n",
      "        [-0.0439, -3.1485],\n",
      "        [-0.0500, -3.0204],\n",
      "        [-0.1336, -2.0792],\n",
      "        [-0.0901, -2.4518],\n",
      "        [-0.0504, -3.0133],\n",
      "        [-0.0422, -3.1861],\n",
      "        [-0.0794, -2.5723],\n",
      "        [-0.0752, -2.6250],\n",
      "        [-0.0567, -2.8973],\n",
      "        [-0.0695, -2.7009],\n",
      "        [-0.0975, -2.3767],\n",
      "        [-0.0573, -2.8879],\n",
      "        [-0.0523, -2.9770],\n",
      "        [-0.0464, -3.0943],\n",
      "        [-0.0490, -3.0396],\n",
      "        [-0.1746, -1.8315],\n",
      "        [-0.0555, -2.9196],\n",
      "        [-0.0594, -2.8531],\n",
      "        [-0.0738, -2.6427],\n",
      "        [-0.0617, -2.8165],\n",
      "        [-0.0425, -3.1784],\n",
      "        [-0.0500, -3.0215],\n",
      "        [-0.0505, -3.0102],\n",
      "        [-0.0490, -3.0409],\n",
      "        [-0.0658, -2.7533],\n",
      "        [-0.0658, -2.7543],\n",
      "        [-0.0509, -3.0032],\n",
      "        [-0.0445, -3.1354],\n",
      "        [-0.0553, -2.9230],\n",
      "        [-0.0662, -2.7486],\n",
      "        [-0.0583, -2.8717],\n",
      "        [-0.0599, -2.8448],\n",
      "        [-0.1036, -2.3182],\n",
      "        [-0.0507, -3.0062],\n",
      "        [-0.0571, -2.8919],\n",
      "        [-0.0830, -2.5305],\n",
      "        [-0.0498, -3.0248],\n",
      "        [-0.0641, -2.7798],\n",
      "        [-0.1045, -2.3104],\n",
      "        [-0.0558, -2.9136],\n",
      "        [-0.0528, -2.9666],\n",
      "        [-0.0511, -2.9994],\n",
      "        [-0.0757, -2.6182],\n",
      "        [-0.0453, -3.1159],\n",
      "        [-0.0524, -2.9755],\n",
      "        [-0.0952, -2.3994],\n",
      "        [-0.0511, -2.9986],\n",
      "        [-0.0628, -2.7990],\n",
      "        [-0.1732, -1.8387],\n",
      "        [-0.1181, -2.1948],\n",
      "        [-0.2558, -1.4885],\n",
      "        [-0.0629, -2.7977],\n",
      "        [-0.0674, -2.7312],\n",
      "        [-0.0578, -2.8802],\n",
      "        [-0.1601, -1.9107],\n",
      "        [-0.0569, -2.8945],\n",
      "        [-0.0454, -3.1155],\n",
      "        [-0.0474, -3.0738],\n",
      "        [-0.0451, -3.1221]])\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def train_model(model, train_data, epochs, show=False):\n",
    "    \n",
    "    #optimizer = torch.optim.Adam(model.parameters())\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "    #loss_function = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    print(\"The loss function being used is {}\".format(loss_function))\n",
    "    errors = []\n",
    "    eval_errors = []\n",
    "    f1_train = []\n",
    "    auc_train = []\n",
    "    f1_eval = []\n",
    "    auc_eval = []\n",
    "    \n",
    "    num_training_batches = len(train_data)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {}\".format(epoch + 1))\n",
    "        print(\"Training mode\")\n",
    "        model.train()\n",
    "        train_iter = iter(train_data)\n",
    "        model.zero_grad()\n",
    "        train_loss = 0\n",
    "        all_preds = []\n",
    "        all_actual = []\n",
    "        for batch_num in range(num_training_batches):\n",
    "            print(\"Batch {}\".format(batch_num + 1))\n",
    "            print(\"Out of {}\".format(num_training_batches))\n",
    "            batch = next(train_iter)\n",
    "            text, class_vector = batch.question_text.transpose(0,1),batch.target.unsqueeze(1)\n",
    "            #print(batch)\n",
    "            #print(text.shape)\n",
    "            #optimizer.zero_grad()\n",
    "            text_pred = model(text)\n",
    "            #print(text_pred)\n",
    "            print(class_vector.shape)\n",
    "            print(torch.max(class_vector,1)[0].shape)\n",
    "            loss = loss_function(text_pred,class_vector.reshape(64,1).squeeze(1)) #torch.max(class_vector,1)[0])\n",
    "            print(loss)\n",
    "            print(text_pred.squeeze(1))\n",
    "            print(torch.max(class_vector,1)[0])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "train_model(net, train_iter,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iters = iter(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wee = next(train_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wee.question_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
