{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastai.vision import *\n",
    "from fastai import * \n",
    "from fastai.text import *\n",
    "from pathlib import Path \n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"my bar!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "from torchvision import transforms, utils\n",
    "from sklearn.metrics import confusion_matrix, f1_score, auc, roc_curve, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = True\n",
    "clean = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('../input/embeddings'),\n",
       " WindowsPath('../input/glove.840B.300d'),\n",
       " WindowsPath('../input/sample_submission.csv'),\n",
       " WindowsPath('../input/test.csv'),\n",
       " WindowsPath('../input/test_Clean.csv'),\n",
       " WindowsPath('../input/tmp_lm'),\n",
       " WindowsPath('../input/train.csv'),\n",
       " WindowsPath('../input/train_Clean.csv')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if lm: path = Path('../input'); \n",
    "list(path.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_df = pd.read_csv(path/'train.csv')\n",
    "test_df = pd.read_csv(path/'test.csv')\n",
    "allText_df = train_df.copy().append(test_df.copy(), sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clean:\n",
    "    train_df = pd.read_csv(path/'train.csv')\n",
    "    test_df = pd.read_csv(path/'test.csv')\n",
    "    \n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "    #allText_df[\"question_text\"] = allText_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "\n",
    "    train_df['question_text'] = train_df['question_text'].apply(lambda x: clean_text(x))\n",
    "    test_df['question_text'] = test_df['question_text'].apply(lambda x: clean_text(x))\n",
    "    #allText_df[\"question_text\"] = allText_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "\n",
    "    train_df['question_text'] = train_df['question_text'].apply(lambda x: clean_numbers(x))\n",
    "    test_df['question_text'] = test_df['question_text'].apply(lambda x:clean_numbers(x))\n",
    "    #allText_df[\"question_text\"] = allText_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "\n",
    "    train_df['question_text'] = train_df['question_text'].apply(lambda x: replace_typical_misspell(x))\n",
    "    test_df['question_text'] = test_df['question_text'].apply(lambda x: replace_typical_misspell(x))\n",
    "    #allText_df[\"question_text\"] = allText_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "\n",
    "    train_x = train_df['question_text'].fillna('_##_').values\n",
    "    test_x = test_df['question_text'].fillna('_##_').values\n",
    "    \n",
    "    train_Full = pd.read_csv(path/'train.csv')\n",
    "    test_Full = pd.read_csv(path/'test.csv')\n",
    "    \n",
    "    train_Full['question_text'] = train_x\n",
    "    test_Full['question_text'] = test_x\n",
    "    \n",
    "    train_Full.to_csv('train_Clean.csv',index=False)\n",
    "    test_Full.to_csv('test_Clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import zipfile\n",
    "zf = zipfile.ZipFile(path/'test.csv.zip')\n",
    "df_test = pd.read_csv(zf.open('test.csv'))\n",
    "zf = zipfile.ZipFile(path/'train.csv.zip')\n",
    "df_train = pd.read_csv(zf.open('train.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_train.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences():\n",
    "    tokenizer = lambda text: text.split() # the function above is the function we will be using to tokenize the text\n",
    "    TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer, lower=True)\n",
    "    LABEL = torchtext.data.Field(sequential=False, use_vocab=False) # sequential and use_vocab=False since no text (binary)\n",
    "    train_datafields = [(\"qid\", None), (\"question_text\", TEXT), (\"target\", LABEL)]\n",
    "    train = torchtext.data.TabularDataset( # If we had a validation set as well, we would add an additional .splits(...)\n",
    "                        # path=\"data/train_cleaned_v2.csv\", # the root directory where the data lies\n",
    "                        path = path/'train_Clean.csv',\n",
    "                        format='csv',\n",
    "                        # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "                        skip_header=True, \n",
    "                        fields=train_datafields)\n",
    "    test_datafields = [(\"qid\", None),\n",
    "                     (\"question_text\", TEXT)] \n",
    "    test = torchtext.data.TabularDataset( \n",
    "                path=path/'test_Clean.csv',\n",
    "                format=\"csv\",\n",
    "                skip_header=True,\n",
    "                fields=test_datafields)\n",
    "    return TEXT, LABEL, train, test\n",
    "# Chat Conversation End\n",
    "# Type a message...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT,LABEL,train,test = prepare_sequences()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LABEL.batch_first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, test, vectors = \"glove.6B.50d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_storage = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,vali = train.split(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "train_iter = BucketIterator(\n",
    "    train, # we pass in the datasets we want the iterator to draw data from\n",
    "    batch_size= 128, \n",
    "    sort_key= lambda x: len(x.question_text), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "    sort_within_batch=False, # sorting would add bias\n",
    "    repeat=False)\n",
    "\n",
    "vali_iter = BucketIterator(\n",
    "    vali, # we pass in the datasets we want the iterator to draw data from\n",
    "    batch_size= 128, \n",
    "    sort_key= lambda x: len(x.question_text), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "    sort_within_batch=False, # sorting would add bias\n",
    "    repeat=False)\n",
    "\n",
    "test_iter = Iterator(\n",
    "    test,\n",
    "    batch_size = 128,\n",
    "    train=False,\n",
    "    sort = False,\n",
    "    sort_within_batch = False,\n",
    "    repeat = False)\n",
    "\n",
    "ngpu = 0\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iter(test_iter).__next__().question_text\n",
    "\n",
    "#len(train_iter)\n",
    "#label_Size = next(iter(train_iter)).target\n",
    "#len(label_Size[label_Size==1])\n",
    "#train_iter.batches\n",
    "#train_iter = iter(train_iter)\n",
    "#train_iter.__next__().question_text\n",
    "#train_iter.__next__().target\n",
    "#iter(train_iter).__next__().question_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#smaller,_ = train.split(0.1)\n",
    "TEXT.vocab.itos[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_iter.dataset.examples.__getitem__(1).__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#smaller.examples[0].question_text\n",
    "test.examples.__getitem__(2).__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(train)\n",
    "#len(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.examples[0].question_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([214945, 50])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT.vocab.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 50\n",
    "hidden_size = 300\n",
    "ver1 = False\n",
    "sigBCE = True\n",
    "bidir = True\n",
    "layer_LSTM = 2\n",
    "layer_Lin = 3\n",
    "dropout_LSTM = 0.1\n",
    "dropout_Lin = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        \n",
    "        hidden_size = 64\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab),embed_size)\n",
    "        self.embedding.weight = nn.Parameter(TEXT.vocab.vectors)\n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size*4,2)\n",
    "        self.relu = nn.ReLU()\n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(16,1)\n",
    "        \n",
    "    def forward(self,train):\n",
    "        #print(train.shape)\n",
    "        h_embedding = self.embedding(train)\n",
    "        #print(h_embedding.shape)\n",
    "        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding,0)))\n",
    "        #print(h_embedding.shape)\n",
    "        \n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        #print(h_lstm.shape)\n",
    "        h_gru, _ = self.gru(h_lstm)\n",
    "        #print(h_gru.shape)\n",
    "        \n",
    "        avg_pool = torch.mean(h_gru,1)\n",
    "        #print(avg_pool.shape)\n",
    "        #avg_pool = avg_pool.view(-1,64)\n",
    "        max_pool, _ = torch.max(h_gru,1)\n",
    "        #print(max_pool.shape)\n",
    "        \n",
    "        \n",
    "        conc = torch.cat((avg_pool, max_pool),1)\n",
    "        print(conc.shape)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        #print(conc.shape)\n",
    "        #conc = self.dropout(conc)\n",
    "        print(conc.shape)\n",
    "        #out = nn.Sigmoid()(conc)\n",
    "        \n",
    "        out = nn.LogSoftmax(dim=1)(conc)\n",
    "        \n",
    "        #out = self.out(conc)\n",
    "        #print(out[:,-1].shape)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "batch = next(iter(train_iter)).question_text\n",
    "net = NeuralNet()\n",
    "#net(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch = next(iter(train_iter)).question_text\n",
    "#batch[30]\n",
    "#layer_Lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1])\n",
      "tensor([[0.4897],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4897],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4896],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4894],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4897],\n",
      "        [0.4897],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4897],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4894],\n",
      "        [0.4895],\n",
      "        [0.4898],\n",
      "        [0.4895],\n",
      "        [0.4897],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4897],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4894],\n",
      "        [0.4894],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4894],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4898],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4897],\n",
      "        [0.4894],\n",
      "        [0.4895],\n",
      "        [0.4897],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4894],\n",
      "        [0.4897],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4894],\n",
      "        [0.4898],\n",
      "        [0.4896],\n",
      "        [0.4897],\n",
      "        [0.4896],\n",
      "        [0.4894],\n",
      "        [0.4894],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4900],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4898],\n",
      "        [0.4898],\n",
      "        [0.4894],\n",
      "        [0.4896],\n",
      "        [0.4897],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4899],\n",
      "        [0.4895],\n",
      "        [0.4898],\n",
      "        [0.4895],\n",
      "        [0.4897],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4898],\n",
      "        [0.4894],\n",
      "        [0.4896]])\n"
     ]
    }
   ],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "                      \n",
    "        self.hidden_dim = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab),embed_size)\n",
    "        self.embedding.weight = nn.Parameter(TEXT.vocab.vectors)\n",
    "        \n",
    "        \n",
    "        #self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        \n",
    "        self.lstm_Layer = nn.LSTM(embed_size, hidden_size,num_layers = layer_LSTM, dropout = dropout_LSTM, bidirectional = bidir)\n",
    "        \n",
    "        self.lin_Layer = []\n",
    "        \n",
    "        self.lin_Layer_Dropout = dropout_Lin\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        if bidir:\n",
    "            scale = 2\n",
    "        else:\n",
    "            scale = 1\n",
    "        \n",
    "        for _ in range(layer_Lin - 1):\n",
    "            self.lin_Layer.append(nn.Linear(hidden_size*scale,hidden_size*scale))\n",
    "            self.lin_Layer = nn.ModuleList(self.lin_Layer)\n",
    "        if sigBCE:\n",
    "            self.hidden2tag = nn.Linear(hidden_size*scale, 1)\n",
    "        else:\n",
    "            self.hidden2tag = nn.Linear(hidden_size*scale, 2)\n",
    "    \n",
    "    def forward(self, train):\n",
    "        #print(train.shape)\n",
    "        embeds = self.embedding(train)\n",
    "        #print(embeds.shape)\n",
    "        #print(embeds.view(len(train),64,-1).shape)\n",
    "        #print(embeds.view(len(train),train.shape[1],-1).shape)\n",
    "        lstm_out, _ = self.lstm_Layer(embeds)#embeds.view(len(train),train.shape[1],-1))\n",
    "        #print(lstm_out.shape)\n",
    "        \n",
    "        if ver1:\n",
    "            #print(lstm_out.shape)\n",
    "            feature = lstm_out[-1,:,:] # -1 grabs last layer in list\n",
    "            #print(feature.shape)\n",
    "        \n",
    "            for layer in self.lin_Layer:\n",
    "                feature = layer(feature)\n",
    "                feature = F.relu(feature)\n",
    "                predict = self.hidden2tag(feature)\n",
    "        \n",
    "            feature = F.relu(feature)\n",
    "            predict = self.hidden2tag(feature)\n",
    "            if sigBCE:\n",
    "                return nn.Sigmoid()(predict)\n",
    "            else:\n",
    "                return nn.LogSoftmax(dim=1)(predict)\n",
    "        else:\n",
    "           \n",
    "            feature = torch.mean(lstm_out,0) #avg_pool\n",
    "            #print(avg_pool.shape)\n",
    "            for layer in self.lin_Layer:\n",
    "                feature = layer(feature)\n",
    "                feature = F.relu(feature)\n",
    "                \n",
    "            target_space = self.hidden2tag(feature)#.view(len(train),-1))\n",
    "            #target_space = self.relu(target_space)\n",
    "            \n",
    "            #print(target_space.shape)\n",
    "            if sigBCE:\n",
    "                return nn.Sigmoid()(target_space)\n",
    "            else:\n",
    "                return F.log_softmax(target_space, dim=1)\n",
    "            #print(target_score.shape)\n",
    "\n",
    "inputs_check = next(iter(train_iter)).question_text\n",
    "\n",
    "#print(inputs_check.shape)\n",
    "\n",
    "model = LSTMTagger()\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = inputs_check\n",
    "    #print(len(inputs))\n",
    "    score = model(inputs)\n",
    "    print(score.shape)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    if  isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger()\n",
    "model.apply(weight_init)\n",
    "if sigBCE:\n",
    "    loss_function = nn.BCELoss(reduction='none')\n",
    "else:\n",
    "    loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  of:  8164  Loss:  0.688176691532135  F1 Val:  0.13138686131386862  Val Acc:  7.03125  High:  0.50770664  Low:  0.5042168\n",
      "2  of:  8164  Loss:  0.6758392453193665  F1 Val:  0.1037037037037037  Val Acc:  5.46875  High:  0.5402855  Low:  0.5379057\n",
      "3  of:  8164  Loss:  0.702202558517456  F1 Val:  0.06060606060606061  Val Acc:  3.125  High:  0.58444995  Low:  0.576483\n",
      "4  of:  8164  Loss:  0.7081770896911621  F1 Val:  0.1037037037037037  Val Acc:  5.46875  High:  0.6109275  Low:  0.59842134\n",
      "5  of:  8164  Loss:  0.7098520994186401  F1 Val:  0.11764705882352941  Val Acc:  6.25  High:  0.62048143  Low:  0.60295963\n",
      "6  of:  8164  Loss:  0.7029476761817932  F1 Val:  0.1037037037037037  Val Acc:  5.46875  High:  0.6173885  Low:  0.5930264\n",
      "7  of:  8164  Loss:  0.7635444402694702  F1 Val:  0.08955223880597014  Val Acc:  4.6875  High:  0.59926856  Low:  0.5718313\n",
      "8  of:  8164  Loss:  0.6823981404304504  F1 Val:  0.0  Val Acc:  92.96875  High:  0.5443564  Low:  0.5188275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xw5735\\PWinFolder\\WPy-3661\\python-3.6.6.amd64\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9  of:  8164  Loss:  0.6906312108039856  F1 Val:  0.0  Val Acc:  90.625  High:  0.5116931  Low:  0.48676753\n",
      "10  of:  8164  Loss:  0.6824336647987366  F1 Val:  0.0  Val Acc:  92.96875  High:  0.48861262  Low:  0.46397918\n",
      "11  of:  8164  Loss:  0.7017262578010559  F1 Val:  0.0  Val Acc:  93.75  High:  0.466568  Low:  0.44214916\n",
      "12  of:  8164  Loss:  0.6652824878692627  F1 Val:  0.0  Val Acc:  93.75  High:  0.45100683  Low:  0.43070987\n",
      "13  of:  8164  Loss:  0.7276193499565125  F1 Val:  0.0  Val Acc:  96.09375  High:  0.4346541  Low:  0.41468245\n",
      "14  of:  8164  Loss:  0.637390673160553  F1 Val:  0.0  Val Acc:  92.1875  High:  0.44155023  Low:  0.41554093\n",
      "15  of:  8164  Loss:  0.6029891967773438  F1 Val:  0.0  Val Acc:  90.625  High:  0.42940304  Low:  0.40427542\n",
      "16  of:  8164  Loss:  0.8174329996109009  F1 Val:  0.0  Val Acc:  90.625  High:  0.41362154  Low:  0.37470463\n",
      "17  of:  8164  Loss:  0.7081589102745056  F1 Val:  0.0  Val Acc:  94.53125  High:  0.4351874  Low:  0.38467008\n",
      "18  of:  8164  Loss:  0.7405253052711487  F1 Val:  0.3478260869565218  Val Acc:  88.28125  High:  0.46785638  Low:  0.40869227\n",
      "19  of:  8164  Loss:  0.701583206653595  F1 Val:  0.1037037037037037  Val Acc:  5.46875  High:  0.51839983  Low:  0.45859194\n",
      "20  of:  8164  Loss:  0.6885482668876648  F1 Val:  0.13138686131386862  Val Acc:  7.03125  High:  0.58000255  Low:  0.5210449\n",
      "21  of:  8164  Loss:  0.6953784227371216  F1 Val:  0.11764705882352941  Val Acc:  6.25  High:  0.6352721  Low:  0.5647374\n",
      "22  of:  8164  Loss:  0.6923102736473083  F1 Val:  0.15827338129496402  Val Acc:  8.59375  High:  0.6777178  Low:  0.59204936\n",
      "23  of:  8164  Loss:  0.7367850542068481  F1 Val:  0.17142857142857143  Val Acc:  9.375  High:  0.6773923  Low:  0.6057151\n",
      "24  of:  8164  Loss:  0.6156191825866699  F1 Val:  0.15827338129496402  Val Acc:  8.59375  High:  0.6795199  Low:  0.5936526\n",
      "25  of:  8164  Loss:  0.6521267890930176  F1 Val:  0.1037037037037037  Val Acc:  5.46875  High:  0.6885462  Low:  0.59013164\n",
      "26  of:  8164  Loss:  0.7376619577407837  F1 Val:  0.15827338129496402  Val Acc:  8.59375  High:  0.6909064  Low:  0.59000987\n",
      "27  of:  8164  Loss:  0.6978466510772705  F1 Val:  0.17142857142857143  Val Acc:  9.375  High:  0.68224037  Low:  0.5617698\n",
      "28  of:  8164  Loss:  0.6460549235343933  F1 Val:  0.11764705882352941  Val Acc:  6.25  High:  0.65573144  Low:  0.5299917\n",
      "29  of:  8164  Loss:  0.6867730021476746  F1 Val:  0.09523809523809523  Val Acc:  10.9375  High:  0.63931316  Low:  0.5163225\n",
      "30  of:  8164  Loss:  0.6775913834571838  F1 Val:  0.1702127659574468  Val Acc:  69.53125  High:  0.62112135  Low:  0.50000155\n",
      "31  of:  8164  Loss:  0.6691449880599976  F1 Val:  0.06060606060606061  Val Acc:  75.78125  High:  0.6204126  Low:  0.47796965\n",
      "32  of:  8164  Loss:  0.6489081978797913  F1 Val:  0.3137254901960785  Val Acc:  72.65625  High:  0.6354657  Low:  0.46037328\n",
      "33  of:  8164  Loss:  0.6987074017524719  F1 Val:  0.27027027027027023  Val Acc:  78.90625  High:  0.6508877  Low:  0.4496378\n",
      "34  of:  8164  Loss:  0.666895866394043  F1 Val:  0.3225806451612903  Val Acc:  83.59375  High:  0.7001319  Low:  0.45026606\n",
      "35  of:  8164  Loss:  0.6808347702026367  F1 Val:  0.3043478260869565  Val Acc:  75.0  High:  0.6682597  Low:  0.42267808\n",
      "36  of:  8164  Loss:  0.6878936886787415  F1 Val:  0.19354838709677416  Val Acc:  80.46875  High:  0.71934813  Low:  0.41259122\n",
      "37  of:  8164  Loss:  0.6509764790534973  F1 Val:  0.0625  Val Acc:  76.5625  High:  0.72976756  Low:  0.40902105\n",
      "38  of:  8164  Loss:  0.6771788001060486  F1 Val:  0.1904761904761905  Val Acc:  60.15625  High:  0.79294765  Low:  0.40632543\n",
      "39  of:  8164  Loss:  0.6585878133773804  F1 Val:  0.15873015873015872  Val Acc:  17.1875  High:  0.84404933  Low:  0.4223952\n",
      "40  of:  8164  Loss:  0.6454756259918213  F1 Val:  0.12030075187969924  Val Acc:  8.59375  High:  0.89559436  Low:  0.4490257\n",
      "41  of:  8164  Loss:  0.7732606530189514  F1 Val:  0.2068965517241379  Val Acc:  64.0625  High:  0.9413086  Low:  0.48880577\n",
      "42  of:  8164  Loss:  0.6686404943466187  F1 Val:  0.2173913043478261  Val Acc:  71.875  High:  0.79421794  Low:  0.41200307\n",
      "43  of:  8164  Loss:  0.6380164623260498  F1 Val:  0.12903225806451615  Val Acc:  78.90625  High:  0.8646725  Low:  0.3926503\n",
      "44  of:  8164  Loss:  0.6064992547035217  F1 Val:  0.21428571428571427  Val Acc:  82.8125  High:  0.84087425  Low:  0.3754474\n",
      "45  of:  8164  Loss:  0.5964235067367554  F1 Val:  0.11111111111111112  Val Acc:  87.5  High:  0.76651514  Low:  0.3566945\n",
      "46  of:  8164  Loss:  0.747511625289917  F1 Val:  0.4615384615384615  Val Acc:  94.53125  High:  0.6933093  Low:  0.32161412\n",
      "47  of:  8164  Loss:  0.812232494354248  F1 Val:  0.125  Val Acc:  89.0625  High:  0.6725609  Low:  0.30791086\n",
      "48  of:  8164  Loss:  0.74212646484375  F1 Val:  0.14285714285714288  Val Acc:  90.625  High:  0.6781899  Low:  0.31541398\n",
      "49  of:  8164  Loss:  0.6458480358123779  F1 Val:  0.16666666666666666  Val Acc:  92.1875  High:  0.6904317  Low:  0.3321792\n",
      "50  of:  8164  Loss:  0.6170843243598938  F1 Val:  0.23076923076923073  Val Acc:  84.375  High:  0.7043703  Low:  0.3483141\n",
      "51  of:  8164  Loss:  0.6187364459037781  F1 Val:  0.125  Val Acc:  78.125  High:  0.7469713  Low:  0.37100375\n",
      "52  of:  8164  Loss:  0.5848244428634644  F1 Val:  0.21428571428571427  Val Acc:  82.8125  High:  0.8914539  Low:  0.39710605\n",
      "53  of:  8164  Loss:  0.7606372833251953  F1 Val:  0.15873015873015872  Val Acc:  58.59375  High:  0.96509016  Low:  0.41534087\n",
      "54  of:  8164  Loss:  0.6532875895500183  F1 Val:  0.14925373134328357  Val Acc:  55.46875  High:  0.9801145  Low:  0.43382686\n",
      "55  of:  8164  Loss:  0.522120475769043  F1 Val:  0.1111111111111111  Val Acc:  62.5  High:  0.9857611  Low:  0.4516697\n",
      "56  of:  8164  Loss:  0.6989912986755371  F1 Val:  0.07017543859649122  Val Acc:  58.59375  High:  0.9863468  Low:  0.45750448\n",
      "57  of:  8164  Loss:  0.6528054475784302  F1 Val:  0.14285714285714285  Val Acc:  43.75  High:  0.9917748  Low:  0.4585299\n",
      "58  of:  8164  Loss:  0.6708732843399048  F1 Val:  0.1621621621621622  Val Acc:  51.5625  High:  0.9818749  Low:  0.45715606\n",
      "59  of:  8164  Loss:  0.6721411943435669  F1 Val:  0.2857142857142857  Val Acc:  60.9375  High:  0.9315015  Low:  0.46162683\n",
      "60  of:  8164  Loss:  0.6482059359550476  F1 Val:  0.36363636363636365  Val Acc:  83.59375  High:  0.96984226  Low:  0.45559835\n",
      "61  of:  8164  Loss:  0.6792210936546326  F1 Val:  0.0  Val Acc:  83.59375  High:  0.9474466  Low:  0.45218116\n",
      "62  of:  8164  Loss:  0.6441255807876587  F1 Val:  0.3684210526315789  Val Acc:  81.25  High:  0.9018893  Low:  0.44201276\n",
      "63  of:  8164  Loss:  0.6412597298622131  F1 Val:  0.25  Val Acc:  90.625  High:  0.78683746  Low:  0.43069646\n",
      "64  of:  8164  Loss:  0.7184280157089233  F1 Val:  0.12500000000000003  Val Acc:  89.0625  High:  0.7426173  Low:  0.42610663\n",
      "65  of:  8164  Loss:  0.6933350563049316  F1 Val:  0.27586206896551724  Val Acc:  83.59375  High:  0.67717224  Low:  0.42737904\n",
      "66  of:  8164  Loss:  0.725587010383606  F1 Val:  0.1142857142857143  Val Acc:  75.78125  High:  0.5335312  Low:  0.4382771\n",
      "67  of:  8164  Loss:  0.6916622519493103  F1 Val:  0.23076923076923075  Val Acc:  84.375  High:  0.750402  Low:  0.46089086\n",
      "68  of:  8164  Loss:  0.6938027143478394  F1 Val:  0.3157894736842105  Val Acc:  89.84375  High:  0.72100735  Low:  0.4644537\n",
      "69  of:  8164  Loss:  0.6801408529281616  F1 Val:  0.15384615384615383  Val Acc:  74.21875  High:  0.71323407  Low:  0.47176525\n",
      "70  of:  8164  Loss:  0.670924961566925  F1 Val:  0.07518796992481203  Val Acc:  3.90625  High:  0.71619457  Low:  0.4842304\n",
      "71  of:  8164  Loss:  0.6494960188865662  F1 Val:  0.1090909090909091  Val Acc:  23.4375  High:  0.726517  Low:  0.5007427\n",
      "72  of:  8164  Loss:  0.6326581835746765  F1 Val:  0.14545454545454545  Val Acc:  63.28125  High:  0.7695403  Low:  0.499287\n"
     ]
    }
   ],
   "source": [
    "validate = True\n",
    "\n",
    "checking = next(iter(train_iter))\n",
    "inputs_check = checking.question_text\n",
    "target_check = checking.target\n",
    "\n",
    "\n",
    "f1_train = []\n",
    "auc_train = []\n",
    "train_accu = []\n",
    "\n",
    "avg_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = inputs_check\n",
    "    score1 = model(inputs)\n",
    "    #print(score)\n",
    "    \n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    training_set = iter(train_iter)\n",
    "    vali_set = iter(vali_iter)\n",
    "    model.zero_grad()\n",
    "    all_preds = []\n",
    "    all_actual = []\n",
    "    for batch_num in range(len(train_iter)):\n",
    "        model.train()\n",
    "        sentence_data = next(training_set)\n",
    "        vali_data = next(vali_set)\n",
    "        sentence_in = sentence_data.question_text\n",
    "        vali_in = vali_data.question_text\n",
    "        \n",
    "        \n",
    "        #target -> target.unsqueeze(1)\n",
    "        if sigBCE:\n",
    "            targets = sentence_data.target.float()#.unsqueeze(1)\n",
    "            vali_target = vali_data.target.float()\n",
    "        else:\n",
    "            targets = torch.LongTensor(sentence_data.target)#.unsqueeze(1)\n",
    "            vali_target = torch.LongTensor(vali.target)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        target_scores = model(sentence_in)\n",
    "        \n",
    "        loss = loss_function(target_scores.squeeze(1), targets)#.squeeze(1))#.reshape(64,1).squeeze(1))\n",
    "        loss = (loss * (targets*17+1)/(targets*17+1).sum()).sum()#.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        vali_scores = model(vali_in)\n",
    "        vali_loss = loss_function(vali_scores.squeeze(1), vali_target).mean()\n",
    "        vali_f1 = (np.asarray(vali_scores.squeeze(1)) > 0.5).astype(int)\n",
    "        if sigBCE:\n",
    "            #prediction = np.asarray(target_scores.squeeze(1)>0.5,dtype=float)\n",
    "            #accuracy = np.asarray(prediction == np.asarray(targets)).sum()/len(targets)*100\n",
    "            prediction = np.asarray(vali_scores.squeeze(1)>0.5,dtype=float)\n",
    "            accuracy = np.asarray(prediction == np.asarray(vali_target)).sum()/len(vali_target)*100\n",
    "        else:\n",
    "            #prediction = target_scores.data.max(1)[1]\n",
    "            #accuracy = np.asarray(prediction==targets).sum()/len(targets)*100\n",
    "            prediction = vali_scores.data.max(1)[1]\n",
    "            accuracy = np.asarray(prediction == vali_target).sum()/len(vali_target)*100\n",
    "        #train_accu.append(accuracy)\n",
    "        \n",
    "        if sigBCE:\n",
    "            all_preds.extend(target_scores.tolist())#torch.round(target_scores).tolist())\n",
    "        else:\n",
    "            all_preds.extend(torch.exp(target_scores).tolist())#torch.round(target_scores).tolist())\n",
    "        all_actual.extend(targets.tolist())\n",
    "        \n",
    "        #print(np.asarray(target_scores)[:,1].max())\n",
    "        \n",
    "        #targets -> targets.reshape(64,1).squeeze(1)\n",
    "        \n",
    "        avg_loss += loss.item()\n",
    "        \n",
    "        #print(loss)\n",
    "        if sigBCE:\n",
    "            print(batch_num+1,\" of: \",len(train_iter), \" Loss: \",loss.item(),\" F1 Val: \",f1_score(vali_f1, vali_target),\" Val Acc: \",accuracy, \" High: \", np.asarray(target_scores).max(),\" Low: \", np.asarray(target_scores).min())\n",
    "        else:\n",
    "            print(batch_num+1,\" of: \",len(train_iter), \" Loss: \",loss.item(),\" Val Loss: \",vali_loss.item(),\" Acc: \",accuracy, \" High: \", np.asarray(torch.exp(target_scores))[:,1].max(),\" Low: \", np.asarray(target_scores).min())\n",
    "        \n",
    "        \n",
    "    \n",
    "    #model.eval()\n",
    "    avg_loss = avg_loss/len(train_iter)\n",
    "    \n",
    "    #if validate:\n",
    "        #avg_val\n",
    "    \n",
    "    \n",
    "    #else:\n",
    "    print(\"Training loss for epoch {} is {}\".format(epoch + 1, avg_loss))\n",
    "    \n",
    "    \n",
    "    #train_confusion_matrix = confusion_matrix(all_actual, all_preds).ravel()\n",
    "    if sigBCE:\n",
    "        pred_Clas = (np.asarray(all_preds) > 0.5).astype(int)#-2.65).astype(int)#\n",
    "    else:\n",
    "        pred_Clas = (np.asarray(all_preds)[:,1] > np.asarray(all_preds)[:,0]).astype(int)#-2.65).astype(int)#\n",
    "    train_confusion_matrix = confusion_matrix(all_actual, pred_Clas).ravel()\n",
    "    \n",
    "    print(\"Confusion matrix for epoch {}, tn={}, fp={}, fn={}, tp={}\".format(epoch + 1, *train_confusion_matrix))\n",
    "    f1_train.append(f1_score(all_actual, pred_Clas))\n",
    "    print(\"F1 score for epoch {} is {}\".format(epoch + 1, f1_train[-1]))\n",
    "    fpr, tpr, _ = roc_curve(all_actual, pred_Clas, pos_label=1)\n",
    "    auc_train.append(auc(fpr, tpr))\n",
    "    print(\"AUC for epoch {} is {}\\n\".format(epoch + 1, auc_train[-1]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = inputs_check\n",
    "    score2 = model(inputs)\n",
    "    #print(score)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.save(model,\"C:/Users/xw5735/PWinFolder/WPy-3661/notebooks/KaggleChal/LSTMTaggerFull.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = torch.load(\"C:/Users/xw5735/PWinFolder/WPy-3661/notebooks/KaggleChal/LSTMTaggerFull.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "nb_classes = 2\n",
    "target = torch.LongTensor(batch_size).random_(nb_classes)\n",
    "#print((targets*5+1))\n",
    "print((loss * (targets*5+1)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction = np.asarray(target_scores.squeeze(1)>0.5,dtype=float)\n",
    "target_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preddd = np.asarray(all_preds)[:,1]\n",
    "#preddd_clas = (preddd>-0.45).astype(int)\n",
    "#train_confusion_matrix = confusion_matrix(all_actual, preddd_clas).ravel()\n",
    "#print(\"Confusion matrix for epoch {}, tn={}, fp={}, fn={}, tp={}\".format(epoch + 1, *train_confusion_matrix))\n",
    "#pred_Clas = (np.asarray(all_preds)[:,1] > np.asarray(all_preds)[:,0]).astype(int)\n",
    "#f1_score(all_actual, pred_Clas)\n",
    "#pred_Clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preddd = (np.asarray(all_preds)[:,1] > np.asarray(all_preds)[:,0]).astype(int)\n",
    "#len(preddd[preddd>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_search(y_true, y_proba, plot=False):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    thresholds = np.append(thresholds, 1.001) \n",
    "    F = 2 / (1/precision + 1/recall)\n",
    "    best_score = np.max(F)\n",
    "    best_th = thresholds[np.argmax(F)]\n",
    "    if plot:\n",
    "        plt.plot(thresholds, F, '-b')\n",
    "        plt.plot([best_th], [best_score], '*r')\n",
    "        plt.show()\n",
    "    search_result = {'threshold': best_th , 'f1': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_search_2(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(100)], disable=True):\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#threshold_search_2(all_actual, preddd)\n",
    "#np.asarray(all_preds)[:,1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preddd = np.asarray(all_preds)[:,1]\n",
    "#preddd_clas = (preddd>-0.45).astype(int)\n",
    "#preddd.max()\n",
    "#len(preddd_clas[preddd_clas == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thr = threshold_search_2(all_actual, np.asarray(all_preds));thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thr = threshold_search(all_actual, np.asarray(all_preds), plot=True);thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_final_preds = []\n",
    "all_final_rounding = []\n",
    "model.eval()\n",
    "testing_batches = iter(test_iter)\n",
    "for batch_num in range(len(test_iter)):\n",
    "    final_preds = model(next(testing_batches).question_text)\n",
    "    prediction = np.asarray(final_preds>0.5,dtype=float)\n",
    "    print(batch_num+1,\"out of: \",len(test_iter),\" max: \",np.asarray(final_preds).max())\n",
    "    all_final_preds.extend(final_preds.tolist())\n",
    "    all_final_rounding.extend(prediction.tolist())\n",
    "    \n",
    "\n",
    "#probab = np.asarray(final_preds)[:,1]\n",
    "\n",
    "#final_preds_clas = (probab>thr['threshold']).astype(int)\n",
    "\n",
    "#final_preds_clas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(path/'sample_submission.csv')\n",
    "sub['prediction'] = (np.asarray(all_final_preds)>0.5).astype(int)\n",
    "sub.to_csv(\"submission_Kag_Full2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(all_final_preds).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_batches = iter(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, weop in enumerate(test_iter):\n",
    "    print(i,\" \",weop.question_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enumerate(test_iter)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itering = next(testing_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itering.input_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itering.question_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.itos[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thr['threshold']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def train_model(model, train_data, epochs, show=False):\n",
    "    \n",
    "    #optimizer = torch.optim.Adam(model.parameters())\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "    #loss_function = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    print(\"The loss function being used is {}\".format(loss_function))\n",
    "    errors = []\n",
    "    eval_errors = []\n",
    "    f1_train = []\n",
    "    auc_train = []\n",
    "    f1_eval = []\n",
    "    auc_eval = []\n",
    "    \n",
    "    num_training_batches = len(train_data)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {}\".format(epoch + 1))\n",
    "        print(\"Training mode\")\n",
    "        model.train()\n",
    "        train_iter = iter(train_data)\n",
    "        model.zero_grad()\n",
    "        train_loss = 0\n",
    "        all_preds = []\n",
    "        all_actual = []\n",
    "        for batch_num in range(num_training_batches):\n",
    "            print(\"Batch {}\".format(batch_num + 1))\n",
    "            print(\"Out of {}\".format(num_training_batches))\n",
    "            batch = next(train_iter)\n",
    "            text, class_vector = batch.question_text.transpose(0,1),batch.target.unsqueeze(1)\n",
    "            #print(batch)\n",
    "            #print(text.shape)\n",
    "            #optimizer.zero_grad()\n",
    "            text_pred = model(text)\n",
    "            #print(text_pred)\n",
    "            print(class_vector.shape)\n",
    "            print(torch.max(class_vector,1)[0].shape)\n",
    "            loss = loss_function(text_pred,class_vector.reshape(64,1).squeeze(1)) #torch.max(class_vector,1)[0])\n",
    "            print(loss)\n",
    "            print(text_pred.squeeze(1))\n",
    "            print(torch.max(class_vector,1)[0])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#train_model(net, train_iter,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_iters = iter(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wee = next(train_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wee.question_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
