{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastai.vision import *\n",
    "from fastai import * \n",
    "from fastai.text import *\n",
    "from pathlib import Path \n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"my bar!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "from torchvision import transforms, utils\n",
    "from sklearn.metrics import confusion_matrix, f1_score, auc, roc_curve, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = True\n",
    "clean = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('../input/embeddings'),\n",
       " WindowsPath('../input/glove.840B.300d'),\n",
       " WindowsPath('../input/sample_submission.csv'),\n",
       " WindowsPath('../input/test.csv'),\n",
       " WindowsPath('../input/test_Clean.csv'),\n",
       " WindowsPath('../input/tmp_lm'),\n",
       " WindowsPath('../input/train.csv'),\n",
       " WindowsPath('../input/train_Clean.csv')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if lm: path = Path('../input'); \n",
    "list(path.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_df = pd.read_csv(path/'train.csv')\n",
    "test_df = pd.read_csv(path/'test.csv')\n",
    "allText_df = train_df.copy().append(test_df.copy(), sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clean:\n",
    "    train_df = pd.read_csv(path/'train.csv')\n",
    "    test_df = pd.read_csv(path/'test.csv')\n",
    "    \n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "    #allText_df[\"question_text\"] = allText_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "\n",
    "    train_df['question_text'] = train_df['question_text'].apply(lambda x: clean_text(x))\n",
    "    test_df['question_text'] = test_df['question_text'].apply(lambda x: clean_text(x))\n",
    "    #allText_df[\"question_text\"] = allText_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "\n",
    "    train_df['question_text'] = train_df['question_text'].apply(lambda x: clean_numbers(x))\n",
    "    test_df['question_text'] = test_df['question_text'].apply(lambda x:clean_numbers(x))\n",
    "    #allText_df[\"question_text\"] = allText_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "\n",
    "    train_df['question_text'] = train_df['question_text'].apply(lambda x: replace_typical_misspell(x))\n",
    "    test_df['question_text'] = test_df['question_text'].apply(lambda x: replace_typical_misspell(x))\n",
    "    #allText_df[\"question_text\"] = allText_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "\n",
    "    train_x = train_df['question_text'].fillna('_##_').values\n",
    "    test_x = test_df['question_text'].fillna('_##_').values\n",
    "    \n",
    "    train_Full = pd.read_csv(path/'train.csv')\n",
    "    test_Full = pd.read_csv(path/'test.csv')\n",
    "    \n",
    "    train_Full['question_text'] = train_x\n",
    "    test_Full['question_text'] = test_x\n",
    "    \n",
    "    train_Full.to_csv('train_Clean.csv',index=False)\n",
    "    test_Full.to_csv('test_Clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import zipfile\n",
    "zf = zipfile.ZipFile(path/'test.csv.zip')\n",
    "df_test = pd.read_csv(zf.open('test.csv'))\n",
    "zf = zipfile.ZipFile(path/'train.csv.zip')\n",
    "df_train = pd.read_csv(zf.open('train.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_train.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences():\n",
    "    tokenizer = lambda text: text.split() # the function above is the function we will be using to tokenize the text\n",
    "    TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer, lower=True)\n",
    "    LABEL = torchtext.data.Field(sequential=False, use_vocab=False) # sequential and use_vocab=False since no text (binary)\n",
    "    train_datafields = [(\"qid\", None), (\"question_text\", TEXT), (\"target\", LABEL)]\n",
    "    train = torchtext.data.TabularDataset( # If we had a validation set as well, we would add an additional .splits(...)\n",
    "                        # path=\"data/train_cleaned_v2.csv\", # the root directory where the data lies\n",
    "                        path = path/'train_Clean.csv',\n",
    "                        format='csv',\n",
    "                        # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "                        skip_header=True, \n",
    "                        fields=train_datafields)\n",
    "    test_datafields = [(\"qid\", None),\n",
    "                     (\"question_text\", TEXT)] \n",
    "    test = torchtext.data.TabularDataset( \n",
    "                path=path/'test_Clean.csv',\n",
    "                format=\"csv\",\n",
    "                skip_header=True,\n",
    "                fields=test_datafields)\n",
    "    return TEXT, LABEL, train, test\n",
    "# Chat Conversation End\n",
    "# Type a message...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT,LABEL,train,test = prepare_sequences()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LABEL.batch_first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, test, vectors = \"glove.6B.50d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_storage = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,vali = train.split(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "train_iter = BucketIterator(\n",
    "    train, # we pass in the datasets we want the iterator to draw data from\n",
    "    batch_size= 128, \n",
    "    sort_key= lambda x: len(x.question_text), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "    sort_within_batch=False, # sorting would add bias\n",
    "    repeat=False)\n",
    "\n",
    "vali_iter = BucketIterator(\n",
    "    vali, # we pass in the datasets we want the iterator to draw data from\n",
    "    batch_size= 128, \n",
    "    sort_key= lambda x: len(x.question_text), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "    sort_within_batch=False, # sorting would add bias\n",
    "    repeat=False)\n",
    "\n",
    "test_iter = Iterator(\n",
    "    test,\n",
    "    batch_size = 128,\n",
    "    train=False,\n",
    "    sort = False,\n",
    "    sort_within_batch = False,\n",
    "    repeat = False)\n",
    "\n",
    "ngpu = 0\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iter(test_iter).__next__().question_text\n",
    "\n",
    "#len(train_iter)\n",
    "#label_Size = next(iter(train_iter)).target\n",
    "#len(label_Size[label_Size==1])\n",
    "#train_iter.batches\n",
    "#train_iter = iter(train_iter)\n",
    "#train_iter.__next__().question_text\n",
    "#train_iter.__next__().target\n",
    "#iter(train_iter).__next__().question_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#smaller,_ = train.split(0.1)\n",
    "TEXT.vocab.itos[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_iter.dataset.examples.__getitem__(1).__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#smaller.examples[0].question_text\n",
    "test.examples.__getitem__(2).__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(train)\n",
    "#len(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.examples[0].question_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([214945, 50])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT.vocab.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 50\n",
    "hidden_size = 300\n",
    "ver1 = False\n",
    "sigBCE = True\n",
    "bidir = True\n",
    "layer_LSTM = 2\n",
    "layer_Lin = 3\n",
    "dropout_LSTM = 0.1\n",
    "dropout_Lin = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        \n",
    "        hidden_size = 64\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab),embed_size)\n",
    "        self.embedding.weight = nn.Parameter(TEXT.vocab.vectors)\n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size*4,2)\n",
    "        self.relu = nn.ReLU()\n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(16,1)\n",
    "        \n",
    "    def forward(self,train):\n",
    "        #print(train.shape)\n",
    "        h_embedding = self.embedding(train)\n",
    "        #print(h_embedding.shape)\n",
    "        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding,0)))\n",
    "        #print(h_embedding.shape)\n",
    "        \n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        #print(h_lstm.shape)\n",
    "        h_gru, _ = self.gru(h_lstm)\n",
    "        #print(h_gru.shape)\n",
    "        \n",
    "        avg_pool = torch.mean(h_gru,1)\n",
    "        #print(avg_pool.shape)\n",
    "        #avg_pool = avg_pool.view(-1,64)\n",
    "        max_pool, _ = torch.max(h_gru,1)\n",
    "        #print(max_pool.shape)\n",
    "        \n",
    "        \n",
    "        conc = torch.cat((avg_pool, max_pool),1)\n",
    "        print(conc.shape)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        #print(conc.shape)\n",
    "        #conc = self.dropout(conc)\n",
    "        print(conc.shape)\n",
    "        #out = nn.Sigmoid()(conc)\n",
    "        \n",
    "        out = nn.LogSoftmax(dim=1)(conc)\n",
    "        \n",
    "        #out = self.out(conc)\n",
    "        #print(out[:,-1].shape)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "batch = next(iter(train_iter)).question_text\n",
    "net = NeuralNet()\n",
    "#net(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch = next(iter(train_iter)).question_text\n",
    "#batch[30]\n",
    "#layer_Lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1])\n",
      "tensor([[0.4897],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4897],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4896],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4894],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4897],\n",
      "        [0.4897],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4897],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4894],\n",
      "        [0.4895],\n",
      "        [0.4898],\n",
      "        [0.4895],\n",
      "        [0.4897],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4897],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4894],\n",
      "        [0.4894],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4894],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4898],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4897],\n",
      "        [0.4894],\n",
      "        [0.4895],\n",
      "        [0.4897],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4894],\n",
      "        [0.4897],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4894],\n",
      "        [0.4898],\n",
      "        [0.4896],\n",
      "        [0.4897],\n",
      "        [0.4896],\n",
      "        [0.4894],\n",
      "        [0.4894],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4900],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4898],\n",
      "        [0.4898],\n",
      "        [0.4894],\n",
      "        [0.4896],\n",
      "        [0.4897],\n",
      "        [0.4895],\n",
      "        [0.4895],\n",
      "        [0.4899],\n",
      "        [0.4895],\n",
      "        [0.4898],\n",
      "        [0.4895],\n",
      "        [0.4897],\n",
      "        [0.4896],\n",
      "        [0.4895],\n",
      "        [0.4898],\n",
      "        [0.4894],\n",
      "        [0.4896]])\n"
     ]
    }
   ],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "                      \n",
    "        self.hidden_dim = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab),embed_size)\n",
    "        self.embedding.weight = nn.Parameter(TEXT.vocab.vectors)\n",
    "        \n",
    "        \n",
    "        #self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        \n",
    "        self.lstm_Layer = nn.LSTM(embed_size, hidden_size,num_layers = layer_LSTM, dropout = dropout_LSTM, bidirectional = bidir)\n",
    "        \n",
    "        self.lin_Layer = []\n",
    "        \n",
    "        self.lin_Layer_Dropout = dropout_Lin\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        if bidir:\n",
    "            scale = 2\n",
    "        else:\n",
    "            scale = 1\n",
    "        \n",
    "        for _ in range(layer_Lin - 1):\n",
    "            self.lin_Layer.append(nn.Linear(hidden_size*scale,hidden_size*scale))\n",
    "            self.lin_Layer = nn.ModuleList(self.lin_Layer)\n",
    "        if sigBCE:\n",
    "            self.hidden2tag = nn.Linear(hidden_size*scale, 1)\n",
    "        else:\n",
    "            self.hidden2tag = nn.Linear(hidden_size*scale, 2)\n",
    "    \n",
    "    def forward(self, train):\n",
    "        #print(train.shape)\n",
    "        embeds = self.embedding(train)\n",
    "        #print(embeds.shape)\n",
    "        #print(embeds.view(len(train),64,-1).shape)\n",
    "        #print(embeds.view(len(train),train.shape[1],-1).shape)\n",
    "        lstm_out, _ = self.lstm_Layer(embeds)#embeds.view(len(train),train.shape[1],-1))\n",
    "        #print(lstm_out.shape)\n",
    "        \n",
    "        if ver1:\n",
    "            #print(lstm_out.shape)\n",
    "            feature = lstm_out[-1,:,:] # -1 grabs last layer in list\n",
    "            #print(feature.shape)\n",
    "        \n",
    "            for layer in self.lin_Layer:\n",
    "                feature = layer(feature)\n",
    "                feature = F.relu(feature)\n",
    "                predict = self.hidden2tag(feature)\n",
    "        \n",
    "            feature = F.relu(feature)\n",
    "            predict = self.hidden2tag(feature)\n",
    "            if sigBCE:\n",
    "                return nn.Sigmoid()(predict)\n",
    "            else:\n",
    "                return nn.LogSoftmax(dim=1)(predict)\n",
    "        else:\n",
    "           \n",
    "            feature = torch.mean(lstm_out,0) #avg_pool\n",
    "            #print(avg_pool.shape)\n",
    "            for layer in self.lin_Layer:\n",
    "                feature = layer(feature)\n",
    "                feature = F.relu(feature)\n",
    "                \n",
    "            target_space = self.hidden2tag(feature)#.view(len(train),-1))\n",
    "            #target_space = self.relu(target_space)\n",
    "            \n",
    "            #print(target_space.shape)\n",
    "            if sigBCE:\n",
    "                return nn.Sigmoid()(target_space)\n",
    "            else:\n",
    "                return F.log_softmax(target_space, dim=1)\n",
    "            #print(target_score.shape)\n",
    "\n",
    "inputs_check = next(iter(train_iter)).question_text\n",
    "\n",
    "#print(inputs_check.shape)\n",
    "\n",
    "model = LSTMTagger()\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = inputs_check\n",
    "    #print(len(inputs))\n",
    "    score = model(inputs)\n",
    "    print(score.shape)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    if  isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger()\n",
    "model.apply(weight_init)\n",
    "if sigBCE:\n",
    "    loss_function = nn.BCELoss(reduction='none')\n",
    "else:\n",
    "    loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  of:  8164  Loss:  0.688176691532135  F1 Val:  0.13138686131386862  Val Acc:  7.03125  High:  0.50770664  Low:  0.5042168\n",
      "2  of:  8164  Loss:  0.6758392453193665  F1 Val:  0.1037037037037037  Val Acc:  5.46875  High:  0.5402855  Low:  0.5379057\n",
      "3  of:  8164  Loss:  0.702202558517456  F1 Val:  0.06060606060606061  Val Acc:  3.125  High:  0.58444995  Low:  0.576483\n",
      "4  of:  8164  Loss:  0.7081770896911621  F1 Val:  0.1037037037037037  Val Acc:  5.46875  High:  0.6109275  Low:  0.59842134\n",
      "5  of:  8164  Loss:  0.7098520994186401  F1 Val:  0.11764705882352941  Val Acc:  6.25  High:  0.62048143  Low:  0.60295963\n",
      "6  of:  8164  Loss:  0.7029476761817932  F1 Val:  0.1037037037037037  Val Acc:  5.46875  High:  0.6173885  Low:  0.5930264\n",
      "7  of:  8164  Loss:  0.7635444402694702  F1 Val:  0.08955223880597014  Val Acc:  4.6875  High:  0.59926856  Low:  0.5718313\n",
      "8  of:  8164  Loss:  0.6823981404304504  F1 Val:  0.0  Val Acc:  92.96875  High:  0.5443564  Low:  0.5188275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xw5735\\PWinFolder\\WPy-3661\\python-3.6.6.amd64\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9  of:  8164  Loss:  0.6906312108039856  F1 Val:  0.0  Val Acc:  90.625  High:  0.5116931  Low:  0.48676753\n",
      "10  of:  8164  Loss:  0.6824336647987366  F1 Val:  0.0  Val Acc:  92.96875  High:  0.48861262  Low:  0.46397918\n",
      "11  of:  8164  Loss:  0.7017262578010559  F1 Val:  0.0  Val Acc:  93.75  High:  0.466568  Low:  0.44214916\n",
      "12  of:  8164  Loss:  0.6652824878692627  F1 Val:  0.0  Val Acc:  93.75  High:  0.45100683  Low:  0.43070987\n",
      "13  of:  8164  Loss:  0.7276193499565125  F1 Val:  0.0  Val Acc:  96.09375  High:  0.4346541  Low:  0.41468245\n",
      "14  of:  8164  Loss:  0.637390673160553  F1 Val:  0.0  Val Acc:  92.1875  High:  0.44155023  Low:  0.41554093\n",
      "15  of:  8164  Loss:  0.6029891967773438  F1 Val:  0.0  Val Acc:  90.625  High:  0.42940304  Low:  0.40427542\n",
      "16  of:  8164  Loss:  0.8174329996109009  F1 Val:  0.0  Val Acc:  90.625  High:  0.41362154  Low:  0.37470463\n",
      "17  of:  8164  Loss:  0.7081589102745056  F1 Val:  0.0  Val Acc:  94.53125  High:  0.4351874  Low:  0.38467008\n",
      "18  of:  8164  Loss:  0.7405253052711487  F1 Val:  0.3478260869565218  Val Acc:  88.28125  High:  0.46785638  Low:  0.40869227\n",
      "19  of:  8164  Loss:  0.701583206653595  F1 Val:  0.1037037037037037  Val Acc:  5.46875  High:  0.51839983  Low:  0.45859194\n",
      "20  of:  8164  Loss:  0.6885482668876648  F1 Val:  0.13138686131386862  Val Acc:  7.03125  High:  0.58000255  Low:  0.5210449\n",
      "21  of:  8164  Loss:  0.6953784227371216  F1 Val:  0.11764705882352941  Val Acc:  6.25  High:  0.6352721  Low:  0.5647374\n",
      "22  of:  8164  Loss:  0.6923102736473083  F1 Val:  0.15827338129496402  Val Acc:  8.59375  High:  0.6777178  Low:  0.59204936\n",
      "23  of:  8164  Loss:  0.7367850542068481  F1 Val:  0.17142857142857143  Val Acc:  9.375  High:  0.6773923  Low:  0.6057151\n",
      "24  of:  8164  Loss:  0.6156191825866699  F1 Val:  0.15827338129496402  Val Acc:  8.59375  High:  0.6795199  Low:  0.5936526\n",
      "25  of:  8164  Loss:  0.6521267890930176  F1 Val:  0.1037037037037037  Val Acc:  5.46875  High:  0.6885462  Low:  0.59013164\n",
      "26  of:  8164  Loss:  0.7376619577407837  F1 Val:  0.15827338129496402  Val Acc:  8.59375  High:  0.6909064  Low:  0.59000987\n",
      "27  of:  8164  Loss:  0.6978466510772705  F1 Val:  0.17142857142857143  Val Acc:  9.375  High:  0.68224037  Low:  0.5617698\n",
      "28  of:  8164  Loss:  0.6460549235343933  F1 Val:  0.11764705882352941  Val Acc:  6.25  High:  0.65573144  Low:  0.5299917\n",
      "29  of:  8164  Loss:  0.6867730021476746  F1 Val:  0.09523809523809523  Val Acc:  10.9375  High:  0.63931316  Low:  0.5163225\n",
      "30  of:  8164  Loss:  0.6775913834571838  F1 Val:  0.1702127659574468  Val Acc:  69.53125  High:  0.62112135  Low:  0.50000155\n",
      "31  of:  8164  Loss:  0.6691449880599976  F1 Val:  0.06060606060606061  Val Acc:  75.78125  High:  0.6204126  Low:  0.47796965\n",
      "32  of:  8164  Loss:  0.6489081978797913  F1 Val:  0.3137254901960785  Val Acc:  72.65625  High:  0.6354657  Low:  0.46037328\n",
      "33  of:  8164  Loss:  0.6987074017524719  F1 Val:  0.27027027027027023  Val Acc:  78.90625  High:  0.6508877  Low:  0.4496378\n",
      "34  of:  8164  Loss:  0.666895866394043  F1 Val:  0.3225806451612903  Val Acc:  83.59375  High:  0.7001319  Low:  0.45026606\n",
      "35  of:  8164  Loss:  0.6808347702026367  F1 Val:  0.3043478260869565  Val Acc:  75.0  High:  0.6682597  Low:  0.42267808\n",
      "36  of:  8164  Loss:  0.6878936886787415  F1 Val:  0.19354838709677416  Val Acc:  80.46875  High:  0.71934813  Low:  0.41259122\n",
      "37  of:  8164  Loss:  0.6509764790534973  F1 Val:  0.0625  Val Acc:  76.5625  High:  0.72976756  Low:  0.40902105\n",
      "38  of:  8164  Loss:  0.6771788001060486  F1 Val:  0.1904761904761905  Val Acc:  60.15625  High:  0.79294765  Low:  0.40632543\n",
      "39  of:  8164  Loss:  0.6585878133773804  F1 Val:  0.15873015873015872  Val Acc:  17.1875  High:  0.84404933  Low:  0.4223952\n",
      "40  of:  8164  Loss:  0.6454756259918213  F1 Val:  0.12030075187969924  Val Acc:  8.59375  High:  0.89559436  Low:  0.4490257\n",
      "41  of:  8164  Loss:  0.7732606530189514  F1 Val:  0.2068965517241379  Val Acc:  64.0625  High:  0.9413086  Low:  0.48880577\n",
      "42  of:  8164  Loss:  0.6686404943466187  F1 Val:  0.2173913043478261  Val Acc:  71.875  High:  0.79421794  Low:  0.41200307\n",
      "43  of:  8164  Loss:  0.6380164623260498  F1 Val:  0.12903225806451615  Val Acc:  78.90625  High:  0.8646725  Low:  0.3926503\n",
      "44  of:  8164  Loss:  0.6064992547035217  F1 Val:  0.21428571428571427  Val Acc:  82.8125  High:  0.84087425  Low:  0.3754474\n",
      "45  of:  8164  Loss:  0.5964235067367554  F1 Val:  0.11111111111111112  Val Acc:  87.5  High:  0.76651514  Low:  0.3566945\n",
      "46  of:  8164  Loss:  0.747511625289917  F1 Val:  0.4615384615384615  Val Acc:  94.53125  High:  0.6933093  Low:  0.32161412\n",
      "47  of:  8164  Loss:  0.812232494354248  F1 Val:  0.125  Val Acc:  89.0625  High:  0.6725609  Low:  0.30791086\n",
      "48  of:  8164  Loss:  0.74212646484375  F1 Val:  0.14285714285714288  Val Acc:  90.625  High:  0.6781899  Low:  0.31541398\n",
      "49  of:  8164  Loss:  0.6458480358123779  F1 Val:  0.16666666666666666  Val Acc:  92.1875  High:  0.6904317  Low:  0.3321792\n",
      "50  of:  8164  Loss:  0.6170843243598938  F1 Val:  0.23076923076923073  Val Acc:  84.375  High:  0.7043703  Low:  0.3483141\n",
      "51  of:  8164  Loss:  0.6187364459037781  F1 Val:  0.125  Val Acc:  78.125  High:  0.7469713  Low:  0.37100375\n",
      "52  of:  8164  Loss:  0.5848244428634644  F1 Val:  0.21428571428571427  Val Acc:  82.8125  High:  0.8914539  Low:  0.39710605\n",
      "53  of:  8164  Loss:  0.7606372833251953  F1 Val:  0.15873015873015872  Val Acc:  58.59375  High:  0.96509016  Low:  0.41534087\n",
      "54  of:  8164  Loss:  0.6532875895500183  F1 Val:  0.14925373134328357  Val Acc:  55.46875  High:  0.9801145  Low:  0.43382686\n",
      "55  of:  8164  Loss:  0.522120475769043  F1 Val:  0.1111111111111111  Val Acc:  62.5  High:  0.9857611  Low:  0.4516697\n",
      "56  of:  8164  Loss:  0.6989912986755371  F1 Val:  0.07017543859649122  Val Acc:  58.59375  High:  0.9863468  Low:  0.45750448\n",
      "57  of:  8164  Loss:  0.6528054475784302  F1 Val:  0.14285714285714285  Val Acc:  43.75  High:  0.9917748  Low:  0.4585299\n",
      "58  of:  8164  Loss:  0.6708732843399048  F1 Val:  0.1621621621621622  Val Acc:  51.5625  High:  0.9818749  Low:  0.45715606\n",
      "59  of:  8164  Loss:  0.6721411943435669  F1 Val:  0.2857142857142857  Val Acc:  60.9375  High:  0.9315015  Low:  0.46162683\n",
      "60  of:  8164  Loss:  0.6482059359550476  F1 Val:  0.36363636363636365  Val Acc:  83.59375  High:  0.96984226  Low:  0.45559835\n",
      "61  of:  8164  Loss:  0.6792210936546326  F1 Val:  0.0  Val Acc:  83.59375  High:  0.9474466  Low:  0.45218116\n",
      "62  of:  8164  Loss:  0.6441255807876587  F1 Val:  0.3684210526315789  Val Acc:  81.25  High:  0.9018893  Low:  0.44201276\n",
      "63  of:  8164  Loss:  0.6412597298622131  F1 Val:  0.25  Val Acc:  90.625  High:  0.78683746  Low:  0.43069646\n",
      "64  of:  8164  Loss:  0.7184280157089233  F1 Val:  0.12500000000000003  Val Acc:  89.0625  High:  0.7426173  Low:  0.42610663\n",
      "65  of:  8164  Loss:  0.6933350563049316  F1 Val:  0.27586206896551724  Val Acc:  83.59375  High:  0.67717224  Low:  0.42737904\n",
      "66  of:  8164  Loss:  0.725587010383606  F1 Val:  0.1142857142857143  Val Acc:  75.78125  High:  0.5335312  Low:  0.4382771\n",
      "67  of:  8164  Loss:  0.6916622519493103  F1 Val:  0.23076923076923075  Val Acc:  84.375  High:  0.750402  Low:  0.46089086\n",
      "68  of:  8164  Loss:  0.6938027143478394  F1 Val:  0.3157894736842105  Val Acc:  89.84375  High:  0.72100735  Low:  0.4644537\n",
      "69  of:  8164  Loss:  0.6801408529281616  F1 Val:  0.15384615384615383  Val Acc:  74.21875  High:  0.71323407  Low:  0.47176525\n",
      "70  of:  8164  Loss:  0.670924961566925  F1 Val:  0.07518796992481203  Val Acc:  3.90625  High:  0.71619457  Low:  0.4842304\n",
      "71  of:  8164  Loss:  0.6494960188865662  F1 Val:  0.1090909090909091  Val Acc:  23.4375  High:  0.726517  Low:  0.5007427\n",
      "72  of:  8164  Loss:  0.6326581835746765  F1 Val:  0.14545454545454545  Val Acc:  63.28125  High:  0.7695403  Low:  0.499287\n",
      "73  of:  8164  Loss:  0.6388938426971436  F1 Val:  0.15  Val Acc:  73.4375  High:  0.8006572  Low:  0.4912177\n",
      "74  of:  8164  Loss:  0.6591888070106506  F1 Val:  0.3333333333333333  Val Acc:  84.375  High:  0.8335369  Low:  0.4755613\n",
      "75  of:  8164  Loss:  0.7075197696685791  F1 Val:  0.0625  Val Acc:  76.5625  High:  0.8304617  Low:  0.4534877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76  of:  8164  Loss:  0.5630213618278503  F1 Val:  0.0  Val Acc:  82.03125  High:  0.8974603  Low:  0.42340174\n",
      "77  of:  8164  Loss:  0.6409696936607361  F1 Val:  0.22222222222222224  Val Acc:  83.59375  High:  0.9024423  Low:  0.39854848\n",
      "78  of:  8164  Loss:  0.6876213550567627  F1 Val:  0.2941176470588235  Val Acc:  81.25  High:  0.8764111  Low:  0.3711197\n",
      "79  of:  8164  Loss:  0.6807613968849182  F1 Val:  0.1  Val Acc:  85.9375  High:  0.919598  Low:  0.34155375\n",
      "80  of:  8164  Loss:  0.6094496250152588  F1 Val:  0.0  Val Acc:  89.84375  High:  0.87793094  Low:  0.2992661\n",
      "81  of:  8164  Loss:  0.5810007452964783  F1 Val:  0.25000000000000006  Val Acc:  90.625  High:  0.79361844  Low:  0.27317765\n",
      "82  of:  8164  Loss:  0.6326757073402405  F1 Val:  0.26666666666666666  Val Acc:  91.40625  High:  0.79573876  Low:  0.24829033\n",
      "83  of:  8164  Loss:  0.6885895729064941  F1 Val:  0.5  Val Acc:  96.875  High:  0.76314235  Low:  0.22429238\n",
      "84  of:  8164  Loss:  0.6453666090965271  F1 Val:  0.11764705882352941  Val Acc:  88.28125  High:  0.74031943  Low:  0.20966958\n",
      "85  of:  8164  Loss:  0.9602183699607849  F1 Val:  0.2631578947368421  Val Acc:  78.125  High:  0.742893  Low:  0.2066215\n",
      "86  of:  8164  Loss:  0.6534373164176941  F1 Val:  0.22580645161290322  Val Acc:  62.5  High:  0.9417791  Low:  0.2429805\n",
      "87  of:  8164  Loss:  0.5400876402854919  F1 Val:  0.15126050420168066  Val Acc:  21.09375  High:  0.9494989  Low:  0.28167343\n",
      "88  of:  8164  Loss:  0.6623893976211548  F1 Val:  0.09756097560975609  Val Acc:  13.28125  High:  0.9948226  Low:  0.35946974\n",
      "89  of:  8164  Loss:  1.013735055923462  F1 Val:  0.09756097560975609  Val Acc:  42.1875  High:  0.9975115  Low:  0.45826155\n",
      "90  of:  8164  Loss:  0.6614940166473389  F1 Val:  0.11764705882352941  Val Acc:  76.5625  High:  0.96256804  Low:  0.3324966\n",
      "91  of:  8164  Loss:  0.6216100454330444  F1 Val:  0.2857142857142857  Val Acc:  88.28125  High:  0.8573338  Low:  0.31814954\n",
      "92  of:  8164  Loss:  0.656419038772583  F1 Val:  0.0  Val Acc:  89.0625  High:  0.6972805  Low:  0.3078342\n",
      "93  of:  8164  Loss:  0.6257070899009705  F1 Val:  0.0  Val Acc:  93.75  High:  0.58308214  Low:  0.30926225\n",
      "94  of:  8164  Loss:  0.659593939781189  F1 Val:  0.0  Val Acc:  94.53125  High:  0.5136745  Low:  0.30871305\n",
      "95  of:  8164  Loss:  0.5457373261451721  F1 Val:  0.0  Val Acc:  96.09375  High:  0.47581944  Low:  0.30961454\n",
      "96  of:  8164  Loss:  0.6791322231292725  F1 Val:  0.0  Val Acc:  89.84375  High:  0.41100675  Low:  0.2981392\n",
      "97  of:  8164  Loss:  0.7426208853721619  F1 Val:  0.0  Val Acc:  90.625  High:  0.40418696  Low:  0.29318276\n",
      "98  of:  8164  Loss:  0.7693606615066528  F1 Val:  0.0  Val Acc:  93.75  High:  0.4226747  Low:  0.30485305\n",
      "99  of:  8164  Loss:  0.7483328580856323  F1 Val:  0.0  Val Acc:  92.1875  High:  0.44684026  Low:  0.32064837\n",
      "100  of:  8164  Loss:  0.711746335029602  F1 Val:  0.28571428571428575  Val Acc:  92.1875  High:  0.48927566  Low:  0.34891617\n",
      "101  of:  8164  Loss:  0.6456022262573242  F1 Val:  0.0  Val Acc:  88.28125  High:  0.52497315  Low:  0.38744274\n",
      "102  of:  8164  Loss:  0.7052803039550781  F1 Val:  0.3478260869565218  Val Acc:  88.28125  High:  0.5541636  Low:  0.41475815\n",
      "103  of:  8164  Loss:  0.6815016865730286  F1 Val:  0.15094339622641506  Val Acc:  29.6875  High:  0.6123597  Low:  0.45029068\n",
      "104  of:  8164  Loss:  0.6890960335731506  F1 Val:  0.06060606060606061  Val Acc:  3.125  High:  0.6944228  Low:  0.4809119\n",
      "105  of:  8164  Loss:  0.6336185932159424  F1 Val:  0.11764705882352941  Val Acc:  6.25  High:  0.70317364  Low:  0.5047461\n",
      "106  of:  8164  Loss:  0.6306774020195007  F1 Val:  0.15827338129496402  Val Acc:  8.59375  High:  0.7681194  Low:  0.5240917\n",
      "107  of:  8164  Loss:  0.6728813052177429  F1 Val:  0.1037037037037037  Val Acc:  5.46875  High:  0.68424624  Low:  0.53713125\n",
      "108  of:  8164  Loss:  0.6211565732955933  F1 Val:  0.06060606060606061  Val Acc:  3.125  High:  0.8357204  Low:  0.5720365\n",
      "109  of:  8164  Loss:  0.8154076337814331  F1 Val:  0.11764705882352941  Val Acc:  6.25  High:  0.8742557  Low:  0.5936965\n",
      "110  of:  8164  Loss:  0.6872654557228088  F1 Val:  0.08955223880597014  Val Acc:  4.6875  High:  0.860259  Low:  0.56122863\n",
      "111  of:  8164  Loss:  0.6150660514831543  F1 Val:  0.11764705882352941  Val Acc:  17.96875  High:  0.9012405  Low:  0.51429397\n",
      "112  of:  8164  Loss:  0.6008660197257996  F1 Val:  0.1081081081081081  Val Acc:  22.65625  High:  0.88089484  Low:  0.46541706\n",
      "113  of:  8164  Loss:  0.6603122353553772  F1 Val:  0.1956521739130435  Val Acc:  42.1875  High:  0.92251116  Low:  0.42196906\n",
      "114  of:  8164  Loss:  0.6126574277877808  F1 Val:  0.24561403508771928  Val Acc:  66.40625  High:  0.9396101  Low:  0.39165822\n",
      "115  of:  8164  Loss:  0.6124286651611328  F1 Val:  0.18750000000000003  Val Acc:  79.6875  High:  0.94430023  Low:  0.35133675\n",
      "116  of:  8164  Loss:  0.5959171652793884  F1 Val:  0.23076923076923078  Val Acc:  84.375  High:  0.8489616  Low:  0.27916786\n",
      "117  of:  8164  Loss:  0.5656629204750061  F1 Val:  0.15384615384615383  Val Acc:  57.03125  High:  0.88276166  Low:  0.25253105\n",
      "118  of:  8164  Loss:  0.49158281087875366  F1 Val:  0.25  Val Acc:  71.875  High:  0.91189563  Low:  0.17128249\n",
      "119  of:  8164  Loss:  0.5552113056182861  F1 Val:  0.1904761904761905  Val Acc:  86.71875  High:  0.9184232  Low:  0.11775788\n",
      "120  of:  8164  Loss:  0.9801459312438965  F1 Val:  0.10526315789473685  Val Acc:  73.4375  High:  0.88776994  Low:  0.047974076\n",
      "121  of:  8164  Loss:  0.776258647441864  F1 Val:  0.32000000000000006  Val Acc:  73.4375  High:  0.928439  Low:  0.1270753\n",
      "122  of:  8164  Loss:  0.5845776796340942  F1 Val:  0.07575757575757576  Val Acc:  4.6875  High:  0.9182922  Low:  0.3435301\n",
      "123  of:  8164  Loss:  0.5708136558532715  F1 Val:  0.14492753623188406  Val Acc:  7.8125  High:  0.93043876  Low:  0.48721597\n",
      "124  of:  8164  Loss:  0.7264657020568848  F1 Val:  0.1037037037037037  Val Acc:  5.46875  High:  0.9411398  Low:  0.5670337\n",
      "125  of:  8164  Loss:  0.8930782079696655  F1 Val:  0.1037037037037037  Val Acc:  5.46875  High:  0.9223987  Low:  0.5976275\n",
      "126  of:  8164  Loss:  0.639390766620636  F1 Val:  0.08955223880597014  Val Acc:  4.6875  High:  0.92472726  Low:  0.56786525\n",
      "127  of:  8164  Loss:  0.6991050243377686  F1 Val:  0.0  Val Acc:  66.40625  High:  0.86103064  Low:  0.5212854\n",
      "128  of:  8164  Loss:  0.7213699221611023  F1 Val:  0.26666666666666666  Val Acc:  91.40625  High:  0.8741538  Low:  0.43676218\n",
      "129  of:  8164  Loss:  0.703777015209198  F1 Val:  0.13333333333333333  Val Acc:  89.84375  High:  0.70904624  Low:  0.33670124\n",
      "130  of:  8164  Loss:  0.6406538486480713  F1 Val:  0.2  Val Acc:  93.75  High:  0.6384629  Low:  0.29100963\n",
      "131  of:  8164  Loss:  0.7057265043258667  F1 Val:  0.4  Val Acc:  95.3125  High:  0.66288865  Low:  0.25828445\n",
      "132  of:  8164  Loss:  0.7727576494216919  F1 Val:  0.23529411764705882  Val Acc:  89.84375  High:  0.67324334  Low:  0.24053825\n",
      "133  of:  8164  Loss:  0.6139901876449585  F1 Val:  0.27272727272727276  Val Acc:  87.5  High:  0.66949654  Low:  0.29709932\n",
      "134  of:  8164  Loss:  0.6764146089553833  F1 Val:  0.32432432432432434  Val Acc:  80.46875  High:  0.67915183  Low:  0.34461254\n",
      "135  of:  8164  Loss:  0.6884661316871643  F1 Val:  0.3466666666666666  Val Acc:  61.71875  High:  0.71458805  Low:  0.38339692\n",
      "136  of:  8164  Loss:  0.5953956246376038  F1 Val:  0.0847457627118644  Val Acc:  15.625  High:  0.8279302  Low:  0.43530962\n",
      "137  of:  8164  Loss:  0.6155704259872437  F1 Val:  0.11764705882352941  Val Acc:  6.25  High:  0.88580406  Low:  0.4762384\n",
      "138  of:  8164  Loss:  0.6993827223777771  F1 Val:  0.07518796992481203  Val Acc:  3.90625  High:  0.9199955  Low:  0.50767905\n",
      "139  of:  8164  Loss:  0.8675515055656433  F1 Val:  0.11764705882352941  Val Acc:  6.25  High:  0.93518883  Low:  0.5312586\n",
      "140  of:  8164  Loss:  0.5475691556930542  F1 Val:  0.16176470588235292  Val Acc:  10.9375  High:  0.8951771  Low:  0.51072514\n",
      "141  of:  8164  Loss:  0.7349100112915039  F1 Val:  0.15873015873015872  Val Acc:  58.59375  High:  0.9382265  Low:  0.4981244\n",
      "142  of:  8164  Loss:  0.6055867671966553  F1 Val:  0.3137254901960784  Val Acc:  72.65625  High:  0.8821745  Low:  0.45985466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143  of:  8164  Loss:  0.6446836590766907  F1 Val:  0.19999999999999998  Val Acc:  81.25  High:  0.90144145  Low:  0.4087614\n",
      "144  of:  8164  Loss:  0.6590395569801331  F1 Val:  0.2857142857142857  Val Acc:  80.46875  High:  0.87392  Low:  0.38520205\n",
      "145  of:  8164  Loss:  0.5721904635429382  F1 Val:  0.20512820512820512  Val Acc:  75.78125  High:  0.8822725  Low:  0.37047508\n",
      "146  of:  8164  Loss:  0.6462609767913818  F1 Val:  0.2439024390243902  Val Acc:  75.78125  High:  0.90560246  Low:  0.32304657\n",
      "147  of:  8164  Loss:  0.5571446418762207  F1 Val:  0.4  Val Acc:  88.28125  High:  0.88072217  Low:  0.31092447\n",
      "148  of:  8164  Loss:  0.6356366872787476  F1 Val:  0.0  Val Acc:  92.1875  High:  0.8961178  Low:  0.25517148\n",
      "149  of:  8164  Loss:  0.39121368527412415  F1 Val:  0.23529411764705882  Val Acc:  79.6875  High:  0.8733852  Low:  0.20730962\n",
      "150  of:  8164  Loss:  0.811250627040863  F1 Val:  0.20689655172413793  Val Acc:  82.03125  High:  0.9032805  Low:  0.15626048\n",
      "151  of:  8164  Loss:  0.6248198747634888  F1 Val:  0.21621621621621623  Val Acc:  77.34375  High:  0.8219819  Low:  0.15929005\n",
      "152  of:  8164  Loss:  0.68733149766922  F1 Val:  0.125  Val Acc:  67.1875  High:  0.9113211  Low:  0.19809017\n",
      "153  of:  8164  Loss:  0.5727307796478271  F1 Val:  0.13793103448275862  Val Acc:  80.46875  High:  0.9784495  Low:  0.24989639\n",
      "154  of:  8164  Loss:  0.5891345143318176  F1 Val:  0.15384615384615383  Val Acc:  74.21875  High:  0.98301876  Low:  0.30278388\n",
      "155  of:  8164  Loss:  0.5429138541221619  F1 Val:  0.2  Val Acc:  87.5  High:  0.9396269  Low:  0.27489194\n",
      "156  of:  8164  Loss:  0.6009080410003662  F1 Val:  0.4444444444444444  Val Acc:  92.1875  High:  0.8522769  Low:  0.24736981\n",
      "157  of:  8164  Loss:  0.5417116284370422  F1 Val:  0.5161290322580645  Val Acc:  88.28125  High:  0.8150912  Low:  0.24483262\n",
      "158  of:  8164  Loss:  0.5491266250610352  F1 Val:  0.19999999999999998  Val Acc:  43.75  High:  0.9587638  Low:  0.28797007\n",
      "159  of:  8164  Loss:  0.5028732419013977  F1 Val:  0.14678899082568808  Val Acc:  27.34375  High:  0.88494784  Low:  0.20312226\n",
      "160  of:  8164  Loss:  0.6304124593734741  F1 Val:  0.23999999999999996  Val Acc:  85.15625  High:  0.9950706  Low:  0.36587092\n",
      "161  of:  8164  Loss:  0.45594120025634766  F1 Val:  0.2222222222222222  Val Acc:  94.53125  High:  0.9884006  Low:  0.18227905\n",
      "162  of:  8164  Loss:  0.8070355653762817  F1 Val:  0.37037037037037035  Val Acc:  86.71875  High:  0.7255384  Low:  0.110742815\n",
      "163  of:  8164  Loss:  0.6228382587432861  F1 Val:  0.21212121212121215  Val Acc:  59.375  High:  0.9778781  Low:  0.1504725\n",
      "164  of:  8164  Loss:  0.7810878753662109  F1 Val:  0.23333333333333334  Val Acc:  64.0625  High:  0.9999279  Low:  0.21655482\n",
      "165  of:  8164  Loss:  0.6005522608757019  F1 Val:  0.2909090909090909  Val Acc:  69.53125  High:  0.9997391  Low:  0.25994432\n",
      "166  of:  8164  Loss:  0.5822831392288208  F1 Val:  0.10526315789473684  Val Acc:  86.71875  High:  0.99682486  Low:  0.25452214\n",
      "167  of:  8164  Loss:  0.380670964717865  F1 Val:  0.25  Val Acc:  95.3125  High:  0.9056735  Low:  0.23254457\n",
      "168  of:  8164  Loss:  0.6566160917282104  F1 Val:  0.1818181818181818  Val Acc:  92.96875  High:  0.45363376  Low:  0.15920106\n",
      "169  of:  8164  Loss:  0.8421110510826111  F1 Val:  0.14285714285714288  Val Acc:  90.625  High:  0.5238008  Low:  0.14049718\n",
      "170  of:  8164  Loss:  0.7861262559890747  F1 Val:  0.5454545454545455  Val Acc:  92.1875  High:  0.6437063  Low:  0.2185503\n",
      "171  of:  8164  Loss:  0.5938985347747803  F1 Val:  0.13043478260869565  Val Acc:  37.5  High:  0.7249373  Low:  0.34695125\n",
      "172  of:  8164  Loss:  0.6512933969497681  F1 Val:  0.07518796992481203  Val Acc:  3.90625  High:  0.7954821  Low:  0.45386916\n",
      "173  of:  8164  Loss:  0.6318060159683228  F1 Val:  0.04580152671755725  Val Acc:  2.34375  High:  0.72155  Low:  0.5311337\n",
      "174  of:  8164  Loss:  0.6945021748542786  F1 Val:  0.07518796992481203  Val Acc:  3.90625  High:  0.77153814  Low:  0.57664955\n",
      "175  of:  8164  Loss:  0.6385159492492676  F1 Val:  0.19718309859154928  Val Acc:  10.9375  High:  0.75129163  Low:  0.59880376\n",
      "176  of:  8164  Loss:  0.6700974106788635  F1 Val:  0.11764705882352941  Val Acc:  6.25  High:  0.79789585  Low:  0.6106465\n",
      "177  of:  8164  Loss:  0.6803064346313477  F1 Val:  0.15827338129496402  Val Acc:  8.59375  High:  0.8150878  Low:  0.6153171\n",
      "178  of:  8164  Loss:  0.6903024911880493  F1 Val:  0.03076923076923077  Val Acc:  1.5625  High:  0.7824994  Low:  0.6103756\n",
      "179  of:  8164  Loss:  0.6945024728775024  F1 Val:  0.1037037037037037  Val Acc:  5.46875  High:  0.8320122  Low:  0.59918994\n",
      "180  of:  8164  Loss:  0.6321876645088196  F1 Val:  0.2222222222222222  Val Acc:  12.5  High:  0.8530006  Low:  0.5823544\n",
      "181  of:  8164  Loss:  0.6957711577415466  F1 Val:  0.13138686131386862  Val Acc:  7.03125  High:  0.8294482  Low:  0.5632712\n",
      "182  of:  8164  Loss:  0.6866946220397949  F1 Val:  0.07575757575757576  Val Acc:  4.6875  High:  0.85662717  Low:  0.53386605\n",
      "183  of:  8164  Loss:  0.6590703725814819  F1 Val:  0.13793103448275862  Val Acc:  60.9375  High:  0.83236444  Low:  0.49928215\n",
      "184  of:  8164  Loss:  0.5921885371208191  F1 Val:  0.13043478260869565  Val Acc:  68.75  High:  0.8099604  Low:  0.45939612\n",
      "185  of:  8164  Loss:  0.6399848461151123  F1 Val:  0.2978723404255319  Val Acc:  74.21875  High:  0.83591527  Low:  0.42495534\n",
      "186  of:  8164  Loss:  0.6916596293449402  F1 Val:  0.2285714285714286  Val Acc:  78.90625  High:  0.8646633  Low:  0.38799277\n",
      "187  of:  8164  Loss:  0.6844038963317871  F1 Val:  0.14814814814814817  Val Acc:  82.03125  High:  0.8499194  Low:  0.363483\n",
      "188  of:  8164  Loss:  0.7309903502464294  F1 Val:  0.10526315789473685  Val Acc:  86.71875  High:  0.82475793  Low:  0.34377834\n",
      "189  of:  8164  Loss:  0.7545068264007568  F1 Val:  0.1739130434782609  Val Acc:  70.3125  High:  0.93195003  Low:  0.3327002\n",
      "190  of:  8164  Loss:  0.7816543579101562  F1 Val:  0.4  Val Acc:  85.9375  High:  0.9058124  Low:  0.3283315\n",
      "191  of:  8164  Loss:  0.5838911533355713  F1 Val:  0.3255813953488372  Val Acc:  77.34375  High:  0.90011084  Low:  0.3372961\n",
      "192  of:  8164  Loss:  0.5498036742210388  F1 Val:  0.20689655172413793  Val Acc:  82.03125  High:  0.94652843  Low:  0.3375626\n",
      "193  of:  8164  Loss:  0.7084559202194214  F1 Val:  0.411764705882353  Val Acc:  84.375  High:  0.9413054  Low:  0.33490363\n",
      "194  of:  8164  Loss:  0.5372586250305176  F1 Val:  0.16129032258064516  Val Acc:  59.375  High:  0.9240118  Low:  0.3370331\n",
      "195  of:  8164  Loss:  0.5940161347389221  F1 Val:  0.2  Val Acc:  81.25  High:  0.95029503  Low:  0.33935937\n",
      "196  of:  8164  Loss:  0.5263481736183167  F1 Val:  0.12307692307692307  Val Acc:  55.46875  High:  0.9336555  Low:  0.33931187\n",
      "197  of:  8164  Loss:  0.5270629525184631  F1 Val:  0.16949152542372883  Val Acc:  61.71875  High:  0.95749307  Low:  0.3358243\n",
      "198  of:  8164  Loss:  0.7274793386459351  F1 Val:  0.1627906976744186  Val Acc:  43.75  High:  0.9914057  Low:  0.33010915\n",
      "199  of:  8164  Loss:  0.6681262850761414  F1 Val:  0.22950819672131148  Val Acc:  63.28125  High:  0.9825821  Low:  0.33850616\n",
      "200  of:  8164  Loss:  0.6377192139625549  F1 Val:  0.07272727272727272  Val Acc:  60.15625  High:  0.95195657  Low:  0.33148116\n",
      "201  of:  8164  Loss:  0.7742735743522644  F1 Val:  0.2380952380952381  Val Acc:  75.0  High:  0.96501744  Low:  0.3272985\n",
      "202  of:  8164  Loss:  0.6710750460624695  F1 Val:  0.23529411764705882  Val Acc:  89.84375  High:  0.8677751  Low:  0.3050319\n",
      "203  of:  8164  Loss:  0.6186707615852356  F1 Val:  0.0  Val Acc:  92.96875  High:  0.5567541  Low:  0.28391838\n",
      "204  of:  8164  Loss:  0.7014216184616089  F1 Val:  0.4  Val Acc:  95.3125  High:  0.5441503  Low:  0.27984172\n",
      "205  of:  8164  Loss:  0.6596377491950989  F1 Val:  0.3  Val Acc:  89.0625  High:  0.5310464  Low:  0.27594873\n",
      "206  of:  8164  Loss:  0.648693859577179  F1 Val:  0.3846153846153846  Val Acc:  87.5  High:  0.55082357  Low:  0.28124875\n",
      "207  of:  8164  Loss:  0.5977538824081421  F1 Val:  0.33333333333333337  Val Acc:  78.125  High:  0.6856579  Low:  0.284233\n",
      "208  of:  8164  Loss:  0.5163193941116333  F1 Val:  0.1951219512195122  Val Acc:  74.21875  High:  0.8222479  Low:  0.303031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209  of:  8164  Loss:  0.5141036510467529  F1 Val:  0.12195121951219512  Val Acc:  43.75  High:  0.9201083  Low:  0.3019124\n",
      "210  of:  8164  Loss:  0.748857319355011  F1 Val:  0.15094339622641506  Val Acc:  29.6875  High:  0.97137165  Low:  0.3276429\n",
      "211  of:  8164  Loss:  0.5755094289779663  F1 Val:  0.1927710843373494  Val Acc:  47.65625  High:  0.9903189  Low:  0.36142164\n",
      "212  of:  8164  Loss:  0.6267097592353821  F1 Val:  0.17857142857142855  Val Acc:  64.0625  High:  0.9969146  Low:  0.36667845\n",
      "213  of:  8164  Loss:  0.5292477011680603  F1 Val:  0.19999999999999998  Val Acc:  87.5  High:  0.98953444  Low:  0.30357\n",
      "214  of:  8164  Loss:  0.40363937616348267  F1 Val:  0.4444444444444444  Val Acc:  92.1875  High:  0.97621274  Low:  0.24288161\n",
      "215  of:  8164  Loss:  0.8017392158508301  F1 Val:  0.3333333333333333  Val Acc:  87.5  High:  0.95479095  Low:  0.18934529\n",
      "216  of:  8164  Loss:  0.57988440990448  F1 Val:  0.2  Val Acc:  93.75  High:  0.8271355  Low:  0.183704\n",
      "217  of:  8164  Loss:  0.5801326632499695  F1 Val:  0.4  Val Acc:  92.96875  High:  0.91901034  Low:  0.20267206\n",
      "218  of:  8164  Loss:  0.7527199983596802  F1 Val:  0.5714285714285715  Val Acc:  97.65625  High:  0.77156  Low:  0.22001664\n",
      "219  of:  8164  Loss:  0.5874357223510742  F1 Val:  0.5  Val Acc:  96.875  High:  0.9739256  Low:  0.27704123\n",
      "220  of:  8164  Loss:  0.5675510168075562  F1 Val:  0.4210526315789474  Val Acc:  91.40625  High:  0.9875044  Low:  0.34103116\n",
      "221  of:  8164  Loss:  0.6009063124656677  F1 Val:  0.3333333333333333  Val Acc:  78.125  High:  0.95270884  Low:  0.39180428\n",
      "222  of:  8164  Loss:  0.7654697895050049  F1 Val:  0.13636363636363635  Val Acc:  70.3125  High:  0.9869774  Low:  0.4410982\n",
      "223  of:  8164  Loss:  0.6757289171218872  F1 Val:  0.15827338129496402  Val Acc:  8.59375  High:  0.99346536  Low:  0.47950542\n",
      "224  of:  8164  Loss:  0.5749351382255554  F1 Val:  0.13138686131386862  Val Acc:  7.03125  High:  0.99285805  Low:  0.50929964\n",
      "225  of:  8164  Loss:  0.6040620803833008  F1 Val:  0.08955223880597014  Val Acc:  4.6875  High:  0.9950033  Low:  0.52000266\n",
      "226  of:  8164  Loss:  0.47378644347190857  F1 Val:  0.046153846153846156  Val Acc:  3.125  High:  0.97195846  Low:  0.5088001\n",
      "227  of:  8164  Loss:  0.6062366962432861  F1 Val:  0.176  Val Acc:  19.53125  High:  0.97701126  Low:  0.50134397\n",
      "228  of:  8164  Loss:  0.5502393245697021  F1 Val:  0.13636363636363635  Val Acc:  40.625  High:  0.96358204  Low:  0.49267298\n",
      "229  of:  8164  Loss:  0.5041952729225159  F1 Val:  0.17391304347826084  Val Acc:  55.46875  High:  0.99406165  Low:  0.47747818\n",
      "230  of:  8164  Loss:  0.584206223487854  F1 Val:  0.4255319148936171  Val Acc:  78.90625  High:  0.97495586  Low:  0.43862507\n",
      "231  of:  8164  Loss:  0.5804359316825867  F1 Val:  0.5  Val Acc:  81.25  High:  0.88723123  Low:  0.38511246\n",
      "232  of:  8164  Loss:  0.3952313959598541  F1 Val:  0.33333333333333337  Val Acc:  84.375  High:  0.98070073  Low:  0.3318042\n",
      "233  of:  8164  Loss:  0.4106810688972473  F1 Val:  0.33333333333333337  Val Acc:  78.125  High:  0.97853094  Low:  0.26273343\n",
      "234  of:  8164  Loss:  0.3881132900714874  F1 Val:  0.358974358974359  Val Acc:  80.46875  High:  0.99009717  Low:  0.20356426\n",
      "235  of:  8164  Loss:  0.5082817673683167  F1 Val:  0.2033898305084746  Val Acc:  63.28125  High:  0.99795735  Low:  0.15792407\n",
      "236  of:  8164  Loss:  0.4001513123512268  F1 Val:  0.35714285714285715  Val Acc:  85.9375  High:  0.9924475  Low:  0.10351599\n",
      "237  of:  8164  Loss:  0.5365009903907776  F1 Val:  0.42857142857142855  Val Acc:  87.5  High:  0.99148786  Low:  0.048328217\n",
      "238  of:  8164  Loss:  0.8376070261001587  F1 Val:  0.11851851851851852  Val Acc:  7.03125  High:  0.99609643  Low:  0.05393783\n",
      "239  of:  8164  Loss:  0.9204778671264648  F1 Val:  0.13138686131386862  Val Acc:  7.03125  High:  0.9991611  Low:  0.8356401\n",
      "240  of:  8164  Loss:  1.2709990739822388  F1 Val:  0.07518796992481203  Val Acc:  3.90625  High:  0.99841404  Low:  0.78098536\n",
      "241  of:  8164  Loss:  0.6133343577384949  F1 Val:  0.0  Val Acc:  92.96875  High:  0.750858  Low:  0.40595976\n",
      "242  of:  8164  Loss:  0.7352827191352844  F1 Val:  0.0  Val Acc:  91.40625  High:  0.4719648  Low:  0.37354016\n",
      "243  of:  8164  Loss:  0.7711820006370544  F1 Val:  0.0  Val Acc:  93.75  High:  0.35109967  Low:  0.29801735\n",
      "244  of:  8164  Loss:  0.8722933530807495  F1 Val:  0.0  Val Acc:  91.40625  High:  0.30338398  Low:  0.26060066\n",
      "245  of:  8164  Loss:  0.8121256232261658  F1 Val:  0.0  Val Acc:  92.1875  High:  0.3251644  Low:  0.26106733\n",
      "246  of:  8164  Loss:  0.8225827813148499  F1 Val:  0.0  Val Acc:  92.1875  High:  0.35110265  Low:  0.28145647\n",
      "247  of:  8164  Loss:  0.7812877297401428  F1 Val:  0.0  Val Acc:  95.3125  High:  0.40374273  Low:  0.318464\n",
      "248  of:  8164  Loss:  0.7602465748786926  F1 Val:  0.2  Val Acc:  93.75  High:  0.49313784  Low:  0.3694422\n",
      "249  of:  8164  Loss:  0.6705771088600159  F1 Val:  0.15827338129496402  Val Acc:  8.59375  High:  0.55445457  Low:  0.43727234\n",
      "250  of:  8164  Loss:  0.6893242597579956  F1 Val:  0.08955223880597014  Val Acc:  4.6875  High:  0.6860386  Low:  0.50173265\n",
      "251  of:  8164  Loss:  0.6670119762420654  F1 Val:  0.15827338129496402  Val Acc:  8.59375  High:  0.8099597  Low:  0.56286997\n",
      "252  of:  8164  Loss:  0.6065316796302795  F1 Val:  0.14492753623188406  Val Acc:  7.8125  High:  0.9142908  Low:  0.5997961\n",
      "253  of:  8164  Loss:  0.927937388420105  F1 Val:  0.19718309859154928  Val Acc:  10.9375  High:  0.9336688  Low:  0.62619823\n",
      "254  of:  8164  Loss:  0.752042829990387  F1 Val:  0.17142857142857143  Val Acc:  9.375  High:  0.857804  Low:  0.60351753\n",
      "255  of:  8164  Loss:  0.6016154289245605  F1 Val:  0.07518796992481203  Val Acc:  3.90625  High:  0.9370874  Low:  0.5695203\n",
      "256  of:  8164  Loss:  0.6212539076805115  F1 Val:  0.11764705882352941  Val Acc:  6.25  High:  0.9097086  Low:  0.5419652\n",
      "257  of:  8164  Loss:  0.6385365128517151  F1 Val:  0.18604651162790697  Val Acc:  72.65625  High:  0.9352565  Low:  0.51342946\n",
      "258  of:  8164  Loss:  0.5737334489822388  F1 Val:  0.16666666666666666  Val Acc:  84.375  High:  0.9121391  Low:  0.47111088\n",
      "259  of:  8164  Loss:  0.5827611088752747  F1 Val:  0.36363636363636365  Val Acc:  83.59375  High:  0.9381973  Low:  0.42027697\n",
      "260  of:  8164  Loss:  0.5405294895172119  F1 Val:  0.22222222222222218  Val Acc:  83.59375  High:  0.94445235  Low:  0.3621314\n",
      "261  of:  8164  Loss:  0.5623199939727783  F1 Val:  0.33333333333333337  Val Acc:  87.5  High:  0.9261775  Low:  0.29136553\n",
      "262  of:  8164  Loss:  0.4405564069747925  F1 Val:  0.16666666666666669  Val Acc:  84.375  High:  0.99060285  Low:  0.23876709\n",
      "263  of:  8164  Loss:  0.6597627401351929  F1 Val:  0.3157894736842105  Val Acc:  79.6875  High:  0.97203034  Low:  0.19001459\n",
      "264  of:  8164  Loss:  0.5895103216171265  F1 Val:  0.0909090909090909  Val Acc:  84.375  High:  0.99680835  Low:  0.16670036\n",
      "265  of:  8164  Loss:  0.580933690071106  F1 Val:  0.18518518518518517  Val Acc:  65.625  High:  0.999361  Low:  0.16658214\n",
      "266  of:  8164  Loss:  0.5552524924278259  F1 Val:  0.10810810810810813  Val Acc:  74.21875  High:  0.9996661  Low:  0.1641209\n",
      "267  of:  8164  Loss:  0.5289356112480164  F1 Val:  0.3225806451612903  Val Acc:  83.59375  High:  0.99963415  Low:  0.1632959\n",
      "268  of:  8164  Loss:  0.6247421503067017  F1 Val:  0.47619047619047616  Val Acc:  91.40625  High:  0.985218  Low:  0.12876163\n",
      "269  of:  8164  Loss:  0.7841483354568481  F1 Val:  0.3333333333333333  Val Acc:  93.75  High:  0.84576035  Low:  0.10032941\n",
      "270  of:  8164  Loss:  0.5802327990531921  F1 Val:  0.18181818181818182  Val Acc:  85.9375  High:  0.7116171  Low:  0.14892131\n",
      "271  of:  8164  Loss:  0.6580524444580078  F1 Val:  0.1846153846153846  Val Acc:  58.59375  High:  0.8124603  Low:  0.23380469\n",
      "272  of:  8164  Loss:  0.5917859077453613  F1 Val:  0.08955223880597014  Val Acc:  4.6875  High:  0.72036934  Low:  0.40080625\n",
      "273  of:  8164  Loss:  0.5642205476760864  F1 Val:  0.08955223880597014  Val Acc:  4.6875  High:  0.7638679  Low:  0.5426605\n",
      "274  of:  8164  Loss:  0.6522877812385559  F1 Val:  0.14492753623188406  Val Acc:  7.8125  High:  0.900274  Low:  0.61630005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275  of:  8164  Loss:  0.5948448181152344  F1 Val:  0.1037037037037037  Val Acc:  5.46875  High:  0.8877891  Low:  0.64529055\n",
      "276  of:  8164  Loss:  0.5951367616653442  F1 Val:  0.04580152671755725  Val Acc:  2.34375  High:  0.8603067  Low:  0.62733895\n",
      "277  of:  8164  Loss:  0.471521258354187  F1 Val:  0.08955223880597014  Val Acc:  4.6875  High:  0.98291564  Low:  0.5945692\n",
      "278  of:  8164  Loss:  0.6254687905311584  F1 Val:  0.06060606060606061  Val Acc:  3.125  High:  0.9196844  Low:  0.5589946\n",
      "279  of:  8164  Loss:  0.7441806197166443  F1 Val:  0.24  Val Acc:  70.3125  High:  0.9774305  Low:  0.5115786\n",
      "280  of:  8164  Loss:  0.5697298049926758  F1 Val:  0.4444444444444444  Val Acc:  84.375  High:  0.93382317  Low:  0.4379214\n",
      "281  of:  8164  Loss:  0.5515004992485046  F1 Val:  0.4166666666666667  Val Acc:  89.0625  High:  0.7707906  Low:  0.37835765\n",
      "282  of:  8164  Loss:  0.5563417673110962  F1 Val:  0.41379310344827586  Val Acc:  86.71875  High:  0.84494525  Low:  0.2922447\n",
      "283  of:  8164  Loss:  0.4277459383010864  F1 Val:  0.39999999999999997  Val Acc:  85.9375  High:  0.9792037  Low:  0.2449204\n",
      "284  of:  8164  Loss:  0.6714171171188354  F1 Val:  0.19672131147540986  Val Acc:  61.71875  High:  0.98747605  Low:  0.20178668\n",
      "285  of:  8164  Loss:  0.4811195433139801  F1 Val:  0.4262295081967213  Val Acc:  72.65625  High:  0.998678  Low:  0.18160583\n",
      "286  of:  8164  Loss:  0.23319381475448608  F1 Val:  0.21621621621621623  Val Acc:  77.34375  High:  0.9999645  Low:  0.15850693\n",
      "287  of:  8164  Loss:  0.8950313329696655  F1 Val:  0.3846153846153846  Val Acc:  87.5  High:  0.9999999  Low:  0.1352844\n",
      "288  of:  8164  Loss:  0.3778613209724426  F1 Val:  0.631578947368421  Val Acc:  94.53125  High:  0.99478614  Low:  0.10343852\n",
      "289  of:  8164  Loss:  1.004250407218933  F1 Val:  0.5714285714285715  Val Acc:  95.3125  High:  0.69549406  Low:  0.08343063\n",
      "290  of:  8164  Loss:  0.5770571231842041  F1 Val:  0.4  Val Acc:  90.625  High:  0.618915  Low:  0.13607281\n",
      "291  of:  8164  Loss:  0.5741252303123474  F1 Val:  0.5714285714285715  Val Acc:  92.96875  High:  0.66641253  Low:  0.20813815\n",
      "292  of:  8164  Loss:  0.517410159111023  F1 Val:  0.36000000000000004  Val Acc:  75.0  High:  0.9261997  Low:  0.28826845\n",
      "293  of:  8164  Loss:  0.3926248550415039  F1 Val:  0.15730337078651685  Val Acc:  41.40625  High:  0.99701935  Low:  0.36211473\n",
      "294  of:  8164  Loss:  0.5817325711250305  F1 Val:  0.08080808080808081  Val Acc:  28.90625  High:  0.99431545  Low:  0.4234382\n",
      "295  of:  8164  Loss:  0.6102415919303894  F1 Val:  0.16666666666666669  Val Acc:  37.5  High:  0.9849571  Low:  0.43957406\n",
      "296  of:  8164  Loss:  0.5147719383239746  F1 Val:  0.1234567901234568  Val Acc:  44.53125  High:  0.9586499  Low:  0.4444973\n",
      "297  of:  8164  Loss:  0.43819916248321533  F1 Val:  0.2711864406779661  Val Acc:  66.40625  High:  0.96085644  Low:  0.42763558\n",
      "298  of:  8164  Loss:  0.5248087048530579  F1 Val:  0.2068965517241379  Val Acc:  64.0625  High:  0.9641288  Low:  0.40380484\n",
      "299  of:  8164  Loss:  0.5102952122688293  F1 Val:  0.3720930232558139  Val Acc:  78.90625  High:  0.90872574  Low:  0.38438392\n",
      "300  of:  8164  Loss:  0.6022951602935791  F1 Val:  0.27586206896551724  Val Acc:  67.1875  High:  0.8849291  Low:  0.36978498\n",
      "301  of:  8164  Loss:  0.4696124792098999  F1 Val:  0.3571428571428571  Val Acc:  85.9375  High:  0.96651834  Low:  0.35719764\n",
      "302  of:  8164  Loss:  0.4956086575984955  F1 Val:  0.3125  Val Acc:  82.8125  High:  0.99583423  Low:  0.34136054\n",
      "303  of:  8164  Loss:  0.4057031273841858  F1 Val:  0.3157894736842105  Val Acc:  89.84375  High:  0.9973037  Low:  0.32662812\n",
      "304  of:  8164  Loss:  0.5892177820205688  F1 Val:  0.29411764705882354  Val Acc:  81.25  High:  0.92197055  Low:  0.30831388\n",
      "305  of:  8164  Loss:  0.44834455847740173  F1 Val:  0.45000000000000007  Val Acc:  82.8125  High:  0.9984724  Low:  0.30526987\n",
      "306  of:  8164  Loss:  0.5967592000961304  F1 Val:  0.4  Val Acc:  74.21875  High:  0.9999944  Low:  0.29368234\n",
      "307  of:  8164  Loss:  0.4377598762512207  F1 Val:  0.35714285714285715  Val Acc:  85.9375  High:  0.999998  Low:  0.27546814\n",
      "308  of:  8164  Loss:  0.5223095417022705  F1 Val:  0.30769230769230765  Val Acc:  92.96875  High:  0.99768794  Low:  0.25105906\n",
      "309  of:  8164  Loss:  0.49091020226478577  F1 Val:  0.6666666666666667  Val Acc:  96.09375  High:  0.9949332  Low:  0.22564502\n",
      "310  of:  8164  Loss:  0.5949780941009521  F1 Val:  0.5555555555555556  Val Acc:  93.75  High:  0.93508726  Low:  0.20688562\n",
      "311  of:  8164  Loss:  0.5329678058624268  F1 Val:  0.3478260869565218  Val Acc:  88.28125  High:  0.965938  Low:  0.19722061\n",
      "312  of:  8164  Loss:  0.5113961100578308  F1 Val:  0.14814814814814817  Val Acc:  82.03125  High:  0.8979834  Low:  0.19858967\n",
      "313  of:  8164  Loss:  0.33640480041503906  F1 Val:  0.1568627450980392  Val Acc:  66.40625  High:  0.9982015  Low:  0.19644359\n",
      "314  of:  8164  Loss:  0.6504930257797241  F1 Val:  0.3333333333333333  Val Acc:  75.0  High:  0.9983895  Low:  0.1946564\n",
      "315  of:  8164  Loss:  0.40857014060020447  F1 Val:  0.21428571428571427  Val Acc:  82.8125  High:  0.9954112  Low:  0.18348527\n",
      "316  of:  8164  Loss:  0.28278324007987976  F1 Val:  0.3157894736842105  Val Acc:  89.84375  High:  0.96616656  Low:  0.1513659\n",
      "317  of:  8164  Loss:  0.4888302981853485  F1 Val:  0.7826086956521738  Val Acc:  96.09375  High:  0.98984045  Low:  0.11347036\n",
      "318  of:  8164  Loss:  0.6269780993461609  F1 Val:  0.631578947368421  Val Acc:  94.53125  High:  0.83022875  Low:  0.0914824\n",
      "319  of:  8164  Loss:  0.7467919588088989  F1 Val:  0.48484848484848486  Val Acc:  86.71875  High:  0.9478755  Low:  0.089357235\n",
      "320  of:  8164  Loss:  0.5126848816871643  F1 Val:  0.23076923076923075  Val Acc:  53.125  High:  0.9264422  Low:  0.123860605\n",
      "321  of:  8164  Loss:  0.43133509159088135  F1 Val:  0.16071428571428573  Val Acc:  26.5625  High:  0.99993956  Low:  0.21307068\n",
      "322  of:  8164  Loss:  0.6241931319236755  F1 Val:  0.16161616161616163  Val Acc:  35.15625  High:  0.9999684  Low:  0.34670424\n",
      "323  of:  8164  Loss:  0.5307995080947876  F1 Val:  0.34285714285714286  Val Acc:  64.0625  High:  0.99966335  Low:  0.3258379\n",
      "324  of:  8164  Loss:  0.6020399332046509  F1 Val:  0.5  Val Acc:  92.1875  High:  0.976694  Low:  0.26677337\n",
      "325  of:  8164  Loss:  0.5670434832572937  F1 Val:  0.0  Val Acc:  92.1875  High:  0.525949  Low:  0.21115464\n",
      "326  of:  8164  Loss:  0.782890260219574  F1 Val:  0.0  Val Acc:  92.96875  High:  0.41581205  Low:  0.17104322\n",
      "327  of:  8164  Loss:  0.9006089568138123  F1 Val:  0.0  Val Acc:  94.53125  High:  0.3780635  Low:  0.14785568\n",
      "328  of:  8164  Loss:  0.8119856715202332  F1 Val:  0.0  Val Acc:  94.53125  High:  0.47046155  Low:  0.14748599\n",
      "329  of:  8164  Loss:  0.6283270716667175  F1 Val:  0.6363636363636364  Val Acc:  93.75  High:  0.6679256  Low:  0.1762263\n",
      "330  of:  8164  Loss:  0.6050155758857727  F1 Val:  0.5  Val Acc:  89.0625  High:  0.9205554  Low:  0.1871493\n",
      "331  of:  8164  Loss:  0.3603307902812958  F1 Val:  0.3888888888888889  Val Acc:  82.8125  High:  0.9420113  Low:  0.23071939\n",
      "332  of:  8164  Loss:  0.4282877445220947  F1 Val:  0.35555555555555557  Val Acc:  77.34375  High:  0.9887136  Low:  0.24698947\n",
      "333  of:  8164  Loss:  0.46163055300712585  F1 Val:  0.45454545454545453  Val Acc:  81.25  High:  0.9992213  Low:  0.24953334\n",
      "334  of:  8164  Loss:  0.5442296266555786  F1 Val:  0.36000000000000004  Val Acc:  75.0  High:  0.9999403  Low:  0.20312658\n",
      "335  of:  8164  Loss:  0.5943755507469177  F1 Val:  0.36363636363636365  Val Acc:  83.59375  High:  0.9999423  Low:  0.15292522\n",
      "336  of:  8164  Loss:  0.49878978729248047  F1 Val:  0.3846153846153846  Val Acc:  87.5  High:  0.9999989  Low:  0.07533868\n",
      "337  of:  8164  Loss:  0.53279709815979  F1 Val:  0.3125  Val Acc:  82.8125  High:  0.9990139  Low:  0.04307069\n",
      "338  of:  8164  Loss:  0.48089683055877686  F1 Val:  0.64  Val Acc:  92.96875  High:  0.99920493  Low:  0.047280755\n",
      "339  of:  8164  Loss:  0.8130252957344055  F1 Val:  0.5416666666666667  Val Acc:  82.8125  High:  0.99047273  Low:  0.052067354\n",
      "340  of:  8164  Loss:  0.4635959565639496  F1 Val:  0.29508196721311475  Val Acc:  66.40625  High:  0.99584395  Low:  0.1509603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341  of:  8164  Loss:  0.3516017496585846  F1 Val:  0.07518796992481203  Val Acc:  3.90625  High:  0.99494004  Low:  0.35475364\n",
      "342  of:  8164  Loss:  0.7224674224853516  F1 Val:  0.15827338129496402  Val Acc:  8.59375  High:  0.988846  Low:  0.5356297\n",
      "343  of:  8164  Loss:  0.59679114818573  F1 Val:  0.13138686131386862  Val Acc:  7.03125  High:  0.9828293  Low:  0.5965235\n",
      "344  of:  8164  Loss:  0.5954466462135315  F1 Val:  0.15827338129496402  Val Acc:  8.59375  High:  0.9699977  Low:  0.61344117\n",
      "345  of:  8164  Loss:  0.7128354907035828  F1 Val:  0.11764705882352941  Val Acc:  6.25  High:  0.9687017  Low:  0.5941382\n",
      "346  of:  8164  Loss:  0.7010738253593445  F1 Val:  0.11764705882352941  Val Acc:  6.25  High:  0.91929686  Low:  0.55475414\n",
      "347  of:  8164  Loss:  0.5991526246070862  F1 Val:  0.25925925925925924  Val Acc:  68.75  High:  0.88675916  Low:  0.50667757\n",
      "348  of:  8164  Loss:  0.5712692141532898  F1 Val:  0.4705882352941177  Val Acc:  85.9375  High:  0.75315624  Low:  0.4252127\n",
      "349  of:  8164  Loss:  0.6284633874893188  F1 Val:  0.1111111111111111  Val Acc:  87.5  High:  0.6858012  Low:  0.3607392\n",
      "350  of:  8164  Loss:  0.5143122673034668  F1 Val:  0.5  Val Acc:  90.625  High:  0.82125443  Low:  0.28248873\n",
      "351  of:  8164  Loss:  0.5491318702697754  F1 Val:  0.3636363636363637  Val Acc:  89.0625  High:  0.6757254  Low:  0.21338733\n",
      "352  of:  8164  Loss:  0.5939699411392212  F1 Val:  0.5  Val Acc:  89.0625  High:  0.6846707  Low:  0.1560659\n",
      "353  of:  8164  Loss:  0.5297356247901917  F1 Val:  0.3404255319148936  Val Acc:  75.78125  High:  0.8293777  Low:  0.12404025\n",
      "354  of:  8164  Loss:  0.4679422974586487  F1 Val:  0.3157894736842105  Val Acc:  69.53125  High:  0.99082035  Low:  0.10496249\n",
      "355  of:  8164  Loss:  0.47146666049957275  F1 Val:  0.3333333333333333  Val Acc:  75.0  High:  0.99470985  Low:  0.086400904\n",
      "356  of:  8164  Loss:  0.23167134821414948  F1 Val:  0.3333333333333333  Val Acc:  75.0  High:  0.97369766  Low:  0.06784683\n",
      "357  of:  8164  Loss:  0.46720245480537415  F1 Val:  0.3125  Val Acc:  82.8125  High:  0.9919356  Low:  0.052801568\n",
      "358  of:  8164  Loss:  0.32603028416633606  F1 Val:  0.25  Val Acc:  90.625  High:  0.998137  Low:  0.0347139\n",
      "359  of:  8164  Loss:  0.14988701045513153  F1 Val:  0.5  Val Acc:  90.625  High:  0.9895107  Low:  0.023439348\n",
      "360  of:  8164  Loss:  0.8462682366371155  F1 Val:  0.1724137931034483  Val Acc:  62.5  High:  0.9993899  Low:  0.015159683\n",
      "361  of:  8164  Loss:  0.3325662612915039  F1 Val:  0.3225806451612903  Val Acc:  67.1875  High:  0.99585956  Low:  0.031120747\n",
      "362  of:  8164  Loss:  0.5256366729736328  F1 Val:  0.22641509433962262  Val Acc:  67.96875  High:  0.9982254  Low:  0.042883687\n",
      "363  of:  8164  Loss:  0.32181522250175476  F1 Val:  0.30769230769230765  Val Acc:  78.90625  High:  0.9947107  Low:  0.03944056\n",
      "364  of:  8164  Loss:  0.38829103112220764  F1 Val:  0.375  Val Acc:  84.375  High:  0.99211204  Low:  0.032418728\n",
      "365  of:  8164  Loss:  0.30520719289779663  F1 Val:  0.4545454545454545  Val Acc:  90.625  High:  0.8158087  Low:  0.022102589\n",
      "366  of:  8164  Loss:  0.5641369819641113  F1 Val:  0.09090909090909091  Val Acc:  84.375  High:  0.9218217  Low:  0.018260391\n",
      "367  of:  8164  Loss:  0.7195579409599304  F1 Val:  0.27118644067796616  Val Acc:  66.40625  High:  0.8968029  Low:  0.02398709\n",
      "368  of:  8164  Loss:  0.37756040692329407  F1 Val:  0.2647058823529412  Val Acc:  60.9375  High:  0.9515173  Low:  0.06699763\n",
      "369  of:  8164  Loss:  0.5138800740242004  F1 Val:  0.09999999999999999  Val Acc:  29.6875  High:  0.9937086  Low:  0.16886164\n",
      "370  of:  8164  Loss:  0.5716622471809387  F1 Val:  0.0980392156862745  Val Acc:  28.125  High:  0.97068125  Low:  0.31868604\n",
      "371  of:  8164  Loss:  0.5293369889259338  F1 Val:  0.12048192771084336  Val Acc:  42.96875  High:  0.9309136  Low:  0.31649417\n",
      "372  of:  8164  Loss:  0.4422341585159302  F1 Val:  0.15  Val Acc:  46.875  High:  0.9681542  Low:  0.26704684\n",
      "373  of:  8164  Loss:  0.45358943939208984  F1 Val:  0.41935483870967744  Val Acc:  71.875  High:  0.90725553  Low:  0.19627438\n",
      "374  of:  8164  Loss:  0.4352213442325592  F1 Val:  0.30000000000000004  Val Acc:  78.125  High:  0.94899684  Low:  0.15420614\n",
      "375  of:  8164  Loss:  0.3569152355194092  F1 Val:  0.30434782608695654  Val Acc:  75.0  High:  0.96728045  Low:  0.11602873\n",
      "376  of:  8164  Loss:  0.3972197473049164  F1 Val:  0.26666666666666666  Val Acc:  82.8125  High:  0.9943884  Low:  0.0806985\n",
      "377  of:  8164  Loss:  0.25608694553375244  F1 Val:  0.42857142857142855  Val Acc:  87.5  High:  0.9963421  Low:  0.063112915\n",
      "378  of:  8164  Loss:  0.3822576403617859  F1 Val:  0.6666666666666667  Val Acc:  89.0625  High:  0.9731559  Low:  0.047460515\n",
      "379  of:  8164  Loss:  0.6297072172164917  F1 Val:  0.25  Val Acc:  76.5625  High:  0.9980531  Low:  0.03800614\n",
      "380  of:  8164  Loss:  0.7396093010902405  F1 Val:  0.18604651162790697  Val Acc:  72.65625  High:  0.9999641  Low:  0.042940058\n",
      "381  of:  8164  Loss:  0.7800726294517517  F1 Val:  0.4444444444444445  Val Acc:  84.375  High:  0.99999595  Low:  0.064752735\n",
      "382  of:  8164  Loss:  0.5188315510749817  F1 Val:  0.5454545454545455  Val Acc:  92.1875  High:  0.9999784  Low:  0.071999304\n",
      "383  of:  8164  Loss:  0.2983092963695526  F1 Val:  0.2727272727272727  Val Acc:  87.5  High:  0.9991391  Low:  0.09097009\n",
      "384  of:  8164  Loss:  0.5297996401786804  F1 Val:  0.43478260869565216  Val Acc:  89.84375  High:  0.84716916  Low:  0.10653727\n",
      "385  of:  8164  Loss:  0.5658056735992432  F1 Val:  0.4545454545454545  Val Acc:  90.625  High:  0.9624919  Low:  0.12512831\n",
      "386  of:  8164  Loss:  0.39179104566574097  F1 Val:  0.6666666666666666  Val Acc:  96.09375  High:  0.7841681  Low:  0.15290545\n",
      "387  of:  8164  Loss:  0.4555073380470276  F1 Val:  0.5555555555555556  Val Acc:  93.75  High:  0.70450264  Low:  0.15857273\n",
      "388  of:  8164  Loss:  0.5038625597953796  F1 Val:  0.3333333333333333  Val Acc:  93.75  High:  0.8649337  Low:  0.15347107\n",
      "389  of:  8164  Loss:  0.4328036308288574  F1 Val:  0.4444444444444444  Val Acc:  84.375  High:  0.92272586  Low:  0.14776227\n",
      "390  of:  8164  Loss:  0.2541751265525818  F1 Val:  0.32  Val Acc:  73.4375  High:  0.9879171  Low:  0.14391369\n",
      "391  of:  8164  Loss:  0.5064228773117065  F1 Val:  0.16666666666666669  Val Acc:  76.5625  High:  0.9998708  Low:  0.12372121\n",
      "392  of:  8164  Loss:  0.21654169261455536  F1 Val:  0.391304347826087  Val Acc:  78.125  High:  0.95568377  Low:  0.09461884\n",
      "393  of:  8164  Loss:  0.2785813808441162  F1 Val:  0.375  Val Acc:  84.375  High:  0.9995431  Low:  0.06227885\n",
      "394  of:  8164  Loss:  0.4320967495441437  F1 Val:  0.22222222222222218  Val Acc:  83.59375  High:  0.9999883  Low:  0.031686414\n",
      "395  of:  8164  Loss:  0.6447471380233765  F1 Val:  0.5  Val Acc:  78.125  High:  0.99496245  Low:  0.018335236\n",
      "396  of:  8164  Loss:  0.5392816066741943  F1 Val:  0.32558139534883723  Val Acc:  77.34375  High:  0.99604356  Low:  0.023373986\n",
      "397  of:  8164  Loss:  0.4504719376564026  F1 Val:  0.45833333333333326  Val Acc:  79.6875  High:  0.99984276  Low:  0.028192768\n",
      "398  of:  8164  Loss:  0.2546005845069885  F1 Val:  0.5  Val Acc:  90.625  High:  0.99782324  Low:  0.029059635\n",
      "399  of:  8164  Loss:  0.2709062993526459  F1 Val:  0.4615384615384615  Val Acc:  89.0625  High:  0.927026  Low:  0.026047247\n",
      "400  of:  8164  Loss:  0.6816055774688721  F1 Val:  0.5  Val Acc:  92.1875  High:  0.935046  Low:  0.01878806\n",
      "401  of:  8164  Loss:  0.6050306558609009  F1 Val:  0.5581395348837209  Val Acc:  85.15625  High:  0.87250066  Low:  0.029614119\n",
      "402  of:  8164  Loss:  0.3704585134983063  F1 Val:  0.14285714285714288  Val Acc:  62.5  High:  0.8775975  Low:  0.06887453\n",
      "403  of:  8164  Loss:  0.41308462619781494  F1 Val:  0.23333333333333334  Val Acc:  64.0625  High:  0.96943337  Low:  0.13036661\n",
      "404  of:  8164  Loss:  0.4486485719680786  F1 Val:  0.15873015873015872  Val Acc:  58.59375  High:  0.9189973  Low:  0.17617893\n",
      "405  of:  8164  Loss:  0.5582860112190247  F1 Val:  0.225  Val Acc:  51.5625  High:  0.95555204  Low:  0.19770978\n",
      "406  of:  8164  Loss:  0.5407307147979736  F1 Val:  0.19047619047619047  Val Acc:  60.15625  High:  0.9760336  Low:  0.1641162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407  of:  8164  Loss:  0.6435242295265198  F1 Val:  0.31746031746031744  Val Acc:  66.40625  High:  0.96899474  Low:  0.12551208\n",
      "408  of:  8164  Loss:  0.3139219880104065  F1 Val:  0.2777777777777778  Val Acc:  79.6875  High:  0.98267674  Low:  0.09389225\n",
      "409  of:  8164  Loss:  0.2989708483219147  F1 Val:  0.2608695652173913  Val Acc:  86.71875  High:  0.93165594  Low:  0.0538035\n",
      "410  of:  8164  Loss:  0.3601706922054291  F1 Val:  0.5454545454545454  Val Acc:  92.1875  High:  0.84131837  Low:  0.027386064\n",
      "411  of:  8164  Loss:  0.158129021525383  F1 Val:  0.36363636363636365  Val Acc:  89.0625  High:  0.9535371  Low:  0.014983722\n",
      "412  of:  8164  Loss:  0.24487482011318207  F1 Val:  0.4166666666666667  Val Acc:  89.0625  High:  0.9194607  Low:  0.0077118785\n",
      "413  of:  8164  Loss:  0.494043231010437  F1 Val:  0.625  Val Acc:  95.3125  High:  0.969646  Low:  0.003804805\n",
      "414  of:  8164  Loss:  0.9405438303947449  F1 Val:  0.45161290322580644  Val Acc:  86.71875  High:  0.994628  Low:  0.003324704\n",
      "415  of:  8164  Loss:  0.879218339920044  F1 Val:  0.39999999999999997  Val Acc:  69.53125  High:  0.9965766  Low:  0.012222229\n",
      "416  of:  8164  Loss:  0.397543728351593  F1 Val:  0.022727272727272724  Val Acc:  32.8125  High:  0.9951971  Low:  0.083662905\n",
      "417  of:  8164  Loss:  0.7982606291770935  F1 Val:  0.14285714285714288  Val Acc:  25.0  High:  0.999223  Low:  0.21435715\n",
      "418  of:  8164  Loss:  0.7514601349830627  F1 Val:  0.15517241379310345  Val Acc:  23.4375  High:  0.9988838  Low:  0.33108792\n",
      "419  of:  8164  Loss:  0.5298274159431458  F1 Val:  0.1142857142857143  Val Acc:  27.34375  High:  0.9989073  Low:  0.40591636\n",
      "420  of:  8164  Loss:  0.6645065546035767  F1 Val:  0.30985915492957744  Val Acc:  61.71875  High:  0.9311894  Low:  0.42761245\n",
      "421  of:  8164  Loss:  0.5446690320968628  F1 Val:  0.4444444444444445  Val Acc:  84.375  High:  0.7918662  Low:  0.42983428\n",
      "422  of:  8164  Loss:  0.6094639897346497  F1 Val:  0.33333333333333337  Val Acc:  93.75  High:  0.6152183  Low:  0.4190793\n",
      "423  of:  8164  Loss:  0.6920945048332214  F1 Val:  0.0  Val Acc:  92.96875  High:  0.51753265  Low:  0.39193538\n",
      "424  of:  8164  Loss:  0.6241129040718079  F1 Val:  0.0  Val Acc:  92.1875  High:  0.45623824  Low:  0.371502\n",
      "425  of:  8164  Loss:  0.6645158529281616  F1 Val:  0.0  Val Acc:  93.75  High:  0.40097067  Low:  0.3511669\n",
      "426  of:  8164  Loss:  0.7905804514884949  F1 Val:  0.0  Val Acc:  92.96875  High:  0.4179261  Low:  0.33620846\n",
      "427  of:  8164  Loss:  0.7597264647483826  F1 Val:  0.33333333333333337  Val Acc:  96.875  High:  0.40577808  Low:  0.33564937\n",
      "428  of:  8164  Loss:  0.691515326499939  F1 Val:  0.5555555555555556  Val Acc:  93.75  High:  0.49301228  Low:  0.34629613\n",
      "429  of:  8164  Loss:  0.6245827674865723  F1 Val:  0.25  Val Acc:  95.3125  High:  0.64770186  Low:  0.36231035\n",
      "430  of:  8164  Loss:  0.6019352674484253  F1 Val:  0.5333333333333333  Val Acc:  89.0625  High:  0.8153439  Low:  0.37823004\n",
      "431  of:  8164  Loss:  0.5271234512329102  F1 Val:  0.35555555555555557  Val Acc:  77.34375  High:  0.9486784  Low:  0.3961833\n",
      "432  of:  8164  Loss:  0.44464433193206787  F1 Val:  0.3076923076923077  Val Acc:  71.875  High:  0.9967644  Low:  0.41101232\n",
      "433  of:  8164  Loss:  0.5797276496887207  F1 Val:  0.3859649122807018  Val Acc:  72.65625  High:  0.9624431  Low:  0.41756546\n",
      "434  of:  8164  Loss:  0.4673244059085846  F1 Val:  0.25806451612903225  Val Acc:  64.0625  High:  0.99298567  Low:  0.4158611\n",
      "435  of:  8164  Loss:  0.53917396068573  F1 Val:  0.3870967741935484  Val Acc:  70.3125  High:  0.99993026  Low:  0.41051692\n",
      "436  of:  8164  Loss:  0.43664729595184326  F1 Val:  0.34285714285714286  Val Acc:  82.03125  High:  0.9999863  Low:  0.39140847\n",
      "437  of:  8164  Loss:  0.5277258157730103  F1 Val:  0.5625  Val Acc:  89.0625  High:  0.99946743  Low:  0.36497623\n",
      "438  of:  8164  Loss:  0.3668676018714905  F1 Val:  0.588235294117647  Val Acc:  94.53125  High:  0.9997298  Low:  0.33568588\n",
      "439  of:  8164  Loss:  0.3411761522293091  F1 Val:  0.6206896551724138  Val Acc:  91.40625  High:  0.9964948  Low:  0.30227455\n",
      "440  of:  8164  Loss:  0.47605788707733154  F1 Val:  0.30769230769230765  Val Acc:  92.96875  High:  0.99944776  Low:  0.27058727\n",
      "441  of:  8164  Loss:  0.6730047464370728  F1 Val:  0.4444444444444445  Val Acc:  92.1875  High:  0.86020434  Low:  0.24487162\n",
      "442  of:  8164  Loss:  0.693924069404602  F1 Val:  0.4615384615384615  Val Acc:  94.53125  High:  0.99900717  Low:  0.23225427\n",
      "443  of:  8164  Loss:  0.3275977373123169  F1 Val:  0.3414634146341463  Val Acc:  78.90625  High:  0.9904045  Low:  0.231818\n",
      "444  of:  8164  Loss:  0.5411501526832581  F1 Val:  0.4166666666666667  Val Acc:  78.125  High:  0.99943  Low:  0.23014246\n",
      "445  of:  8164  Loss:  0.39814722537994385  F1 Val:  0.2857142857142857  Val Acc:  68.75  High:  0.9999994  Low:  0.23311423\n",
      "446  of:  8164  Loss:  0.8443217277526855  F1 Val:  0.4  Val Acc:  88.28125  High:  0.9987464  Low:  0.23036234\n",
      "447  of:  8164  Loss:  0.20630314946174622  F1 Val:  0.42857142857142855  Val Acc:  93.75  High:  0.99823135  Low:  0.19746885\n",
      "448  of:  8164  Loss:  0.44907596707344055  F1 Val:  0.380952380952381  Val Acc:  89.84375  High:  0.97372967  Low:  0.17594802\n",
      "449  of:  8164  Loss:  0.31521710753440857  F1 Val:  0.4545454545454545  Val Acc:  90.625  High:  0.9988971  Low:  0.15631138\n",
      "450  of:  8164  Loss:  0.4686066806316376  F1 Val:  0.4285714285714285  Val Acc:  87.5  High:  0.9437515  Low:  0.13714013\n",
      "451  of:  8164  Loss:  0.6081981062889099  F1 Val:  0.2857142857142857  Val Acc:  84.375  High:  0.89501923  Low:  0.12178931\n",
      "452  of:  8164  Loss:  0.45088234543800354  F1 Val:  0.36666666666666664  Val Acc:  70.3125  High:  0.9864803  Low:  0.11824717\n",
      "453  of:  8164  Loss:  0.5094248056411743  F1 Val:  0.4827586206896553  Val Acc:  76.5625  High:  0.9999654  Low:  0.12048302\n",
      "454  of:  8164  Loss:  0.43794506788253784  F1 Val:  0.16666666666666666  Val Acc:  68.75  High:  0.9999887  Low:  0.119507074\n",
      "455  of:  8164  Loss:  0.49822527170181274  F1 Val:  0.21212121212121213  Val Acc:  59.375  High:  0.9998429  Low:  0.11624988\n",
      "456  of:  8164  Loss:  0.3006943464279175  F1 Val:  0.3333333333333333  Val Acc:  81.25  High:  0.9989342  Low:  0.11547839\n",
      "457  of:  8164  Loss:  0.4104940891265869  F1 Val:  0.4444444444444444  Val Acc:  88.28125  High:  0.9982809  Low:  0.11141424\n",
      "458  of:  8164  Loss:  0.5745366811752319  F1 Val:  0.55  Val Acc:  85.9375  High:  0.67915547  Low:  0.10350564\n",
      "459  of:  8164  Loss:  0.4954735338687897  F1 Val:  0.21276595744680848  Val Acc:  71.09375  High:  0.8758977  Low:  0.104619704\n",
      "460  of:  8164  Loss:  0.3686457872390747  F1 Val:  0.3018867924528302  Val Acc:  71.09375  High:  0.9582173  Low:  0.1155796\n",
      "461  of:  8164  Loss:  0.6530477404594421  F1 Val:  0.30434782608695654  Val Acc:  75.0  High:  0.999302  Low:  0.09824547\n",
      "462  of:  8164  Loss:  0.3923105001449585  F1 Val:  0.3  Val Acc:  78.125  High:  0.9759413  Low:  0.081200466\n",
      "463  of:  8164  Loss:  0.399963915348053  F1 Val:  0.43750000000000006  Val Acc:  85.9375  High:  0.9060933  Low:  0.064491965\n",
      "464  of:  8164  Loss:  0.2682097852230072  F1 Val:  0.34285714285714286  Val Acc:  82.03125  High:  0.998024  Low:  0.054632206\n",
      "465  of:  8164  Loss:  0.28563904762268066  F1 Val:  0.48275862068965514  Val Acc:  88.28125  High:  0.9980813  Low:  0.046968747\n",
      "466  of:  8164  Loss:  0.3847515285015106  F1 Val:  0.3272727272727273  Val Acc:  71.09375  High:  0.9791726  Low:  0.040387515\n",
      "467  of:  8164  Loss:  0.4598254859447479  F1 Val:  0.2222222222222222  Val Acc:  67.1875  High:  0.99364823  Low:  0.03473943\n",
      "468  of:  8164  Loss:  0.6804631352424622  F1 Val:  0.2857142857142857  Val Acc:  68.75  High:  0.9997414  Low:  0.049037226\n",
      "469  of:  8164  Loss:  0.5746753811836243  F1 Val:  0.2222222222222222  Val Acc:  78.125  High:  0.9993222  Low:  0.032334857\n",
      "470  of:  8164  Loss:  0.39775311946868896  F1 Val:  0.42424242424242425  Val Acc:  85.15625  High:  0.9986934  Low:  0.03312846\n",
      "471  of:  8164  Loss:  0.26545393466949463  F1 Val:  0.6046511627906976  Val Acc:  86.71875  High:  0.996216  Low:  0.021010572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472  of:  8164  Loss:  0.7173976302146912  F1 Val:  0.6206896551724138  Val Acc:  91.40625  High:  0.987507  Low:  0.018992584\n",
      "473  of:  8164  Loss:  0.8863150477409363  F1 Val:  0.4444444444444444  Val Acc:  92.1875  High:  0.87474823  Low:  0.022925505\n",
      "474  of:  8164  Loss:  0.3132234215736389  F1 Val:  0.4117647058823529  Val Acc:  84.375  High:  0.79774904  Low:  0.041893817\n",
      "475  of:  8164  Loss:  0.3376782238483429  F1 Val:  0.3333333333333333  Val Acc:  81.25  High:  0.81879634  Low:  0.071904786\n",
      "476  of:  8164  Loss:  0.3007202744483948  F1 Val:  0.29090909090909095  Val Acc:  69.53125  High:  0.99188954  Low:  0.0929435\n",
      "477  of:  8164  Loss:  0.5065605640411377  F1 Val:  0.1142857142857143  Val Acc:  51.5625  High:  0.9968838  Low:  0.1207587\n",
      "478  of:  8164  Loss:  0.5038951635360718  F1 Val:  0.06896551724137931  Val Acc:  57.8125  High:  0.989897  Low:  0.15113592\n",
      "479  of:  8164  Loss:  0.5304414629936218  F1 Val:  0.10526315789473684  Val Acc:  60.15625  High:  0.9987915  Low:  0.16529821\n",
      "480  of:  8164  Loss:  0.4166211783885956  F1 Val:  0.375  Val Acc:  76.5625  High:  0.99528944  Low:  0.17410333\n",
      "481  of:  8164  Loss:  0.4425707459449768  F1 Val:  0.5263157894736842  Val Acc:  85.9375  High:  0.92897797  Low:  0.15918836\n",
      "482  of:  8164  Loss:  0.4525086283683777  F1 Val:  0.6666666666666667  Val Acc:  93.75  High:  0.82337743  Low:  0.16257262\n",
      "483  of:  8164  Loss:  0.40232205390930176  F1 Val:  0.32  Val Acc:  86.71875  High:  0.80173945  Low:  0.15132713\n",
      "484  of:  8164  Loss:  0.3767246603965759  F1 Val:  0.10256410256410257  Val Acc:  72.65625  High:  0.9222922  Low:  0.14607114\n",
      "485  of:  8164  Loss:  0.38090911507606506  F1 Val:  0.3125  Val Acc:  82.8125  High:  0.99933714  Low:  0.14210024\n",
      "486  of:  8164  Loss:  0.3845299184322357  F1 Val:  0.3333333333333333  Val Acc:  81.25  High:  0.99924904  Low:  0.13780962\n",
      "487  of:  8164  Loss:  0.2843308448791504  F1 Val:  0.23529411764705882  Val Acc:  79.6875  High:  0.99995196  Low:  0.13144854\n",
      "488  of:  8164  Loss:  0.4776430130004883  F1 Val:  0.29268292682926833  Val Acc:  77.34375  High:  0.99994195  Low:  0.12145296\n",
      "489  of:  8164  Loss:  0.6208478808403015  F1 Val:  0.25  Val Acc:  85.9375  High:  0.9999529  Low:  0.11027526\n",
      "490  of:  8164  Loss:  0.357170045375824  F1 Val:  0.47619047619047616  Val Acc:  91.40625  High:  0.99824727  Low:  0.10776121\n",
      "491  of:  8164  Loss:  0.42696118354797363  F1 Val:  0.22222222222222224  Val Acc:  89.0625  High:  0.99630415  Low:  0.10674939\n",
      "492  of:  8164  Loss:  0.33970779180526733  F1 Val:  0.21052631578947367  Val Acc:  88.28125  High:  0.99989974  Low:  0.10749321\n",
      "493  of:  8164  Loss:  0.35115236043930054  F1 Val:  0.5161290322580645  Val Acc:  88.28125  High:  0.99188155  Low:  0.10939633\n",
      "494  of:  8164  Loss:  0.3211173415184021  F1 Val:  0.5454545454545455  Val Acc:  92.1875  High:  0.9978828  Low:  0.11166242\n",
      "495  of:  8164  Loss:  0.385969877243042  F1 Val:  0.42424242424242425  Val Acc:  85.15625  High:  0.98741955  Low:  0.12012061\n",
      "496  of:  8164  Loss:  0.2912690043449402  F1 Val:  0.13636363636363635  Val Acc:  70.3125  High:  0.99999785  Low:  0.12587291\n",
      "497  of:  8164  Loss:  0.5477367639541626  F1 Val:  0.10526315789473684  Val Acc:  73.4375  High:  0.9995104  Low:  0.12678993\n",
      "498  of:  8164  Loss:  0.4618939161300659  F1 Val:  0.3137254901960785  Val Acc:  72.65625  High:  0.9956781  Low:  0.12703142\n",
      "499  of:  8164  Loss:  0.2646125257015228  F1 Val:  0.30434782608695654  Val Acc:  75.0  High:  0.9996786  Low:  0.12758377\n",
      "500  of:  8164  Loss:  0.33457648754119873  F1 Val:  0.2105263157894737  Val Acc:  88.28125  High:  0.961385  Low:  0.115837395\n",
      "501  of:  8164  Loss:  0.32960087060928345  F1 Val:  0.7058823529411764  Val Acc:  96.09375  High:  0.9946413  Low:  0.10296337\n",
      "502  of:  8164  Loss:  0.34737899899482727  F1 Val:  0.7000000000000001  Val Acc:  95.3125  High:  0.9150093  Low:  0.08677219\n",
      "503  of:  8164  Loss:  0.3164072036743164  F1 Val:  0.6666666666666666  Val Acc:  92.96875  High:  0.7919936  Low:  0.08002768\n",
      "504  of:  8164  Loss:  0.43775492906570435  F1 Val:  0.4666666666666667  Val Acc:  87.5  High:  0.9333088  Low:  0.059881907\n",
      "505  of:  8164  Loss:  0.3082484006881714  F1 Val:  0.43902439024390244  Val Acc:  82.03125  High:  0.9912305  Low:  0.051507197\n",
      "506  of:  8164  Loss:  0.19059205055236816  F1 Val:  0.42857142857142855  Val Acc:  87.5  High:  0.99858594  Low:  0.043420322\n",
      "507  of:  8164  Loss:  0.5782803297042847  F1 Val:  0.46153846153846156  Val Acc:  78.125  High:  0.9934942  Low:  0.03643692\n",
      "508  of:  8164  Loss:  0.3501826524734497  F1 Val:  0.42105263157894735  Val Acc:  82.8125  High:  0.9758101  Low:  0.033915814\n",
      "509  of:  8164  Loss:  0.36485549807548523  F1 Val:  0.4150943396226415  Val Acc:  75.78125  High:  0.99981993  Low:  0.029007753\n",
      "510  of:  8164  Loss:  0.3663106858730316  F1 Val:  0.5517241379310346  Val Acc:  89.84375  High:  1.0  Low:  0.023971973\n",
      "511  of:  8164  Loss:  0.5458002090454102  F1 Val:  0.42857142857142855  Val Acc:  87.5  High:  0.9985114  Low:  0.024134414\n",
      "512  of:  8164  Loss:  0.3077303171157837  F1 Val:  0.27586206896551724  Val Acc:  83.59375  High:  0.9974062  Low:  0.02578436\n",
      "513  of:  8164  Loss:  0.5207741856575012  F1 Val:  0.3720930232558139  Val Acc:  78.90625  High:  0.9859574  Low:  0.0301724\n",
      "514  of:  8164  Loss:  0.38256824016571045  F1 Val:  0.3404255319148936  Val Acc:  75.78125  High:  0.9803094  Low:  0.03936544\n",
      "515  of:  8164  Loss:  0.6162997484207153  F1 Val:  0.27586206896551724  Val Acc:  67.1875  High:  0.9813705  Low:  0.044609364\n",
      "516  of:  8164  Loss:  0.4656726121902466  F1 Val:  0.17073170731707318  Val Acc:  46.875  High:  0.9068771  Low:  0.08104619\n",
      "517  of:  8164  Loss:  0.4669269919395447  F1 Val:  0.1627906976744186  Val Acc:  43.75  High:  0.9910034  Low:  0.11128378\n",
      "518  of:  8164  Loss:  0.5089682340621948  F1 Val:  0.2222222222222222  Val Acc:  61.71875  High:  0.95284986  Low:  0.17927724\n"
     ]
    }
   ],
   "source": [
    "validate = True\n",
    "\n",
    "checking = next(iter(train_iter))\n",
    "inputs_check = checking.question_text\n",
    "target_check = checking.target\n",
    "\n",
    "\n",
    "f1_train = []\n",
    "auc_train = []\n",
    "train_accu = []\n",
    "\n",
    "avg_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = inputs_check\n",
    "    score1 = model(inputs)\n",
    "    #print(score)\n",
    "    \n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    avg_loss = 0\n",
    "    training_set = iter(train_iter)\n",
    "    vali_set = iter(vali_iter)\n",
    "    model.zero_grad()\n",
    "    all_preds = []\n",
    "    all_actual = []\n",
    "    for batch_num in range(len(train_iter)):\n",
    "        model.train()\n",
    "        sentence_data = next(training_set)\n",
    "        vali_data = next(vali_set)\n",
    "        sentence_in = sentence_data.question_text\n",
    "        vali_in = vali_data.question_text\n",
    "        \n",
    "        \n",
    "        #target -> target.unsqueeze(1)\n",
    "        if sigBCE:\n",
    "            targets = sentence_data.target.float()#.unsqueeze(1)\n",
    "            vali_target = vali_data.target.float()\n",
    "        else:\n",
    "            targets = torch.LongTensor(sentence_data.target)#.unsqueeze(1)\n",
    "            vali_target = torch.LongTensor(vali.target)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        target_scores = model(sentence_in)\n",
    "        \n",
    "        loss = loss_function(target_scores.squeeze(1), targets)#.squeeze(1))#.reshape(64,1).squeeze(1))\n",
    "        loss = (loss * (targets*17+1)/(targets*17+1).sum()).sum()#.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        vali_scores = model(vali_in)\n",
    "        vali_loss = loss_function(vali_scores.squeeze(1), vali_target).mean()\n",
    "        vali_f1 = (np.asarray(vali_scores.squeeze(1)) > 0.5).astype(int)\n",
    "        if sigBCE:\n",
    "            #prediction = np.asarray(target_scores.squeeze(1)>0.5,dtype=float)\n",
    "            #accuracy = np.asarray(prediction == np.asarray(targets)).sum()/len(targets)*100\n",
    "            prediction = np.asarray(vali_scores.squeeze(1)>0.5,dtype=float)\n",
    "            accuracy = np.asarray(prediction == np.asarray(vali_target)).sum()/len(vali_target)*100\n",
    "        else:\n",
    "            #prediction = target_scores.data.max(1)[1]\n",
    "            #accuracy = np.asarray(prediction==targets).sum()/len(targets)*100\n",
    "            prediction = vali_scores.data.max(1)[1]\n",
    "            accuracy = np.asarray(prediction == vali_target).sum()/len(vali_target)*100\n",
    "        #train_accu.append(accuracy)\n",
    "        \n",
    "        if sigBCE:\n",
    "            all_preds.extend(target_scores.tolist())#torch.round(target_scores).tolist())\n",
    "        else:\n",
    "            all_preds.extend(torch.exp(target_scores).tolist())#torch.round(target_scores).tolist())\n",
    "        all_actual.extend(targets.tolist())\n",
    "        \n",
    "        #print(np.asarray(target_scores)[:,1].max())\n",
    "        \n",
    "        #targets -> targets.reshape(64,1).squeeze(1)\n",
    "        \n",
    "        avg_loss += loss.item()\n",
    "        \n",
    "        #print(loss)\n",
    "        if sigBCE:\n",
    "            print(batch_num+1,\" of: \",len(train_iter), \" Loss: \",loss.item(),\" F1 Val: \",f1_score(vali_f1, vali_target),\" Val Acc: \",accuracy, \" High: \", np.asarray(target_scores).max(),\" Low: \", np.asarray(target_scores).min())\n",
    "        else:\n",
    "            print(batch_num+1,\" of: \",len(train_iter), \" Loss: \",loss.item(),\" Val Loss: \",vali_loss.item(),\" Acc: \",accuracy, \" High: \", np.asarray(torch.exp(target_scores))[:,1].max(),\" Low: \", np.asarray(target_scores).min())\n",
    "        \n",
    "        \n",
    "    \n",
    "    #model.eval()\n",
    "    avg_loss = avg_loss/len(train_iter)\n",
    "    \n",
    "    #if validate:\n",
    "        #avg_val\n",
    "    \n",
    "    \n",
    "    #else:\n",
    "    print(\"Training loss for epoch {} is {}\".format(epoch + 1, avg_loss))\n",
    "    \n",
    "    \n",
    "    #train_confusion_matrix = confusion_matrix(all_actual, all_preds).ravel()\n",
    "    if sigBCE:\n",
    "        pred_Clas = (np.asarray(all_preds) > 0.5).astype(int)#-2.65).astype(int)#\n",
    "    else:\n",
    "        pred_Clas = (np.asarray(all_preds)[:,1] > np.asarray(all_preds)[:,0]).astype(int)#-2.65).astype(int)#\n",
    "    train_confusion_matrix = confusion_matrix(all_actual, pred_Clas).ravel()\n",
    "    \n",
    "    print(\"Confusion matrix for epoch {}, tn={}, fp={}, fn={}, tp={}\".format(epoch + 1, *train_confusion_matrix))\n",
    "    f1_train.append(f1_score(all_actual, pred_Clas))\n",
    "    print(\"F1 score for epoch {} is {}\".format(epoch + 1, f1_train[-1]))\n",
    "    fpr, tpr, _ = roc_curve(all_actual, pred_Clas, pos_label=1)\n",
    "    auc_train.append(auc(fpr, tpr))\n",
    "    print(\"AUC for epoch {} is {}\\n\".format(epoch + 1, auc_train[-1]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = inputs_check\n",
    "    score2 = model(inputs)\n",
    "    #print(score)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"C:/Users/xw5735/PWinFolder/WPy-3661/notebooks/KaggleChal/LSTMTaggerFull22.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = torch.load(\"C:/Users/xw5735/PWinFolder/WPy-3661/notebooks/KaggleChal/LSTMTaggerFull.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "nb_classes = 2\n",
    "target = torch.LongTensor(batch_size).random_(nb_classes)\n",
    "#print((targets*5+1))\n",
    "print((loss * (targets*5+1)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction = np.asarray(target_scores.squeeze(1)>0.5,dtype=float)\n",
    "target_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preddd = np.asarray(all_preds)[:,1]\n",
    "#preddd_clas = (preddd>-0.45).astype(int)\n",
    "#train_confusion_matrix = confusion_matrix(all_actual, preddd_clas).ravel()\n",
    "#print(\"Confusion matrix for epoch {}, tn={}, fp={}, fn={}, tp={}\".format(epoch + 1, *train_confusion_matrix))\n",
    "#pred_Clas = (np.asarray(all_preds)[:,1] > np.asarray(all_preds)[:,0]).astype(int)\n",
    "#f1_score(all_actual, pred_Clas)\n",
    "#pred_Clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preddd = (np.asarray(all_preds)[:,1] > np.asarray(all_preds)[:,0]).astype(int)\n",
    "#len(preddd[preddd>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_search(y_true, y_proba, plot=False):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    thresholds = np.append(thresholds, 1.001) \n",
    "    F = 2 / (1/precision + 1/recall)\n",
    "    best_score = np.max(F)\n",
    "    best_th = thresholds[np.argmax(F)]\n",
    "    if plot:\n",
    "        plt.plot(thresholds, F, '-b')\n",
    "        plt.plot([best_th], [best_score], '*r')\n",
    "        plt.show()\n",
    "    search_result = {'threshold': best_th , 'f1': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_search_2(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in tqdm([i * 0.01 for i in range(100)], disable=True):\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#threshold_search_2(all_actual, preddd)\n",
    "#np.asarray(all_preds)[:,1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preddd = np.asarray(all_preds)[:,1]\n",
    "#preddd_clas = (preddd>-0.45).astype(int)\n",
    "#preddd.max()\n",
    "#len(preddd_clas[preddd_clas == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thr = threshold_search_2(all_actual, np.asarray(all_preds));thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thr = threshold_search(all_actual, np.asarray(all_preds), plot=True);thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_final_preds = []\n",
    "all_final_rounding = []\n",
    "model.eval()\n",
    "testing_batches = iter(test_iter)\n",
    "for batch_num in range(len(test_iter)):\n",
    "    final_preds = model(next(testing_batches).question_text)\n",
    "    prediction = np.asarray(final_preds>0.5,dtype=float)\n",
    "    print(batch_num+1,\"out of: \",len(test_iter),\" max: \",np.asarray(final_preds).max())\n",
    "    all_final_preds.extend(final_preds.tolist())\n",
    "    all_final_rounding.extend(prediction.tolist())\n",
    "    \n",
    "\n",
    "#probab = np.asarray(final_preds)[:,1]\n",
    "\n",
    "#final_preds_clas = (probab>thr['threshold']).astype(int)\n",
    "\n",
    "#final_preds_clas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(path/'sample_submission.csv')\n",
    "sub['prediction'] = (np.asarray(all_final_preds)>0.5).astype(int)\n",
    "sub.to_csv(\"submission_Kag_Full3.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(all_final_preds).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_batches = iter(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, weop in enumerate(test_iter):\n",
    "    print(i,\" \",weop.question_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enumerate(test_iter)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itering = next(testing_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itering.input_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itering.question_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.vocab.itos[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thr['threshold']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def train_model(model, train_data, epochs, show=False):\n",
    "    \n",
    "    #optimizer = torch.optim.Adam(model.parameters())\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "    #loss_function = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    print(\"The loss function being used is {}\".format(loss_function))\n",
    "    errors = []\n",
    "    eval_errors = []\n",
    "    f1_train = []\n",
    "    auc_train = []\n",
    "    f1_eval = []\n",
    "    auc_eval = []\n",
    "    \n",
    "    num_training_batches = len(train_data)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {}\".format(epoch + 1))\n",
    "        print(\"Training mode\")\n",
    "        model.train()\n",
    "        train_iter = iter(train_data)\n",
    "        model.zero_grad()\n",
    "        train_loss = 0\n",
    "        all_preds = []\n",
    "        all_actual = []\n",
    "        for batch_num in range(num_training_batches):\n",
    "            print(\"Batch {}\".format(batch_num + 1))\n",
    "            print(\"Out of {}\".format(num_training_batches))\n",
    "            batch = next(train_iter)\n",
    "            text, class_vector = batch.question_text.transpose(0,1),batch.target.unsqueeze(1)\n",
    "            #print(batch)\n",
    "            #print(text.shape)\n",
    "            #optimizer.zero_grad()\n",
    "            text_pred = model(text)\n",
    "            #print(text_pred)\n",
    "            print(class_vector.shape)\n",
    "            print(torch.max(class_vector,1)[0].shape)\n",
    "            loss = loss_function(text_pred,class_vector.reshape(64,1).squeeze(1)) #torch.max(class_vector,1)[0])\n",
    "            print(loss)\n",
    "            print(text_pred.squeeze(1))\n",
    "            print(torch.max(class_vector,1)[0])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#train_model(net, train_iter,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_iters = iter(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wee = next(train_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wee.question_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
